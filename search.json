[
  {
    "objectID": "syllabus626.html",
    "href": "syllabus626.html",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "",
    "text": "Paul Schrimpf\n112 Iona\npaul.schrimpf@ubc.ca\nOffice Hours: Tuesdays 11:30am-12:20pm Iona 112\n\n\n\nBo-Yu Chen\nboyu0721@student.ubc.ca\nOffice Hours: TBA \n\n\n\nLecture: Mondays and Wednesdays 2:00pm - 3:20pm, Buchanan B306\nTutorial: Wednesday 12:30pm - 1:50pm, Buchanan D214\n\n\n\nOur primary textbook will be notes written by Kyunchul Song. You can find them on Canvas. Other recommended (but not required) textbooks include:\n\nAris Spanos (2019) Probability Theory and Statistical Inference: Empirical Modeling with Observational Data\nBruce Hansen (2022) Probability and Statistics for Economists and Econometrics\nFumio Hayashi (2000) Econometrics\nGeorge Casella and Roger L. Berger (2002) Statistical Inference\nTakeshi Amemiya (1985) Advanced Econometrics"
  },
  {
    "objectID": "syllabus626.html#basic-information",
    "href": "syllabus626.html#basic-information",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "",
    "text": "Paul Schrimpf\n112 Iona\npaul.schrimpf@ubc.ca\nOffice Hours: Tuesdays 11:30am-12:20pm Iona 112\n\n\n\nBo-Yu Chen\nboyu0721@student.ubc.ca\nOffice Hours: TBA \n\n\n\nLecture: Mondays and Wednesdays 2:00pm - 3:20pm, Buchanan B306\nTutorial: Wednesday 12:30pm - 1:50pm, Buchanan D214\n\n\n\nOur primary textbook will be notes written by Kyunchul Song. You can find them on Canvas. Other recommended (but not required) textbooks include:\n\nAris Spanos (2019) Probability Theory and Statistical Inference: Empirical Modeling with Observational Data\nBruce Hansen (2022) Probability and Statistics for Economists and Econometrics\nFumio Hayashi (2000) Econometrics\nGeorge Casella and Roger L. Berger (2002) Statistical Inference\nTakeshi Amemiya (1985) Advanced Econometrics"
  },
  {
    "objectID": "syllabus626.html#grading",
    "href": "syllabus626.html#grading",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Grading",
    "text": "Grading\n\nProblem Sets (25%)\nThere will be 6-9 problem sets. A problem set with the lowest grade will be dropped from final grading.\n\n\nMidterm Exam (30%)\nGiven in class on Wednesday, October 15. Closed books and notes. There will be no make-up exam for midterm. When a student is excused from midterm exam, his or her grade will be based on reweighting of the problem sets (25%) and the final exam (75%).\n\n\nFinal Exam (45%)\nClosed books and notes."
  },
  {
    "objectID": "syllabus626.html#part-i-basics",
    "href": "syllabus626.html#part-i-basics",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part I: Basics",
    "text": "Part I: Basics\n\nProbability\n\nRandom Variables, and Distributions\nConditional Expectations and Conditional Distributions\nFamily of Distributions\n\nBasics of Inference - Estimation - Hypothesis Testing"
  },
  {
    "objectID": "syllabus626.html#part-ii-generalized-linear-model",
    "href": "syllabus626.html#part-ii-generalized-linear-model",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part II: Generalized Linear Model",
    "text": "Part II: Generalized Linear Model\n\nPreliminaries of Projection Geometry\nGeneralized Linear Models and Gauss-Markov Theorem\nTests of Linear Hypothesis"
  },
  {
    "objectID": "syllabus626.html#part-iii-tools-of-asymptotic-theory",
    "href": "syllabus626.html#part-iii-tools-of-asymptotic-theory",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part III: Tools of Asymptotic Theory",
    "text": "Part III: Tools of Asymptotic Theory\n\nModes of Convergence\nSlutsky’s Lemma/Continuous Mapping Theorem\nDelta Methods"
  },
  {
    "objectID": "syllabus626.html#part-v-linear-models-with-endogeneity",
    "href": "syllabus626.html#part-v-linear-models-with-endogeneity",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part V: Linear Models with Endogeneity",
    "text": "Part V: Linear Models with Endogeneity\n\nIdentification and Endogeneity\nIdentification through IV and Inference\n\n\n\n\n\n\n\nStudent Success\n\n\n\n\n\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here: https://senate.ubc.ca/policies-resources-support-student-success/.\n\n\n\n\n\n\n\n\n\nPolicy on Academic Honesty\n\n\n\n\n\nIt is the policy of the Vancouver School of Economics to report all violations of UBC’s standards for academic integrity to the office of the Dean of Arts. All violations of academic integrity standards will result in a grade of zero on the relevant assessment (exam, paper, assignment etc.). Students who do not have a previous offence may have the option to enter into a diversionary process with the Dean of Arts to resolve their misconduct (https://academicintegrity.ubc.ca/diversionary-process/). Any student who has a previous academic offence will be referred to the President’s Advisory Committee on Student Discipline (PACSD) (https://universitycounsel.ubc.ca/homepage/guides-and-resources/discipline/). PACSD may impose additional penalties including: a transcript notation indicating that the student has committed an academic offence, zero in the course, and/or suspension or expulsion from the University. You are personally responsible for understanding and following the UBC’s policies for academic integrity: https://vancouver.calendar.ubc.ca/campus-wide-policies-and-regulations/academic-honesty-and-standards.\n\n\n\n\n\n\n\n\n\nPolicy on Academic Concessions\n\n\n\n\n\nThere are only three acceptable grounds for academic concessions at UBC: unexpected changes in personal responsibilities that create a schedule conflict; medical circumstances; and compassionate grounds when the student experiences a traumatic event, sexual assault, or death in the family or of a close friend. Academic concessions for graded work and exams are granted for work that will be missed due to unexpected situations or circumstances. Situations that are expected (such as time constraints due to workload in other courses) or are predictable (such as being scheduled for paid work) are not grounds for academic concession.\nRequests for academic concessions should be made before the due date for that graded work and/or the writing of the exam. UBC policy does not allow for concessions to students who have missed work because they have registered for a course after the due date for that work. You can read more about the rules for academic concessions here: https://students.ubc.ca/enrolment/academic-learning-resources/academic-concessions. Students in the Faculty of Arts who require a concession can apply for concessions using this form here: https://students.air.arts.ubc.ca/academic-concession-form/. Students in other Faculties should consult their faculty website on academic concessions. Please note that the role of the faculty advising office is to review the evidence and to either support or not support concession requests. The final decision to grant the request always rests with your instructor.\n\n\n\n\n\n\n\n\n\nPolicy on the Use of AI Tools\n\n\n\n\n\nThe use of AI tools is permitted except where explicitly forbidden. However, that usage must be documented and attributed within your assessment(s). Students are responsible for all factual inaccuracies that are created by the use of AI tools."
  },
  {
    "objectID": "projection/projection.html#reading",
    "href": "projection/projection.html#reading",
    "title": "Least Squares as a Projection",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 5 (which is the basis for these slides)\nSupplemental: Schrimpf (2018), Schrimpf (2013b), Schrimpf (2013a), Schrimpf (2013d), Schrimpf (2013c)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\]"
  },
  {
    "objectID": "projection/projection.html#gauss-markov-theorem",
    "href": "projection/projection.html#gauss-markov-theorem",
    "title": "Least Squares as a Projection",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\n\n\nTheorem: Gauss-Markov\n\n\nIf \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the best linear unbiased estimator (BLUE) of \\(a'\\beta = a'\\hat{\\beta}\\) where \\(\\hat{\\beta} = (X'X)^{-1} X'y\\)\n\n\n\n\n\nThis chapter: use linear algebra to further understand and generalize this result"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces",
    "href": "projection/projection.html#orthogonal-subspaces",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\\(V \\subseteq \\R^n\\), inner product space\n\\(L \\subset V\\) a subspace\n\n\n\n\nDefinition\n\n\nAn \\(L \\subset V\\) is a subspace if \\(\\forall x, y \\in L\\), \\(\\alpha,\n\\beta \\in \\R\\), \\(\\alpha x + \\beta y \\in L\\)"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces-1",
    "href": "projection/projection.html#orthogonal-subspaces-1",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\\(V \\subseteq \\R^n\\), inner product space\n\\(L \\subset V\\) a subspace\n\n\n\n\nDefinition\n\n\nGiven a subspace \\(L \\subset V\\) the orthogonal complement of \\(L\\) is \\[\nL^\\perp = \\{x \\in V: x' l = 0 \\,\\forall l \\in L\\}\n\\]\n\n\n\n\nFor any \\(y \\in V\\), \\(\\exists y_1 \\in L\\), \\(y_2 \\in L^\\perp\\) s.t. \\(y = y_1 + y_2\\)"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces-2",
    "href": "projection/projection.html#orthogonal-subspaces-2",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\n\nLemma 1.1\n\n\nLet \\(L_1\\) and \\(L_2\\) be subspaces of \\(V\\), then \\[\n(\\underbrace{L_1 + L_2}_{\\{l_1 + l_2 \\in V:  l_1 \\in L_2, l_2 \\in L_2\\}})^\\perp = L_1^\\perp \\cap L_2^\\perp\n\\] and \\[\n(L_1 \\cap L_2)^\\perp = L_1^\\perp + L_2^\\perp\n\\]"
  },
  {
    "objectID": "projection/projection.html#projection",
    "href": "projection/projection.html#projection",
    "title": "Least Squares as a Projection",
    "section": "Projection",
    "text": "Projection\n\n\n\nDefinition\n\n\n\\(P_L y \\in  L\\) is the projection of \\(y\\) on \\(L\\) if \\[\n\\norm{y - P_L y } = \\inf_{w \\in L} \\norm{y - w}\n\\]\n\n\n\n\n\n\n\nProjection Theorem\n\n\n\n\\(P_L y\\) exists, is unique, and is a linear function of \\(y\\)\nFor any \\(y_1^* \\in L\\), \\(y_1^* = P_L y\\) iff \\(y- y_1^* \\perp L\\)\n\n\n\n\n\n2 implies if \\(y = y_1 + y_2\\) with \\(y_1 \\in L\\) and \\(y_2 \\in L^\\perp\\), then \\(y_1 = P_L y\\)"
  },
  {
    "objectID": "projection/projection.html#projection-map",
    "href": "projection/projection.html#projection-map",
    "title": "Least Squares as a Projection",
    "section": "Projection Map",
    "text": "Projection Map\n\n\n\nTheorem 1.2\n\n\nA linear map \\(G: V \\to L\\) is the projection map onto \\(L\\) iff \\(Gy = y\\) \\(\\forall y \\in L\\) and \\(Gy = 0\\) \\(\\forall y \\in L^\\perp\\)"
  },
  {
    "objectID": "projection/projection.html#projection-map-1",
    "href": "projection/projection.html#projection-map-1",
    "title": "Least Squares as a Projection",
    "section": "Projection Map",
    "text": "Projection Map\n\n\n\nDefinition\n\n\nLinear \\(G: V \\to V\\) is\n\nidempotent if \\(G (G y) =  G y\\) \\(\\forall y \\in V\\)\nsymmetric if \\(G'y = G y\\) \\(\\forall y \\in V\\)\n\n\n\n\n\n\n\nTheorem 1.3\n\n\nA linear map \\(G: V \\to V\\) is a projection map onto its range, \\(\\mathcal{R}(G)\\), iff \\(G\\) is idempotent and symmetric."
  },
  {
    "objectID": "projection/projection.html#projection-differences",
    "href": "projection/projection.html#projection-differences",
    "title": "Least Squares as a Projection",
    "section": "Projection Differences",
    "text": "Projection Differences\n\n\n\nTheorem 1.4\n\n\nLet \\(L \\subset V\\) and \\(L_0 \\subset L\\) be subspaces. Then \\(P_L - P_{L_0} = P_{L \\cap L_0^\\perp}\\)"
  },
  {
    "objectID": "projection/projection.html#projection-onto-x",
    "href": "projection/projection.html#projection-onto-x",
    "title": "Least Squares as a Projection",
    "section": "Projection onto \\(X\\)",
    "text": "Projection onto \\(X\\)\n\n\n\nDefinition\n\n\nFor linear \\(H: \\R^s \\to \\R^r\\), the g-inverse of \\(H\\) is any \\(H^{-}\\) s.t. \\(H H^{-} H = H\\)\n\n\n\n\n\n\n\nTheorem 1.5\n\n\nLet \\(X: \\R^k \\to \\R^n\\) be linear. The projection onto \\(\\mathcal{R}(X)\\) is \\(P_X = X(X'X)^- X'\\) where \\((X'X)^{-}\\) is any g-inverse of \\(X'X\\)"
  },
  {
    "objectID": "projection/projection.html#projection-spectrum",
    "href": "projection/projection.html#projection-spectrum",
    "title": "Least Squares as a Projection",
    "section": "Projection Spectrum",
    "text": "Projection Spectrum\n\n\n\nDefinition\n\n\nLet \\(A: V \\to V\\) be linear. Then \\(\\lambda\\) is an eigenvalue of \\(A\\) and \\(v \\neq 0\\) is an associated eigenvector if \\(A v = \\lambda v\\)\n\n\n\n\nExistence of (possibly complex) eigenvalues as roots of characteristic polynomial\n\n\n\n\nLemma 1.2\n\n\nThe eigenvalues of a symmetric and idempotent matrix, \\(P\\) are either \\(0\\) or \\(1\\). Furthmore rank of \\(P\\) is the sum of its eigenvalues."
  },
  {
    "objectID": "projection/projection.html#projection-rank",
    "href": "projection/projection.html#projection-rank",
    "title": "Least Squares as a Projection",
    "section": "Projection Rank",
    "text": "Projection Rank\n\n\n\nTheorem 1.6\n\n\n\n\\(\\mathrm{rank}(P_X) = \\mathrm{rank}(X)\\)\n\\(\\rank(I-P_X) = n - \\rank(X)\\)"
  },
  {
    "objectID": "projection/projection.html#generalized-linear-model-1",
    "href": "projection/projection.html#generalized-linear-model-1",
    "title": "Least Squares as a Projection",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\n\\[\nY = \\theta + u\n\\]\n\n\\(\\theta \\in L \\subset \\R^n\\), \\(L\\) a known subspace\n\\(u \\in \\R^n\\) unobserved\n\n\n\nExample: \\[\nY_i = x_{i,1} \\beta_1 + \\cdots + x_{i,k} \\beta_k + u_i\n\\]\n\n\\(X_k \\equiv (x_{1,k}, ... , x{n,k})'\\), \\(X \\equiv(X_1, ...,  X_k)\\), \\(\\beta \\equiv (\\beta_1, ..., \\beta_k)'\\), \\(y \\equiv (Y_1, ..., Y_n)'\\), \\(u \\equiv (u_1, ..., u_n)'\\) \\[y= X\\beta + u\\] fits setup with \\(L = \\mathcal{R}(X)\\)"
  },
  {
    "objectID": "projection/projection.html#least-squares",
    "href": "projection/projection.html#least-squares",
    "title": "Least Squares as a Projection",
    "section": "Least-Squares",
    "text": "Least-Squares\n\n\\(\\hat{\\theta} = P_L y\\), i.e. \\(\\norm{y - \\hat{\\theta} y }^2 = \\inf_{w \\in L} \\norm{y - w}^2\\)"
  },
  {
    "objectID": "projection/projection.html#gauss-markov-theorem-2",
    "href": "projection/projection.html#gauss-markov-theorem-2",
    "title": "Least Squares as a Projection",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\n\n\nTheorem: Gauss-Markov\n\n\nIf \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the best linear unbiased estimator (BLUE) of \\(a'\\theta = a'\\hat{\\theta}\\) where \\(\\hat{\\theta} = P_L y\\)\n\n\n\n\n\n\n\nCorollary\n\n\nIf \\[\ny = X'\\beta + u\n\\] and \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the BLUE of \\(c'\\beta\\) is \\(c'\\hat{\\beta}\\) with \\(\\hat{\\beta} =\n(X'X)^{-1} X' y\\)"
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html",
    "href": "problemsets/midterm2024/midterm2024_solution.html",
    "title": "ECON 626: Midterm",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\nSuppose \\(y_i^* = x_i' \\beta + \\epsilon_i\\) for \\(i=1, ..., n\\) with \\(x_i \\in \\R^k\\). Also suppose \\(y_i^*\\) is not always observed. Instead, you only observe \\(y_i = y_i^*\\) if \\(o_i = 1\\). The observed data is \\(\\left\\lbrace\\left(x_i, o_i, y_i = \\begin{cases} y_i^* & \\text{ if } o_i = 1 \\\\\n  \\text{missing} & \\text{ otherwise}\n\\end{cases}\\right)\\right\\rbrace_{i=1}^n\\). Throughout, assume that observations for different \\(i\\) are independent and identically distributed, \\(\\Er[\\epsilon_i | x_i] = 0\\), and \\(\\Er[x_ix_i' o_i]\\) is nonsingular.\n\n\n\nShow that if \\(\\epsilon_i\\) is independent of \\(o_i\\) conditional on \\(x_i\\), then \\(\\beta\\) is identified.\nNo longer assuming \\(\\epsilon_i\\) is independent of \\(o_i\\), show that \\(\\beta\\) is not identified by finding an observationally equivalent \\(\\tilde{\\beta}\\). [Hint: suppose \\(\\Er[o | x] &lt; 1\\) and consider \\(\\tilde{\\epsilon} = \\begin{cases} x_i' (\\beta - \\tilde{\\beta}) + \\epsilon_i & \\text{ if } o_i = 1 \\\\\n  x_i'(\\tilde{\\beta} - \\beta) \\frac{\\Er[o|x_i]}{\\Er[1-o|x_i]} + \\epsilon_i & \\text{ if } o_i = 0 \\end{cases}\\). You should verify that this \\(\\tilde{\\epsilon}\\) still meets the assumption that \\(\\Er[\\tilde{\\epsilon} | x_i] = 0\\).]\nNow suppose you observe \\(z_i \\in \\R^k\\) such that \\(\\Er[o_i z_i x_i']\\) is nonsingular and \\(\\Er[\\epsilon z_i | o_i = 1] = 0\\). Show that \\(\\beta\\) is identified.\n\n\nSolution. \n\nSince \\(\\epsilon\\) is independent of \\(o_i\\) given \\(x_i\\), we have \\[\n\\Er[x_i o_i \\epsilon ] = \\Er[x_i o_i \\Er[\\epsilon_i | x_i, o_i]] = \\Er[x_i o_i \\Er[\\epsilon_i | x_i]] = 0.\n\\] We can use this for identification. \\[\n\\begin{align*}\n\\Er[x_i o_i (y_i - x_i'\\beta)] & = 0 \\\\\n\\beta = & \\Er[x_ix_i'o_i]^{-1} \\Er[x_i y_i o_i]\n\\end{align*}\n\\]\nAs suggested by the hint, given any observed \\(\\{y_i, x_i, o_i\\}\\) created by \\(\\beta\\) and \\(\\epsilon_i\\), note that \\[\n\\begin{align*}\ny_i & = x_i'\\beta + \\epsilon_i \\text{ if } o_i = 1 \\\\\n& = x_i'\\tilde{\\beta} + \\underbrace{x_i'(\\beta - \\tilde{\\beta}) + \\epsilon_i}_{\\tilde{\\epsilon}_i} \\text{ if } o_i = 1\n\\end{align*}\n\\] so changing \\(\\beta\\) to \\(\\tilde{\\beta}\\) and \\(\\epsilon\\) to \\(\\tilde{\\epsilon}\\) leaves the observed data unchanged. Also, \\[\n\\begin{align*}\n\\Er[\\tilde{\\epsilon} | x] & = \\Er[o(x'(\\beta-\\tilde{\\beta}) + (1-o)x'(\\tilde{\\beta} - \\beta) \\frac{\\Er[o|x]}{\\Er[1-o|x]} + \\epsilon | x] \\\\\n= & \\Er[o|x](x'(\\beta-\\tilde{\\beta}) + \\Er[(1-o)|x]x'(\\tilde{\\beta} - \\beta) \\frac{\\Er[o|x]}{\\Er[1-o|x]} \\\\\n= & 0\n\\end{align*}\n\\] so \\(\\tilde{\\epsilon}\\) still meets the restriction that \\(\\Er[\\tilde{\\epsilon}|x]=0\\).\nSimilar to part 1, \\[\n\\Er[o_i z_i \\epsilon ] = \\Er[o_i \\Er[z_i \\epsilon | o_i]] = 0\n\\] so \\[\n\\begin{align*}\n\\Er[z_i o_i (y_i - x_i'\\beta)] & = 0 \\\\\n\\beta = & \\Er[z_ix_i'o_i]^{-1} \\Er[z_i y_i o_i]\n\\end{align*}\n\\] identifies \\(\\beta\\).\n\n\n\n\n\n\nConstruct a sample estimator for \\(\\beta\\) based on your answer to 1.a.1 and show that it is unbiased.\nSuppose that \\(\\epsilon_i \\sim N(0, 1)\\), independent of \\(o_i\\) and \\(x_i\\), and that \\(P(o_i=1|x_i) = g(x_i'\\alpha)\\) for some known function \\(g\\) and unknown parameter \\(\\alpha\\). Write the loglikelihood for \\((\\alpha,\\beta)\\), and show that \\(\\hat{\\beta}^{MLE}\\) does not depend on \\(\\alpha\\). Remember that the normal pdf is \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\\)\n\n\nSolution. \n\nWe have \\[\n\\begin{align*}\n\\Er[\\hat{\\beta}] & = \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i y_i) \\right] \\\\\n& = \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i (x_i'\\beta + \\epsilon_i)) \\right] \\\\\n& = \\beta + \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i \\epsilon_i) \\right] \\\\\n& = \\beta + \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i \\Er[\\epsilon_i|x_1,...,x_n, o_1,...,o_n]) \\right] \\\\\n& = \\beta\n\\end{align*}\n\\]\nThe log likelihood is \\[\n\\log \\ell(\\alpha,\\beta) = \\sum_{i=1}^n \\left( o_i\\left(\\frac{-\\log(2\\pi)}{2} - \\frac{(y_i - x_i'\\beta)^2}{2} + \\log(g(x_i'\\alpha))\\right) + (1-o_i)\\log(1-g(x_i'\\alpha)) \\right)\n\\] The first order condition for \\(\\hat{\\beta}^{MLE}\\) is \\[\n0 = \\sum o_i(y_i -x_i'\\hat{\\beta}^{MLE}) x_i\n\\] which does not depend on \\(\\alpha\\).\n\n\n\n\n\nAs in the previous part, suppose that \\(\\epsilon_i \\sim N(0, 1)\\), independent of \\(o_i\\) and \\(x_i\\), and that \\(P(o_i=1|x_i) = g(x_i'\\alpha)\\) for some known function \\(g\\) and unknown parameter \\(\\alpha\\).\n\nDerive the Cramér-Rao lower bound for \\(\\theta = (\\alpha,\\beta)\\)\nNow, treat \\(\\alpha\\) as known and derive the Cramér-Rao lower bound for just \\(\\beta\\). Does knowing \\(\\alpha\\) help with estimating \\(\\beta\\)?\n\n\nSolution. \n\nThe score for \\(\\alpha\\) is \\(\\frac{\\partial}{\\partial \\alpha}\\ell(\\alpha,\\beta) = \\sum_{i=1}^n \\frac{o_i}{g(x_i'\\alpha)} g'(x_i'\\alpha) x_i - \\frac{1-o_i}{1-g(x_i'\\alpha)} g'(x_i'\\alpha) x_i\\)\n\nThe Hessian is \\[\nH = \\begin{pmatrix} -\\sum_{i=1}^n x_i x_i' & 0 \\\\ 0 & A_n(\\alpha) \\end{pmatrix}\n\\] where \\(A_n(\\alpha)\\) is the derivative of the score for \\(\\alpha\\). We can see from the score for \\(\\alpha\\) that it does not depend on \\(\\beta\\). The Cramer Rao lower bound is $^{-1} =\n\\[\\begin{pmatrix} \\Er[n x_i x_i']^{-1} & 0 \\\\ 0 & A_n(\\alpha)^{-1} \\end{pmatrix}\\]\n\nNow the Cramer Rao lower bound is just \\(\\Er[n x_i x_i']^{-1}\\), which is the same as if \\(\\alpha\\) were unknown. Knowing \\(\\alpha\\) does not help estimate \\(\\beta\\) in this situation."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#identification",
    "href": "problemsets/midterm2024/midterm2024_solution.html#identification",
    "title": "ECON 626: Midterm",
    "section": "",
    "text": "Show that if \\(\\epsilon_i\\) is independent of \\(o_i\\) conditional on \\(x_i\\), then \\(\\beta\\) is identified.\nNo longer assuming \\(\\epsilon_i\\) is independent of \\(o_i\\), show that \\(\\beta\\) is not identified by finding an observationally equivalent \\(\\tilde{\\beta}\\). [Hint: suppose \\(\\Er[o | x] &lt; 1\\) and consider \\(\\tilde{\\epsilon} = \\begin{cases} x_i' (\\beta - \\tilde{\\beta}) + \\epsilon_i & \\text{ if } o_i = 1 \\\\\n  x_i'(\\tilde{\\beta} - \\beta) \\frac{\\Er[o|x_i]}{\\Er[1-o|x_i]} + \\epsilon_i & \\text{ if } o_i = 0 \\end{cases}\\). You should verify that this \\(\\tilde{\\epsilon}\\) still meets the assumption that \\(\\Er[\\tilde{\\epsilon} | x_i] = 0\\).]\nNow suppose you observe \\(z_i \\in \\R^k\\) such that \\(\\Er[o_i z_i x_i']\\) is nonsingular and \\(\\Er[\\epsilon z_i | o_i = 1] = 0\\). Show that \\(\\beta\\) is identified.\n\n\nSolution. \n\nSince \\(\\epsilon\\) is independent of \\(o_i\\) given \\(x_i\\), we have \\[\n\\Er[x_i o_i \\epsilon ] = \\Er[x_i o_i \\Er[\\epsilon_i | x_i, o_i]] = \\Er[x_i o_i \\Er[\\epsilon_i | x_i]] = 0.\n\\] We can use this for identification. \\[\n\\begin{align*}\n\\Er[x_i o_i (y_i - x_i'\\beta)] & = 0 \\\\\n\\beta = & \\Er[x_ix_i'o_i]^{-1} \\Er[x_i y_i o_i]\n\\end{align*}\n\\]\nAs suggested by the hint, given any observed \\(\\{y_i, x_i, o_i\\}\\) created by \\(\\beta\\) and \\(\\epsilon_i\\), note that \\[\n\\begin{align*}\ny_i & = x_i'\\beta + \\epsilon_i \\text{ if } o_i = 1 \\\\\n& = x_i'\\tilde{\\beta} + \\underbrace{x_i'(\\beta - \\tilde{\\beta}) + \\epsilon_i}_{\\tilde{\\epsilon}_i} \\text{ if } o_i = 1\n\\end{align*}\n\\] so changing \\(\\beta\\) to \\(\\tilde{\\beta}\\) and \\(\\epsilon\\) to \\(\\tilde{\\epsilon}\\) leaves the observed data unchanged. Also, \\[\n\\begin{align*}\n\\Er[\\tilde{\\epsilon} | x] & = \\Er[o(x'(\\beta-\\tilde{\\beta}) + (1-o)x'(\\tilde{\\beta} - \\beta) \\frac{\\Er[o|x]}{\\Er[1-o|x]} + \\epsilon | x] \\\\\n= & \\Er[o|x](x'(\\beta-\\tilde{\\beta}) + \\Er[(1-o)|x]x'(\\tilde{\\beta} - \\beta) \\frac{\\Er[o|x]}{\\Er[1-o|x]} \\\\\n= & 0\n\\end{align*}\n\\] so \\(\\tilde{\\epsilon}\\) still meets the restriction that \\(\\Er[\\tilde{\\epsilon}|x]=0\\).\nSimilar to part 1, \\[\n\\Er[o_i z_i \\epsilon ] = \\Er[o_i \\Er[z_i \\epsilon | o_i]] = 0\n\\] so \\[\n\\begin{align*}\n\\Er[z_i o_i (y_i - x_i'\\beta)] & = 0 \\\\\n\\beta = & \\Er[z_ix_i'o_i]^{-1} \\Er[z_i y_i o_i]\n\\end{align*}\n\\] identifies \\(\\beta\\)."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#estimation",
    "href": "problemsets/midterm2024/midterm2024_solution.html#estimation",
    "title": "ECON 626: Midterm",
    "section": "",
    "text": "Construct a sample estimator for \\(\\beta\\) based on your answer to 1.a.1 and show that it is unbiased.\nSuppose that \\(\\epsilon_i \\sim N(0, 1)\\), independent of \\(o_i\\) and \\(x_i\\), and that \\(P(o_i=1|x_i) = g(x_i'\\alpha)\\) for some known function \\(g\\) and unknown parameter \\(\\alpha\\). Write the loglikelihood for \\((\\alpha,\\beta)\\), and show that \\(\\hat{\\beta}^{MLE}\\) does not depend on \\(\\alpha\\). Remember that the normal pdf is \\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}\\)\n\n\nSolution. \n\nWe have \\[\n\\begin{align*}\n\\Er[\\hat{\\beta}] & = \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i y_i) \\right] \\\\\n& = \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i (x_i'\\beta + \\epsilon_i)) \\right] \\\\\n& = \\beta + \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i \\epsilon_i) \\right] \\\\\n& = \\beta + \\Er\\left[ (\\sum o_i x_i x_i')^{-1} (\\sum o_i x_i \\Er[\\epsilon_i|x_1,...,x_n, o_1,...,o_n]) \\right] \\\\\n& = \\beta\n\\end{align*}\n\\]\nThe log likelihood is \\[\n\\log \\ell(\\alpha,\\beta) = \\sum_{i=1}^n \\left( o_i\\left(\\frac{-\\log(2\\pi)}{2} - \\frac{(y_i - x_i'\\beta)^2}{2} + \\log(g(x_i'\\alpha))\\right) + (1-o_i)\\log(1-g(x_i'\\alpha)) \\right)\n\\] The first order condition for \\(\\hat{\\beta}^{MLE}\\) is \\[\n0 = \\sum o_i(y_i -x_i'\\hat{\\beta}^{MLE}) x_i\n\\] which does not depend on \\(\\alpha\\)."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#efficiency",
    "href": "problemsets/midterm2024/midterm2024_solution.html#efficiency",
    "title": "ECON 626: Midterm",
    "section": "",
    "text": "As in the previous part, suppose that \\(\\epsilon_i \\sim N(0, 1)\\), independent of \\(o_i\\) and \\(x_i\\), and that \\(P(o_i=1|x_i) = g(x_i'\\alpha)\\) for some known function \\(g\\) and unknown parameter \\(\\alpha\\).\n\nDerive the Cramér-Rao lower bound for \\(\\theta = (\\alpha,\\beta)\\)\nNow, treat \\(\\alpha\\) as known and derive the Cramér-Rao lower bound for just \\(\\beta\\). Does knowing \\(\\alpha\\) help with estimating \\(\\beta\\)?\n\n\nSolution. \n\nThe score for \\(\\alpha\\) is \\(\\frac{\\partial}{\\partial \\alpha}\\ell(\\alpha,\\beta) = \\sum_{i=1}^n \\frac{o_i}{g(x_i'\\alpha)} g'(x_i'\\alpha) x_i - \\frac{1-o_i}{1-g(x_i'\\alpha)} g'(x_i'\\alpha) x_i\\)\n\nThe Hessian is \\[\nH = \\begin{pmatrix} -\\sum_{i=1}^n x_i x_i' & 0 \\\\ 0 & A_n(\\alpha) \\end{pmatrix}\n\\] where \\(A_n(\\alpha)\\) is the derivative of the score for \\(\\alpha\\). We can see from the score for \\(\\alpha\\) that it does not depend on \\(\\beta\\). The Cramer Rao lower bound is $^{-1} =\n\\[\\begin{pmatrix} \\Er[n x_i x_i']^{-1} & 0 \\\\ 0 & A_n(\\alpha)^{-1} \\end{pmatrix}\\]\n\nNow the Cramer Rao lower bound is just \\(\\Er[n x_i x_i']^{-1}\\), which is the same as if \\(\\alpha\\) were unknown. Knowing \\(\\alpha\\) does not help estimate \\(\\beta\\) in this situation."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#generated-sigma-fields",
    "href": "problemsets/midterm2024/midterm2024_solution.html#generated-sigma-fields",
    "title": "ECON 626: Midterm",
    "section": "Generated \\(\\sigma\\)-fields",
    "text": "Generated \\(\\sigma\\)-fields\nLet \\(X(\\omega) = \\begin{cases} 1 & \\text{ if } \\omega = a \\text{ or } b \\\\\n  0 & \\text{ otherwise}\n\\end{cases}\\). List the elements of \\(\\sigma(X)\\).\n\nSolution. \\(\\sigma(X) = \\{ \\{a,b\\}, \\{c,d\\}, \\Omega, \\emptyset \\}\\)."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#independence",
    "href": "problemsets/midterm2024/midterm2024_solution.html#independence",
    "title": "ECON 626: Midterm",
    "section": "Independence",
    "text": "Independence\nLet \\(Y(\\omega) = \\begin{cases} 1 & \\text{ if } \\omega = b \\text{ or } c \\\\\n  0 & \\text{ otherwise}\n\\end{cases}\\). Can \\(X\\) and \\(Y\\) be independent?\n\nSolution. Yes, for example if \\(P(a) = P(b) = P(c) = P(d) = 1/4\\), then \\(P(X=x,Y=y) = 1/4 = P(X=x)P(Y=y)\\) for \\(x, y \\in \\{0,1\\}^2\\)."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#testing",
    "href": "problemsets/midterm2024/midterm2024_solution.html#testing",
    "title": "ECON 626: Midterm",
    "section": "Testing",
    "text": "Testing\n(For this and subsequent parts, ignore any restrictions on \\(\\theta_x\\) and \\(\\theta_y\\) implied by the sample space and form of \\(X\\) and \\(Y\\) in the first two parts).\nSuppose you observe an independent and identically distributed sample of \\(\\{(x_i, y_i)\\}_{i=1}^n\\). For each \\(i\\), \\(x_i = 1\\) with probability \\(\\theta_x\\) and 0 otherwise, and \\(y_i = 1\\) with probability \\(\\theta_y\\) and 0 otherwise. Assume \\(x_i\\) is independent of \\(y_i\\).\n\nFind the most powerful test for testing \\(H_0: (\\theta_x, \\theta_y) = (\\theta_x^0, \\theta_y^0)\\) against \\(H_1: (\\theta_x, \\theta_y) = (\\theta_x^1, \\theta_y^1)\\).\nShow that there is a most powerful test for testing \\(H_0: \\theta_x = \\theta_x^0\\) against \\(H_1: \\theta_x = \\theta_x^1\\), where under the null and alternative, \\(\\theta_y\\) is unrestricted.\n\n\nSolution. \n\nBy the Neyman-Pearson lemma, the likelihood ratio test is most powerful. The log likelihood ratio is \\[\n\\begin{align*}\n\\tau = \\sum_{i=1}^n & x_i(\\log(\\theta_x^1) - \\log(\\theta_x^0)) + (1-x_i)(\\log(1-\\theta_x^1)-\\log(1-\\theta_x^0)) + \\\\\n& + y_i(\\log(\\theta_y^1) - \\log(\\theta_y^0)) + (1-y_i)(\\log(1-\\theta_y^1)-\\log(1-\\theta_y^0)) \\\\\n= & n_x(\\log(\\theta_x^1) - \\log(\\theta_x^0)) + (n-n_x)(\\log(1-\\theta_x^1)-\\log(1-\\theta_x^0)) + \\\\\n& +\nn_y(\\log(\\theta_y^1) - \\log(\\theta_y^0)) + (n-n_y)(\\log(1-\\theta_y^1)-\\log(1-\\theta_y^0)) \\\\\n\\end{align*}\n\\]\n\nwhere \\(n_x = \\sum_{i=1}^n x_i\\) and \\(n_y = \\sum_{i=1}^n y_i\\). For a test of size \\(\\alpha\\), we would find \\(c\\) such that \\(P(\\tau&gt;c|H_0) = \\alpha\\) and reject if \\(\\tau &gt; c\\).\n\nWe can interpret this as testing the point null and alternative \\(H_0: \\theta_x = \\theta_x^0, \\theta_y = \\theta_y^0\\), against \\(H_1:  \\theta_x = \\theta_x^1, \\theta_y = \\theta_y^1\\) with \\(\\theta_y^1 = \\theta_y^0\\). In that case, the test statistic becomes \\[\n\\tau = n_x(\\log(\\theta_x^1) - \\log(\\theta_x^0)) + (n-n_x)(\\log(1-\\theta_x^1)-\\log(1-\\theta_x^0))\n\\] and importantly does not depend on \\(\\theta_y\\) nor \\(n_y\\). Thus, the same likelihood ratio test will be most powerful for any \\(\\theta_y\\)."
  },
  {
    "objectID": "problemsets/midterm2024/midterm2024_solution.html#behavior-of-averages",
    "href": "problemsets/midterm2024/midterm2024_solution.html#behavior-of-averages",
    "title": "ECON 626: Midterm",
    "section": "Behavior of Averages",
    "text": "Behavior of Averages\n\nNote that \\(\\Er[x_i] = \\theta_x\\) and \\(\\Er[(x_i - \\theta_x)^2] = \\theta_x(1-\\theta_x)\\). Use Markov’s inequality to show that \\[\nP\\left(\\left\\vert \\frac{1}{n} \\sum_{i=1}^n x_i - \\theta_x \\right\\vert &gt; \\epsilon \\right) \\leq \\frac{\\theta_x(1-\\theta_x)}{n \\epsilon^2}.\n\\]\nShow that \\[\n\\lim_{n \\to \\infty} P\\left(\\left\\vert \\frac{1}{n} \\sum_{i=1}^n x_i - \\theta_x \\right\\vert &gt; \\epsilon \\right) = 0.\n\\]\n\n\nSolution. \n\nThis is Markov’s inequality with \\(k=2\\), because \\(\\Er[\\left(\\frac{1}{n} \\sum_{i=1}^n x_i - \\theta_x \\right)^2] = \\frac{\\theta_x (1-\\theta_x)}{n}\\).\nTaking limits of the previous part directly gives this conclusion."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html",
    "href": "problemsets/midterm2022/midterm2022_solution.html",
    "title": "ECON 626: Midterm Solutions",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#identification",
    "href": "problemsets/midterm2022/midterm2022_solution.html#identification",
    "title": "ECON 626: Midterm Solutions",
    "section": "Identification",
    "text": "Identification\n\nIf \\(m\\) is strictly increasing show that \\(\\beta\\) is identified by explicitly writing \\(\\beta\\) as a function of the distribution of \\(X\\) and \\(Y\\).\n\n\nSolution. Since \\(m\\) is increasing, \\(m^{-1}\\) exists. We can then identify \\(\\beta\\) as \\[\n\\begin{aligned}\n\\beta = & \\Er[X_iX_i']^{-1} \\Er[X_i m^{-1}(Y_i)] \\\\\n= & \\Er[X_iX_i']^{-1} \\Er[X_i (X_i' \\beta + u_i)]  = \\beta\n\\end{aligned}\n\\]\n\n\nSuppose \\(m(z) = 1\\{z \\geq 0\\}\\). For simplicitly, let \\(k=1\\) and \\(X_i \\in \\{-1, 1\\}\\). Show that \\(\\beta\\) is not identified by finding an observationally equivalent \\(\\tilde{\\beta}\\).\n\n\nSolution. Since \\(Y \\in \\{0,1\\}\\) and \\(P(Y=0|X) = 1-P(Y=1|X)\\), just looking at \\(P(Y=1|X)\\) completely describes \\(P(Y|X)\\). For any \\(\\beta\\), we have \\[\n\\begin{aligned}\nP_{Y,X}(\\dot|\\beta, F_u) = & P(Y=1|X;\\beta, F_u) P(X) \\\\\n= & P(X\\beta + u \\geq 0 | X; F_u) P(X) \\\\\n= & \\left[ 1 - F_u(-X\\beta) \\right] P(X) \\\\\n= & \\begin{cases} \\left[ 1 - F_u(\\beta) \\right] P(X=-1) & \\text{ if} X=-1 \\\\\n\\left[ 1 - F_u(-\\beta) \\right] P(X=1) & \\text{ if} X=1\n\\end{cases}\n\\end{aligned}\n\\] If \\(\\beta \\neq 0\\), \\(\\beta\\) is observationally equivalent to \\(\\tilde{\\beta} = s \\beta\\) for any \\(s \\neq 0\\) with \\(\\tilde{u} = s\nu\\). The even \\(X\\beta + u \\geq 0\\) is not affected by multiplying by \\(s\\), so \\(P(X\\beta + u \\geq 0 | X; F_u) P(X)  = P(X\\tilde{\\beta} + \\tilde{u} \\geq 0 | X; F_\\tilde{u}) P(X)\\).\nIf \\(\\beta = 0\\), then \\(\\beta\\) is observationally equivalent to any \\(\\tilde{\\beta}&gt;0\\) (also negative ones with an appropriate modified \\(\\tilde{u}\\)) and \\(\\tilde{u}\\) with \\[\nF_{\\tilde{u}}(x) = \\begin{cases} F_u(x + \\tilde{\\beta}) & \\text{ if } x &lt; -\\tilde{\\beta} \\\\\nF_u(0) & \\text{ if } -\\tilde{\\beta} &lt; x &lt; \\tilde{\\beta} \\\\\nF_u(x-\\tilde{\\beta}) & \\text{ if } \\tilde{\\beta} &lt; x \\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#estimation",
    "href": "problemsets/midterm2022/midterm2022_solution.html#estimation",
    "title": "ECON 626: Midterm Solutions",
    "section": "Estimation",
    "text": "Estimation\nConstruct a sample analogue estimator for \\(\\beta\\) based on your answer to 1.i.1.1 Show whether your estimator is unbiased.\n\nSolution. The sample analogue estimator is \\[\n\\hat{\\beta} = \\left( \\sum_{i=1}^n X_i X_i' \\right)^{-1} \\left(\\sum_{i=1}^n X_i m^{-1}(Y_i) \\right)\n\\] It is unbiased because \\[\n\\begin{aligned}\n\\Er[\\hat{\\beta}] & = \\Er\\left[\\left( \\sum_{i=1}^n X_i X_i'\\right)^{-1} (\\sum_{i=1}^n X_i m^{-1}(Y_i) ) \\right] \\\\\n& = \\Er\\left[\\left( \\sum_{i=1}^n X_i X_i'\\right)^{-1} \\sum_{i=1}^n X_i (X_i' \\beta + u_i) \\right] \\\\\n& = \\beta + \\Er\\left[\\left( \\sum_{i=1}^n X_i X_i'\\right)^{-1} \\sum_{i=1}^n X_i u_i  \\right] \\\\\n& = \\beta\n\\end{aligned}\n\\] where the final equality is because \\(X\\) and \\(u\\) are independent and \\(\\Er[u]=0\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#efficiency",
    "href": "problemsets/midterm2022/midterm2022_solution.html#efficiency",
    "title": "ECON 626: Midterm Solutions",
    "section": "Efficiency",
    "text": "Efficiency\nLet \\(X = (X_1, ..., X_n)'\\) denote the \\(n \\times k\\) matrix of \\(X_i\\), and \\(m^{-1}(y) = (m^{-1}(Y_i), ..., m^{-1}(Y_n))\\) For this section, you may treat \\(X\\) as non-stochastic.\n\nAssume that \\(\\Er[uu'] = \\sigma^2 I_n\\). What is the minimal variance unbiased estimator for \\(c'\\beta\\) that is a linear function of \\(m^{-1}(y)\\)?\n\n\nSolution. By the Gauss Markov Theorem, \\(\\hat{\\beta}\\) is the best linear unbiased estimator.\n\n\nAssume \\(u_i = S_i \\epsilon_i\\) with \\(S_i\\) observed, \\(\\Er[\\epsilon] =0\\), and \\(\\Er[\\epsilon\\epsilon'] = I_n\\). What is the minimal variance unbiased estimator that is a linear function of \\(m^{-1}(y)?\\)\n\n\nSolution. If we let \\(\\tilde{y}_i = S_i^{-1} m^{-1}(Y_i)\\) and \\(\\tilde{X}_i =\nS_i^{-1} X_i\\), then we have the linear model \\[\n\\tilde{y}_i = \\tilde{X}_i \\beta + \\underbrace{\\epsilon_i }_{\\equiv S_i^{-1} u_i}\n\\] This model has \\(\\Er[\\epsilon]=0\\) and \\(\\Er[\\epsilon\\epsilon'] = I_n\\), so the Gauss-Markov theorem applies. The best linear (in \\(\\tilde{y}_i)\\) unbiased estimator \\(c'\\beta\\) is \\(c'\\hat{\\beta}^W = c'(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' \\tilde{y}\\). Note that \\[\nc' \\hat{\\beta}^W = c'(X' diag(S)^{-2}X)^{-1} X' diag(S)^{-2} \\tilde{y}\n\\] so this estimator is linear in \\(m^{-1}(y)\\), and thus is the best linear in \\(m^{-1}(y)\\) unbiased estimator.\n\n\nSuppose that \\(u_i \\sim N(0, \\sigma^2)\\). Show that \\(c'(X'X)^{-1}X'm^{-1}(y)\\) is the minimal variance unbiased estimator for \\(c'\\beta\\).\n\n\nSolution. The log likelihood is \\[\n\\ell(\\beta) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (m^{-1}(y_i) - X_i' \\beta)^2\n\\] The score is \\[\ns(\\beta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n X_i (m^{-1}(y_i) - X_i' \\beta)\n\\] The hessian is \\[\nH(\\beta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n -X_i X_i'\n\\] The Cramer-Rao Lower bound is \\[\nI(\\beta) = \\Er[-H^{-1}] = \\sigma^2 (X'X)^{-1}\n\\] We must compare this with \\(\\var(\\hat{\\beta})\\). \\[\n\\begin{aligned}\n\\var(\\hat{\\beta}) = & \\var( (X'X)^{-1} X'm^{-1}(y)) \\\\\n= & (X'X)^{-1} \\var(X'm^{-1}(y)) (X'X)^{-1} \\\\\n= & (X'X)^{-1} \\var(X'u) (X'X)^{-1} \\\\\n= & (X'X)^{-1} X'X \\sigma^2 (X'X)^{-1} \\\\\n= & (X'X)^{-1}\\sigma^2  = I(\\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#sigma-fields",
    "href": "problemsets/midterm2022/midterm2022_solution.html#sigma-fields",
    "title": "ECON 626: Midterm Solutions",
    "section": "\\(\\sigma\\) fields",
    "text": "\\(\\sigma\\) fields\n\nSuppose \\(K=5\\) and \\(g(u) = |u - (K+1)/2|\\). What is \\(\\sigma(Y)\\)?\n\n\nSolution. The support of \\(Y\\) is \\(\\{0, 1, 2\\}\\). The preimage of these sets are \\(\\{3\\}\\), \\(\\{2,4\\}\\), and \\(\\{1, 5\\}\\). \\(\\sigma(Y)\\) additionally contains unions, complements, and intersections of these sets, so \\[\n\\sigma(Y) = \\{ \\emptyset, \\{3\\}, \\{1, 5\\}, \\{2, 4\\}, \\{2,3,4\\}, \\{1, 3, 5\\}, \\{1, 2, 4, 5\\}, \\{1, 2, 3, 4, 5\\} \\}\n\\]\n(If we interpret \\(u\\) as a random variable, then actually, we should replace the sets listed as \\(\\sigma(Y)\\) with \\(u^{-1}\\) of these sets, so that we end up with sets in some unspecified sample space, \\(\\Omega\\)).\n\n\nSuppose \\(g\\) is one to one. What is \\(\\sigma(Y)\\)?\n\n\nSolution. Then for any \\(A \\subset \\{1, ..., K\\}\\), \\(g^{-1}(g(A)) = A\\), so \\(\\sigma(Y) = \\sigma(U) =\\) power set of \\(\\{u^{-1}(1) ,..., u^{-1}(K) \\}\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#identification-1",
    "href": "problemsets/midterm2022/midterm2022_solution.html#identification-1",
    "title": "ECON 626: Midterm Solutions",
    "section": "Identification",
    "text": "Identification\nShow that if if \\(g\\) is one to one, then \\(\\theta_1, ... , \\theta_k\\) are identified.\n\nSolution. If \\(g\\) is one to one, then \\(\\theta_j = P(Y=g(j))\\) identifies \\(\\theta_j\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#estimation-1",
    "href": "problemsets/midterm2022/midterm2022_solution.html#estimation-1",
    "title": "ECON 626: Midterm Solutions",
    "section": "Estimation",
    "text": "Estimation\nAssuming \\(g\\) is one to one, find the maximum likelihood estimator for \\(\\theta\\), and show whether it is unbiased.\n\nSolution. The likelihood is \\[\n\\begin{aligned}\n\\ell(\\theta; Y) = & \\prod_{i=1}^n \\prod_{j=1}^K \\theta_j^{1\\{Y_i = g(j) \\}} \\\\\n= & \\prod_{j=1}^K \\theta_j^{\\sum_{i=1}^n 1\\{Y_i = g(j) \\}} \\\\\n\\log \\ell (\\theta; Y) = & \\sum_{j=1}^K \\left( \\sum_{i=1}^n 1\\{Y_i = g(j) \\} \\right) \\log \\theta_j\n\\end{aligned}\n\\] To simplify notation, let \\(n_k = \\sum_{i=1}^n  1\\{Y_i = g(k) \\}\\). We want to solve \\[\n\\max_\\theta \\sum_{k=1}^K n_k \\log \\theta_k \\text{ s.t. } \\sum_{k=1}^K \\theta_k = 1\n\\] The first order conditions are \\[\n\\frac{n_k}{\\theta_k} = \\lambda\n\\] or \\(n_k = \\lambda \\theta_k\\). Summing across \\(k\\), and using the constraint we have \\(\\lambda =\n\\sum_{k} n_k = n\\). Thus, \\[\n\\hat{\\theta}_k = \\frac{n_k}{n}\n\\]\nThis is unbiased because \\[\n\\Er[\\hat{\\theta}_k] = \\Er\\left[ \\frac{1}{n} \\sum_i 1\\{Y_i = g(k) \\}  \\right] = P(U_i = k) = \\theta_k\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#testing",
    "href": "problemsets/midterm2022/midterm2022_solution.html#testing",
    "title": "ECON 626: Midterm Solutions",
    "section": "Testing",
    "text": "Testing\n\nFor \\(K=2\\), find the most powerful test for testing \\(H_0: \\theta_1 = \\theta_1^0\\) against \\(H_1: \\theta_1 = \\theta_1^a\\).\n\n\nSolution. By the Neyman-Pearson Lemma, the likelihood ratio test is most powerfull. The likelihood here is: \\[\n\\begin{aligned}\nf(Y;\\theta) & = \\prod_{i=1}^n \\theta^{1\\{Y_i = g(1)\\}} (1-\\theta)^{1\\{Y_i = g(2) \\}} \\\\\n& = \\theta^{n_1}(1-\\theta)^{n_2}\n\\end{aligned}\n\\] where \\(n_j = \\sum_i 1\\{Y_i = g(j)\\}\\). Note that \\(n_2 = n-n_1\\)\nThe likelihood ratio is then \\[\n\\tau(Y) = \\frac{(\\theta_1^a)^{n_1} (1-\\theta_1^a)^{n-n_1}} { (\\theta_1^0)^{n_1} (1-\\theta_1^0)^{n-n_1}}\n\\] To find a critical region, notice that this only depend on the data through \\(n_1\\). Under \\(H_0\\), the distribution of \\(n_1\\) is \\[\nP(n_1 = m) = \\frac{n!}{m!(n-m)!} (\\theta_1^0)^m (1-\\theta_1^0)^{n - m}\n\\] (After writing the rest of the solution, I suppose this formula isn’t really needed).\nTo proceed further, assume that \\(\\theta_1^a &gt; \\theta_1^0\\), then \\(\\tau(Y) \\equiv \\tau(n_1)\\) is increasing as a function \\(n_1\\), and \\[\nP(\\tau(n_1) &gt; \\tau(c) | H_0) = P(n_1 &gt; c | H_0)\n\\] For a test of size \\(\\alpha\\), we choose \\(c\\) such that \\(P(n_1 &gt; c | H_0)\n= \\alpha\\), and reject \\(H_0\\) if \\(n_1 &gt; c\\).\nFor \\(\\theta_1^a &lt; \\theta_1^0\\), by similar reasoning, we have \\[\nP(\\tau(n_1) &gt; \\tau(c) | H_0) = P(n_1 &lt; c | H_0)\n\\] where the second inequality is flipped because now \\(\\tau\\) decreases with \\(n_1\\). Thus, the critical region is \\(\\{n_1: n_1 &lt; c\\}\\)\n\n\nIs this test also most powerful against the alternative \\(H_1: \\theta_1 \\neq \\theta_1^0\\)? (Hint: does the critical region depend on \\(\\theta_1^a\\)?)\n\n\nSolution. The critical region above does not depend on the exact value of \\(\\theta_1^a\\) because \\(P(n_1 &gt; c | H_0)\\) does not depend on \\(\\theta_1^a\\). However, it does depend on whether \\(\\theta_1^a\\) is less than or greater than \\(\\theta_1^0\\). Hence, the test is most powerful against \\(H_1: \\theta_1 &gt; \\theta_1^0\\) or \\(H_1: \\theta_1 &lt; \\theta_1^0\\), but not \\(H_1: \\theta_1 \\neq \\theta_1^0\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#footnotes",
    "href": "problemsets/midterm2022/midterm2022_solution.html#footnotes",
    "title": "ECON 626: Midterm Solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you could not answer that part, suppose you had shown that \\(\\beta = \\Er[X_i X_i']^{-1} \\Er[X_i m^{-1}(Y_i)]\\).↩︎"
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html",
    "href": "problemsets/final2023/final2023_solutions.html",
    "title": "ECON 626: Final - Solutions",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\nYou have 150 minutes to complete the exam. The last two pages have some possibly useful formulas.\nThere are 100 total points."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#fixed-effects-6-points",
    "href": "problemsets/final2023/final2023_solutions.html#fixed-effects-6-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Fixed Effects (6 points)",
    "text": "Fixed Effects (6 points)\nTo estimate the effect of the new plans, the authors estimate a two-way fixed effects regression \\[\ny_{it} = \\alpha_i + \\delta_t + \\beta T_{it} + \\epsilon_{it}\n\\] where \\(y_{it}\\) health spending by employee \\(i\\) in year \\(t\\), \\(T_{it}\\) is an indicator for having the new plans, and \\(\\alpha_i\\) and \\(\\delta_t\\) are fixed effects. Table A3 below shows estimates of \\(\\beta\\). Briefly describe a problem with these estimates.\n\nSolution. With variation in treatment timing and treatment heterogeneity, the two way fixed effects estimate, \\(\\hat{\\beta}\\) recovers a weighted sum of individual treatment effects. In some cases, these weights can be negative, and regardless, do not have a clear interpretation."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#what-should-we-have-done-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#what-should-we-have-done-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "What should we have done? (7 points)",
    "text": "What should we have done? (7 points)\nDescribe a better estimator for the effect of these new plans on spending. Be clear about what sort of average effect your estimator recovers and clearly state any assumptions needed for your estimator to have a causal interpretation.\n\nSolution. If we assume parallel trends, \\[\n\\Er[y_{it}(0) - y_{it-s}(0) | T_{it}=1, T_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | T_{it}=0, T_{it-s}=0]\n\\] Then, \\[\n\\begin{align*}\nATT_{t,t-s} & = \\Er[y_{it}(1)-y_{it}(0) | T_{it}=1,T_{it-s}=0]  \\\\\n& = \\Er[y_{it} - y_{it-s} | T_{it}=1,T_{t-s}=0] - \\Er[y_{it} - y_{it-s} | T_{it}=0,T_{t-s}=0]\n\\end{align*}\n\\] This difference in differences of conditional expectations of observed data, so it can be estimated by replacing the expectations with sample averages. \\[\n\\begin{align*}\n\\widehat{ATT}_{t,t-s} = \\frac{\\sum_i (y_{it}-y_{it-s})T_{it}(1-T_{it-s})}{\\sum_i T_{it}(1-T_{it-s})} - \\frac{\\sum_i (y_{it}-y_{it-s})(1-T_{it})(1-T_{it-s})}{\\sum_i (1-T_{it})(1-T_{it-s})}\n\\end{align*}\n\\] These estimate the average treatment effect on people who were treated at time \\(t\\) and untreated at time \\(t-s\\).\nIf we want a single estimate, we can take an average of the \\(ATT_{t,t-s}\\) for example, \\[\n\\widehat{\\bar{ATT}} = \\sum_{t=2004}^{2006} \\frac{\\sum_{i} T_{it}}{\\sum_{i,t} T_{it}} \\widehat{ATT}_{t,2003}\n\\] This is an average treatment effect on the treated weighted by the number of people treated. Other averages are also reasonable to report."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#comparing-columns-1-4",
    "href": "problemsets/final2023/final2023_solutions.html#comparing-columns-1-4",
    "title": "ECON 626: Final - Solutions",
    "section": "Comparing Columns (1) & (4)",
    "text": "Comparing Columns (1) & (4)\nThe difference in magnitude in columns (1) and (4) is striking. Assume (as is approximately true) that 1/4 of people in the sample were switched to the new plans in 2004, 1/4 in 2005, 1/4 in 2006, and 1/4 not switched.\n\nWeights (7 points) slighty more difficult\nHow does the average treatment effect in 2006 of the people who switched in 2004 — \\(\\Er[y_{i,2006}(1) - y_{i,2006}(0) | \\text{switched in 2004}]\\) — affect the fixed effects estimate of \\(\\hat{\\beta}\\)? Hint: using the notation in the “Definitions and Results”, what is \\(\\hat{\\omega}_{it}\\) for these people?\n\nSolution. We have \\[\n\\begin{align*}\n\\hat{\\omega}_{it} \\sum \\tilde{T}_{it}^2 & = T_{it} - \\bar{T}_i - \\bar{T}_t + \\bar{T} \\\\\n& = 1 - 3/4 - 3/4 + (1/4*3/4 + 1/4*1/2 + 1/4*1/4) \\\\\n& = -1/2 + 3/8 \\\\\n& = -1/4\n\\end{align*}\n\\]\nHence, \\(\\Er[y_{i,2006}(1) - y_{i,2006}(0) | \\text{switched in 2004}]\\) has a negative weight in \\(\\hat{\\beta}\\).\n\n\n\nChange in Weights with Attrition (6 points) slightly more difficult\nSuppose that switching to the new plans makes people more likely to leave Alcoa, so that column (4) has a lower portion of people switched to the new plans and higher portion not switched. How does that change the weight on \\(\\Er[y_{i,2006}(1) - y_{i,2006}(0) | \\text{switched in 2004}]\\) in \\(\\hat{\\beta}\\)?\n\nSolution. As in the previous part, \\[\n\\begin{align*}\n\\hat{\\omega}_{it} \\sum \\tilde{T}_{it}^2 & = T_{it} - \\bar{T}_i - \\bar{T}_t + \\bar{T} \\\\\n& = 1 - 3/4 - \\bar{T}_{t} + \\bar{T} \\\\\n\\end{align*}\n\\] The question says that less of the sample will be in new plans, so \\(\\bar{T}_t\\) and \\(\\bar{T}\\) will both be lower. Generally, \\(\\bar{T}_t\\) will decrease by more than \\(\\bar{T}\\). For example, suppose the untreated are half instead of one quarter of the sample in column (4), then \\(\\bar{T}_t = 3/8\\) and \\(\\bar{T} = 3/16\\). This makes \\(\\hat{\\omega}_{it} \\sum \\tilde{T}_{it}^2 = 1/16\\), so we now have a positive weight on \\(\\Er[y_{i,2006}(1) - y_{i,2006}(0) | \\text{switched in 2004}]\\). This could explain the much larger estimate in column (4)."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#ols-6-points",
    "href": "problemsets/final2023/final2023_solutions.html#ols-6-points",
    "title": "ECON 626: Final - Solutions",
    "section": "OLS (6 points)",
    "text": "OLS (6 points)\nThe authors estimate \\[\nRC_m = \\alpha + \\beta C_m + \\eta X_m + \\gamma Z_m + \\epsilon_m\n\\] where \\(RC_m\\) is a relational contracting index for mill \\(m\\), \\(C_m\\) is the number of competing mills within 10km of mill \\(m\\), \\(X_m\\) are mill characteristics, and \\(Z_m\\) are geographic characteristics of the land around the mill.\nGive one reason why \\(\\Er[C_m \\epsilon_m]\\) might not be zero, speculate on the sign of \\(\\Er[C_m \\epsilon_m]\\), and say whether whether you expect \\(\\hat{\\beta}^{OLS}\\) to be greater or less than \\(\\beta\\)."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#instrument-6-points",
    "href": "problemsets/final2023/final2023_solutions.html#instrument-6-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument (6 points)",
    "text": "Instrument (6 points)\nAs an instrument for \\(C_m\\), the authors use the predicted number of mills 5-10km away from an engineering model of coffee production. It predicts mill locations based on geography and local climate. Column (2) of Table II shows estimates of the first stage regression of \\(C_m\\) on this instrument. What assumption can we check by looking at this regression? Are you confident that this assumption is met? If not, what should be done about it? Hint:\\(F=t^2 \\approx \\left(\\frac{1.6}{0.4}\\right)^2 \\approx 16\\).\n\nSolution. The first stage F statistic indicates that the instrument might be weak. That is, the instrument might not be corrrelated with \\(C_m\\). With this small of an F-statistic we should use an identification robust inference method instead of the usual t-test in column (4). The AR test or VtF statistic should be used instead."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#dependence-6-points",
    "href": "problemsets/final2023/final2023_solutions.html#dependence-6-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Dependence (6 points)",
    "text": "Dependence (6 points)\nThe sample consists of mills in Rwanda, some of which might be near one another. Briefly, how does this affect the consistency and asymptotic distribution of the estimates of \\(\\beta\\) in Table II? What in the table, if anything, needs to be calculated differently than if observations were independent?\n\nSolution. Mills near one another likely interact with the same or similar farmers. Local variation in relational practices could then show up as spatial correlation in \\(\\epsilon_m\\). As long as the correlation is not too strong, usual estimates remain consistent. However, the asymptotic variance will be affected. The standard errors need to be calculated differently."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#more-outcomes-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#more-outcomes-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "More Outcomes (7 points)",
    "text": "More Outcomes (7 points)\nTable 3 shows estimates of the same model as table 2, except instead of the relational contracting index as the outcome, the outcome is one of the components of the index. Different columns show different components. As shown, the coefficients are all negative and most are statistically significant. Should this make us less concerned about the potential weak instrument problem? Why or why not? Hint: these regressions all have the same first stage. A really good answer to this question would be fairly precise and perhaps derive the joint distribution of these estimates under weak instrument asymptotitcs.\n\nSolution. To simplify consider a model without controls. We have 13 equations of interest \\[\ny_{jm} = \\beta_j C_m + \\epsilon_{jm}\n\\] that share a common first stage \\[\nC_m = \\pi W_m + u_m.\n\\] The IV estimates are \\[\n\\begin{align*}\n\\hat{\\beta}_j = & \\frac{W'y_j}{W'C} \\\\\n= & \\beta_j + \\frac{W'\\epsilon_j}{\\pi W'W + W'u}\n\\end{align*}\n\\] We have a weak instrument problem when the signal in the instrument – \\(\\pi W'W\\) is of the same magnitude as the noise — \\(W'u\\). This can be problematic because we might divide by something near \\(0\\) and get very noisy results. The stability of the results here and especially the fact that (13) is near 0, gives us some indication that \\(\\pi W'W +\nW'u\\) is not too close to 0. However, the signal and noise being similar magnitude can still be a problem, we could have \\(\\pi W'W +\nW'u\\) being the opposite sign or much different magnitude than \\(\\pi W'W\\). I do not think Table III helps rule out that possibility."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#estimator-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#estimator-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Estimator (7 points)",
    "text": "Estimator (7 points)\nSuppose you observe \\(\\{y_i,d_i,x_i\\}_{i=1}^n\\), and want to estimate the model \\[\ny_i = \\theta d_i + x_i'\\beta + \\epsilon_i.\n\\] Assume \\(\\Er[d \\epsilon] = 0\\), but it might be that \\(\\Er[x \\epsilon]\n\\neq 0\\). Fortunately you have an estimate of \\(\\beta\\), \\(\\hat{\\beta}\\) (say from some other dataset), that is consistent and asymptotically normal, \\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\).\nUse \\(\\hat{\\beta}\\) and the assumption that \\(\\Er[d \\epsilon]=0\\) to find an estimator for \\(\\theta\\).\n\nSolution. We can put the model into the moment condition and derive a plug-in estimator. \\[\n\\begin{align*}\n0 = & \\Er[d (y - \\theta d - x'\\beta)] \\\\\n\\theta  = & \\frac{\\Er[d(y-x'\\beta)}{\\Er[d^2]}\n\\end{align*}\n\\] so assuming \\(\\Er[d^2] \\neq 0\\), we can use the estimator \\[\n\\hat{\\theta} = \\frac{\\sum_i d_i (y_i -\nx_i'\\hat{\\beta})}{\\sum_i d_i^2}\n\\]"
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#consistency-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#consistency-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Consistency (7 points)",
    "text": "Consistency (7 points)\nAssume that the data is i.i.d. and \\(d\\), \\(x\\), and \\(\\epsilon\\) have finite second moments. Show that \\(\\hat{\\theta} = \\frac{\\sum_i d_i (y_i -\nx_i'\\hat{\\beta})}{\\sum_i d_i^2}\\) is a consistent estimate of \\(\\theta\\).\n\nSolution. \\[\n\\begin{align*}\n\\hat{\\theta} & = \\frac{\\sum_i d_i (y_i -\nx_i'\\hat{\\beta})}{\\sum_i d_i^2} \\\\\n& = \\frac{\\sum_i d_i (y_i - x_i'\\beta)}{\\sum_i d_i^2} -\n\\frac{\\sum_i d_i x_i'(\\hat{\\beta} - \\beta))}{\\sum_i d_i^2} \\\\\n& = \\theta + \\frac{\\sum_i d_i\\epsilon_i}{\\sum_i d_i^2} -\n\\frac{\\sum_i d_i x_i'(\\hat{\\beta} - \\beta))}{\\sum_i d_i^2} \\\\\n\\end{align*}\n\\] Using the Cauchy-Schwarz and triangle inequalities, we have \\[\n\\left\\vert \\frac{\\sum_i d_i x_i'(\\hat{\\beta} - \\beta))}{\\sum_i d_i^2} \\right\\vert\n\\leq \\frac{\\left(\\sum_i |d_i|\\norm{x_i} \\right) \\norm{\\hat{\\beta} - \\beta}}{\\sum_i d_i^2}\n\\] Assume that \\(\\Er[|d|\\norm{x}]\\) is finite (arguably, it’s a second moment of \\(d\\) and \\(x\\)). Then the law of large numbers applies, and \\[\n\\left\\vert \\frac{\\sum_i d_i x_i'(\\hat{\\beta} - \\beta))}{\\sum_i d_i^2} \\right\\vert \\leq o_p(1)\n\\] Additionally, by the law of large numbers, \\(\\frac{\\sum_i\n  d_i\\epsilon_i}{\\sum_i d_i^2} \\inprob \\Er[d\\epsilon]/\\Er[d^2] = 0\\).\nHence, \\(\\hat{\\theta} \\inprob \\theta\\)"
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#distribution-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#distribution-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Distribution (7 points)",
    "text": "Distribution (7 points)\nFind the asymptotic distribution of \\(\\hat{\\theta}\\).\n\nSolution. As in the previous part, \\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\theta} - \\theta)\n= & \\frac{\\frac{1}{\\sqrt{n}}\\sum_i d_i\\epsilon_i}{\\frac{1}{n} \\sum_i d_i^2} -\n\\frac{\\frac{1}{n} \\sum_i d_i x_i'}{\\frac{1}{n} \\sum_i d_i^2}\\sqrt{n}(\\hat{\\beta} - \\beta) \\\\\n\\end{align*}\n\\] Assume that \\(\\hat{\\beta}\\) is independent of \\(d_i \\epsilon_i\\). Then, using the CLT and law of large numbers, \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta) \\indist \\frac{\\xi}\\Er[d^2] - \\frac{\\Er[dx]}{\\Er[d^2]} \\xi_\\beta\n\\] where \\(\\xi \\sim N(0, \\Er[d^2 \\epsilon^2]\\) and \\(\\xi_\\beta \\sim N(0,\\Sigma)\\). Equivalently, \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta) \\indist N\\left(0 ,\\frac{1}{\\Er[d^2]^2}\\left(\\Er[d^2 \\epsilon^2] + \\Er[dx]\\Sigma \\Er[x'd]\\right)\\right)\n\\]"
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#better-estimator-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#better-estimator-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Better Estimator (7 points)",
    "text": "Better Estimator (7 points)\nSuppose that \\(\\Er[d|x] = x'\\gamma\\), and you have a consistent, asymptotically normal estimator \\(\\hat{\\gamma}\\) with \\(\\sqrt{n}(\\hat{\\gamma} - \\gamma)\n\\indist N(0,\\Gamma)\\). Also assume that \\(\\Er\\left[ (d - x'\\gamma) \\epsilon \\right] = 0\\). Show that \\(\\tilde{\\theta} = \\frac{\\sum_i (d_i - x_i'\\hat{\\gamma})(y_i - x_i'\\hat{\\beta})} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i}\\) is a consistent estimator for \\(\\theta\\).\n\nSolution. \\[\n\\begin{align*}\n\\tilde{\\theta} = & \\frac{\\sum_i (d_i - x_i'\\hat{\\gamma})(y_i - x_i'\\hat{\\beta})} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i} \\\\\n= & \\theta - \\frac{\\sum_i (d_i - x_i'\\hat{\\gamma})x_i'(\\hat{\\beta} - \\beta)} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i}\n+ \\frac{\\sum_i (d_i - x_i'\\hat{\\gamma})\\epsilon_i} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i} \\\\\n= & \\theta - \\frac{\\sum_i (d_i - x_i'\\gamma)x_i'(\\hat{\\beta} - \\beta)} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i}\n+  \\frac{\\sum_i x_i'(\\hat{\\gamma} - \\gamma)x_i'(\\hat{\\beta} - \\beta)} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i}\n+ \\frac{\\sum_i (d_i - x_i'\\gamma)\\epsilon_i} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i}\n- \\frac{\\sum_i (x_i'(\\hat{\\gamma} - \\gamma))\\epsilon_i} {\\sum_i (d_i - x_i'\\hat{\\gamma})d_i}\n\\end{align*}\n\\]\nAfter multiplying the numerator and denominator by \\(1/n\\), we can write the denominator as \\[\n\\begin{align*}\n\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i = & \\frac{1}{n} \\sum_i (d_i - x_i'\\gamma)d_i - \\frac{1}{n} \\sum_i  d_ix_i'(\\hat{\\gamma} -\\gamma) \\\\\n\\inprob \\Er[(d-x'\\gamma)d]\n\\end{align*}\n\\]\nEach term in the numerator is either an average of a mean zero random variable, or an average times estimation error in \\(\\beta\\) or \\(\\gamma\\). Thus, \\[\n\\tilde{\\theta} \\inprob \\theta\n\\]"
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#why-is-it-better-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#why-is-it-better-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Why is it Better? (7 points)",
    "text": "Why is it Better? (7 points)\nFind the asymptotic distribution of \\(\\tilde{\\theta}\\).\n\nSolution. Using the algebra from the previous part, we have \\[\n\\begin{align*}\n\\sqrt{n}(\\tilde{\\theta} - \\theta) = &\n- \\frac{\\frac{1}{n} \\sum_i (d_i - x_i'\\gamma)x_i' \\sqrt{n}(\\hat{\\beta} - \\beta)} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i}  \\\\\n& +  \\frac{(\\hat{\\gamma} - \\gamma)' \\frac{1}{n} \\sum_i x_ix_i' \\sqrt{n} (\\hat{\\beta} - \\beta)} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i} \\\\\n& + \\frac{\\frac{1}{\\sqrt{n}} \\sum_i (d_i - x_i'\\gamma)\\epsilon_i} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i}  \\\\\n& - \\frac{ \\sqrt{n}(\\hat{\\gamma} - \\gamma)) \\frac{1}{n} \\sum_i (x_i'\\epsilon_i} {\\frac{1}{n}\\sum_i (d_i - x_i'\\hat{\\gamma})d_i} \\\\\n= & \\frac{\\frac{1}{\\sqrt{n}} \\sum_i (d_i - x_i'\\gamma)\\epsilon_i} {\\frac{1}{n} \\sum_i (d_i - x_i'\\gamma)d_i}  + o_p(1)\n\\end{align*}\n\\] Note how each time \\(\\hat{\\beta} - \\beta\\) or \\(\\hat{\\gamma}-\\gamma\\) appear, they get multiplied by something else that is \\(O_p(n^{-1/2})\\) and so vanish from the final line.\nApplying the CLT and LLN, we have \\[\n\\sqrt{n}(\\tilde{\\theta} - \\theta) \\indist N(0, \\Er[(d - x'\\gamma)^2 \\epsilon^2]/\\Er[(d-x'\\gamma)d]).\n\\] This is the same distribution as if we knew \\(\\beta\\) and \\(\\gamma\\) and plugged them in."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#whats-really-great-about-it-7-points-more-difficult",
    "href": "problemsets/final2023/final2023_solutions.html#whats-really-great-about-it-7-points-more-difficult",
    "title": "ECON 626: Final - Solutions",
    "section": "What’s really Great About it? (7 points) more difficult",
    "text": "What’s really Great About it? (7 points) more difficult\nSuppose that \\(\\hat{\\gamma}\\) and \\(\\hat{\\beta}\\) are not \\(\\sqrt{n}\\) asymptotically normal. Instead, you only know that \\(\\norm{\\hat{\\gamma} - \\gamma} = o_p(n^{-1/4})\\) and \\(\\norm{\\hat{\\beta} - \\beta} = o_p(n^{-1/4})\\). Show that the asymptotic distribution of \\(\\sqrt{n}(\\tilde{\\theta} - \\theta)\\) is the same as in the previous part.\n\nSolution. In the previous part, we can redistribute the \\(n^{1/2}\\) to write \\[\n\\begin{align*}\n\\sqrt{n}(\\tilde{\\theta} - \\theta) = &\n- \\frac{\\frac{1}{n^{3/4}} \\sum_i (d_i - x_i'\\gamma)x_i' n^{1/4}(\\hat{\\beta} - \\beta)} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i}  \\\\\n& +  \\frac{n^{1/4}(\\hat{\\gamma} - \\gamma)' \\frac{1}{n} \\sum_i x_ix_i' n^{1/4} (\\hat{\\beta} - \\beta)} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i} \\\\\n& + \\frac{\\frac{1}{n^{1/2}} \\sum_i (d_i - x_i'\\gamma)\\epsilon_i} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i}  \\\\\n& - \\frac{ n^{1/4}(\\hat{\\gamma} - \\gamma)) \\frac{1}{n^{3/4}} \\sum_i (x_i'\\epsilon_i} {\\frac{1}{n}\\sum_i (d_i - x_i'\\hat{\\gamma})d_i} \\\\\n= & \\frac{\\frac{1}{n^{1/2}} \\sum_i (d_i - x_i'\\gamma)\\epsilon_i} {\\frac{1}{n} \\sum_i (d_i - x_i'\\hat{\\gamma})d_i}  + o_p(1)\n\\end{align*}\n\\] where we used the fact that terms like \\(\\frac{1}{n^{1/2}} \\sum_i (d_i\n- x_i'\\gamma)x_i' = O_p(1)\\), so \\(\\frac{1}{n^{3/4}} \\sum_i (d_i\n- x_i'\\gamma)x_i' \\inprob 0\\), and \\[\nn^{1/4}(\\hat{\\gamma} - \\gamma)' \\frac{1}{n} \\sum_i x_ix_i' n^{1/4} (\\hat{\\beta} - \\beta) = o_p(1) \\Er[xx'] o_p(1) = o_p(1)\n\\]\nThus, we see that even if \\(\\gamma\\) and \\(\\beta\\) are estimated at slow rates, \\(\\tilde{\\theta}\\) can be \\(\\sqrt{n}\\) asymptotically normal."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#fixed-effects-inconsistent-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#fixed-effects-inconsistent-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Fixed Effects Inconsistent (7 points)",
    "text": "Fixed Effects Inconsistent (7 points)\nShow that for \\(T\\) fixed and \\(N \\to \\infty\\), the fixed effects estimator, i.e. regressing \\(y_{it}\\) on \\(y_{it-1} - \\bar{y}_i\\) and \\(x_{it} - \\bar{x}_i\\), is not consistent. Hint: what is \\(\\Er[\\bar{y}_i u_{it}]\\)?\n\nSolution. After partialing out the fixed effects, \\(y_{it-1} - \\bar{y}_i\\) will be correlated with \\(u_{it}\\). We can be more specific with some additional assumptions. Suppose \\(u_{it}\\) is uncorrelated over time. Then, \\[\n\\Er[\\bar{y}_i u_{iT}] = \\frac{1}{T} \\Er[u_{iT}^2]\n\\]\n\\[\n\\Er[\\bar{y}_i u_{iT-1}] = \\frac{1}{T} (1 + \\rho)\\Er[u_{i{T-1}}^2]\n\\]\n\\[\n\\Er[\\bar{y}_i u_{iT-2}] = \\frac{1}{T} (1 + \\rho + \\rho^2)\\Er[u_{i{T-2}}^2]\n\\] and so on.\nIf there were no \\(x_{it}\\) in the model, we would have \\[\n\\begin{align*}\n\\hat{\\rho} = & \\rho + \\frac{\\sum_{it} (y_{it} - \\bar{y})u_{it}}{\\sum_{it} (y_{it} - \\bar{y})^2} \\\\\n\\inprob & \\rho - \\frac{\\frac{1}{T}\\sum_t \\Er[\\bar{y} u_{it}]}{\\Er[(y_{it} - \\bar{y})^2]} = \\rho - \\frac{bias}{T}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#first-differences-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#first-differences-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "First Differences (7 points)",
    "text": "First Differences (7 points)\nLet \\(\\Delta y_{it} = y_{it} - y_{it-1}\\). Take differences of the model to eliminate \\(\\alpha_i\\), leaving \\[\n\\Delta y_{it} = \\rho \\Delta y_{it-1} + \\Delta x_{it}'\\beta + \\Delta u_{it}.\n\\] Will OLS on this equation be consistent?\n\nSolution. OLS will not be consistent because \\(\\Er[\\Delta y_{it-1} \\Delta u_{it}]\n= \\Er[(y_{it-1} - y_{it-2})(u_{it} - u_{it-1}) = \\Er[-y_{it-1} u_{it-1}]\n\\neq 0\\)."
  },
  {
    "objectID": "problemsets/final2023/final2023_solutions.html#gmm-7-points",
    "href": "problemsets/final2023/final2023_solutions.html#gmm-7-points",
    "title": "ECON 626: Final - Solutions",
    "section": "GMM (7 points)",
    "text": "GMM (7 points)\nAssume \\(T \\geq 3\\). Argue that \\(\\Er[\\Delta u_{it} y_{it-\\ell}]=0\\) for \\(\\ell \\geq 2\\), and that \\(\\Er[\\Delta y_{it-1} y_{it-\\ell}] \\neq 0\\). Use this fact, along with the assumptions above to derive a GMM estimator for \\(\\rho\\) and \\(\\beta\\).\n\nSolution. The weak exogeneity assumption implies \\(\\Er[\\Delta u_{it} y_{it-\\ell}]=0\\) for \\(\\ell \\geq 2\\).\nThe model implies past \\(y\\) are correlated with future \\(y\\), and so \\(\\Er[\\Delta y_{it-1} y_{it-\\ell}] \\neq 0\\). We can use the moments conditions \\[\n\\begin{align*}\n0 = & \\begin{pmatrix} \\Er[\\Delta u_{it} y_{it-2}] \\\\ \\Er[\\Delta u_{it} \\Delta x_{it}] \\end{pmatrix}\n\\end{align*}\n\\] This gives \\(1 +\\) dimension of \\(x\\) moment conditions to estimate \\(\\rho\\) and \\(\\beta\\). To make an estimator, we minimize the empirical moments \\[\n\\begin{align*}\n(\\hat{\\rho},\\hat{\\beta}) \\in \\mathrm{arg}\\min_{\\rho,\\beta} & \\left(\\frac{1}{N(T-2)} \\sum_{i=1,t=3}^{N,T} (\\Delta y_{it} - \\rho \\Delta y_{it-1} \\Delta - x_{it}' \\beta) y_{it-2}\\right)^2 + \\\\\n& +\n\\norm{\\frac{1}{N(T-2)} \\sum_{i=1,t=3}^{N,T} (\\Delta y_{it} - \\rho \\Delta y_{it-1} - \\Delta x_{it}' \\beta) \\Delta x_{it}}^2\n\\end{align*}\n\\] Additional moments could be also be used (more lags of \\(y\\), \\(\\Er[\\Delta u_{it} \\Delta x_{is}]=0\\) for \\(t\\neq s\\))."
  },
  {
    "objectID": "problemsets/08/ps08.html",
    "href": "problemsets/08/ps08.html",
    "title": "ECON 626: Problem Set 8",
    "section": "",
    "text": "\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "problemsets/08/ps08.html#ols-is-inconsistent",
    "href": "problemsets/08/ps08.html#ols-is-inconsistent",
    "title": "ECON 626: Problem Set 8",
    "section": "OLS is Inconsistent",
    "text": "OLS is Inconsistent\nLet \\(\\hat{\\beta}^{OLS} = \\frac{\\sum X_i Y_i}{\\sum X_i^2}\\) be the least squares estimator. Let \\(\\pi = P(X_i^*=1)\\). Compute \\(\\plim \\hat{\\beta}^{OLS}\\) in terms of \\(p\\), \\(\\pi\\), and \\(\\beta\\)."
  },
  {
    "objectID": "problemsets/08/ps08.html#instrument",
    "href": "problemsets/08/ps08.html#instrument",
    "title": "ECON 626: Problem Set 8",
    "section": "Instrument?",
    "text": "Instrument?\nSuppose you also observe \\(Z_i \\in \\{0,1\\}\\) with \\[\nP(Z_i = 1 | X_i^* = 1) = P(Z_i=0 | X_i^*=0) = q\n\\] where \\(q&gt;1/2\\), and \\(Z_i\\) and \\(X_i\\) are independent conditional on \\(X_i^*\\). Let \\(\\hat{\\beta}^{IV} = \\frac{\\sum Z_i Y_i}{\\sum Z_i X_i}\\) be the instrumental variable estimator. Compute \\(\\plim \\hat{\\beta}^{IV}\\)"
  },
  {
    "objectID": "problemsets/08/ps08.html#or-something-else",
    "href": "problemsets/08/ps08.html#or-something-else",
    "title": "ECON 626: Problem Set 8",
    "section": "Or Something Else?",
    "text": "Or Something Else?\nDescribe how \\(X\\), \\(Y\\), and \\(Z\\) could be used to estimate \\(\\beta\\).\nHint: think of \\(\\beta\\), \\(p\\), \\(q\\), and \\(\\pi = P(X^*_i = 1)\\) as four parameters to estimate, and come up with four moment conditions that involve these parameters."
  },
  {
    "objectID": "problemsets/08/ps08.html#first-differences",
    "href": "problemsets/08/ps08.html#first-differences",
    "title": "ECON 626: Problem Set 8",
    "section": "First Differences",
    "text": "First Differences\nLet \\(\\Delta y_{it} = y_{it} - y_{it-1}\\). Take differences of the model to eliminate \\(\\alpha_i\\), leaving \\[\n\\Delta y_{it} = \\rho \\Delta y_{it-1} + \\Delta x_{it}'\\beta + \\Delta u_{it}.\n\\] Will OLS on this equation be consistent?"
  },
  {
    "objectID": "problemsets/08/ps08.html#fixed-effects-inconsistent",
    "href": "problemsets/08/ps08.html#fixed-effects-inconsistent",
    "title": "ECON 626: Problem Set 8",
    "section": "Fixed Effects Inconsistent",
    "text": "Fixed Effects Inconsistent\nShow that for \\(T\\) fixed and \\(N \\to \\infty\\), the fixed effects estimator, i.e. regressing \\(y_{it}\\) on \\(y_{it-1} - \\bar{y}_i\\) and \\(x_{it} - \\bar{x}_i\\), is not consistent. Hint: what is \\(\\Er[\\bar{y}_i u_{it}]\\)?"
  },
  {
    "objectID": "problemsets/08/ps08.html#gmm",
    "href": "problemsets/08/ps08.html#gmm",
    "title": "ECON 626: Problem Set 8",
    "section": "GMM",
    "text": "GMM\nAssume \\(T \\geq 3\\). Argue that \\(\\Er[\\Delta u_{it} y_{it-\\ell}]=0\\) for \\(\\ell \\geq 2\\), and that \\(\\Er[\\Delta y_{it-1} y_{it-\\ell}] \\neq 0\\). Use this fact, along with the assumptions above to derive a GMM estimator for \\(\\rho\\) and \\(\\beta\\)."
  },
  {
    "objectID": "problemsets/07/ps07.html",
    "href": "problemsets/07/ps07.html",
    "title": "ECON 626: Problem Set 7",
    "section": "",
    "text": "\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "problemsets/07/ps07.html#twfe",
    "href": "problemsets/07/ps07.html#twfe",
    "title": "ECON 626: Problem Set 7",
    "section": "TWFE",
    "text": "TWFE\nChen and French (2023) begin by estimating a two-way fixed effects (TWFE) model, \\[\nF_{it} = \\alpha + \\beta_1 MM_{it} + \\beta_2 RM_{it} + X_{it}\\gamma + \\eta_i + \\theta_t + \\varepsilon_{it}\n\\] where \\(F_{it}\\) is log traffic fatalaties per 100,000 in state \\(i\\) and year \\(t\\), \\(MM_{it}\\) is an indicator for medical marijuana being legal, \\(RM_{it}\\) is an indicator for legalized recreational marijuana, and \\(X_{it}\\) are state covariates including traffic volume, economic conditions, and demographics. Chen and French (2023) present estimates of the TWFE model “for comparison with the literature,” but also compute other estimators. Why? What is a problem with TWFE here?"
  },
  {
    "objectID": "problemsets/07/ps07.html#multiple-treatments",
    "href": "problemsets/07/ps07.html#multiple-treatments",
    "title": "ECON 626: Problem Set 7",
    "section": "Multiple Treatments",
    "text": "Multiple Treatments\nSuppose \\(1/2\\) of states legalized medical marijuana in a single year and no other changes occurred in \\(MM_{it}\\). Also suppose \\(1/4\\) of states legalized recretaional marijuana in a single later year and no other changes occurred in \\(RM_{it}\\). That is, there is no variation in treatment timing, but there are multiple treatments. For simplicity, ignore covariates. Would estimating a TWFE model, \\[\nF_{it} = \\alpha + \\beta_1 MM_{it} + \\beta_2 RM_{it} + \\eta_i + \\theta_t + \\varepsilon_{it}\n\\] recover an interpretable average treatment effect on the treated? Your answer should include some algebra or perhaps computer computations relating \\(\\hat{\\beta}_1\\) and/or \\(\\hat{\\beta}_2\\) to potential outcomes. Feel free to make reasonable additional assumptions to help simplify the analysis."
  },
  {
    "objectID": "problemsets/07/ps07.html#did_m-did_ell",
    "href": "problemsets/07/ps07.html#did_m-did_ell",
    "title": "ECON 626: Problem Set 7",
    "section": "\\(DID_M\\), \\(DID_\\ell\\)",
    "text": "\\(DID_M\\), \\(DID_\\ell\\)\nChen and French (2023) also report the \\(DID_M\\) and \\(DID_\\ell\\) estimators? What are these? What sort of average treatment effect do they estimate? (You will need to read Chen and French (2023) and perhaps also Chaisemartin and D’Haultfœuille (2020) to answer this.)"
  },
  {
    "objectID": "problemsets/07/ps07.html#results",
    "href": "problemsets/07/ps07.html#results",
    "title": "ECON 626: Problem Set 7",
    "section": "Results",
    "text": "Results\nTable 2 and Figure 1 show the main results of the paper. What conclusions would you draw from these?\n\n\n\ntable 2\n\n\n\n\n\nfigure 1"
  },
  {
    "objectID": "problemsets/05/ps05.html",
    "href": "problemsets/05/ps05.html",
    "title": "ECON 626: Problem Set 5",
    "section": "",
    "text": "\\[\n\\def\\indep{{\\perp\\!\\!\\!\\perp}}\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\]\n\nProblem 1\nSuppose \\(X_n = O_p(a_n)\\) and \\(Y_n - c = O_p(a_n)\\) for \\(c \\neq 0\\) and \\(a_n \\to 0\\). Show that \\(\\frac{X_n}{Y_n} = O_p(a_n)\\).\nHint: \\(\\frac{X_n}{Y_n} = \\frac{X_n}{c} + \\frac{X_n}{Y_n} (c - Y_n) \\frac{1}{c}\\), use Lemma 3.3 and Exericse 3.1 from Song (2021).\n\n\nProblem 2\nSuppose that each individual person \\(i\\) has potential outcomes \\(Y_i(1)\\) and \\(Y_i(0)\\) depending on the treated state or the untreated state. The average treatment effect is defined to be \\[\n    \\tau = \\Er[Y_i(1) - Y_i(0)].\n\\] We assume that the econometrician observes \\((Y_i,D_i)\\), where \\(Y_i = D_i Y_i(1) + (1 - D_i)Y_i(0)\\), \\(D_i\\) is a binary variable representing the treatment status, and that \\((Y_(1),Y_i(0))\\) is independent of \\(D_i\\). Finally, assume that \\(0 &lt; \\Er[D_i] &lt;1\\), \\(Y_i \\in [0,1]\\) for all \\(i =1,...,n\\), and that \\((Y_i(1),Y_i(0),D_i)\\) are i.i.d. across \\(i\\)’s.\n\nShow that \\[\n\\hat{\\tau} = \\frac{\\sum_{i=1}^n Y_i D_i}{\\sum_{i=1}^n D_i} - \\frac{\\sum_{i=1}^n Y_i (1-D_i)}{\\sum_{i=1}^n (1-D_i)}\n\\] is a consistent estimator for \\(\\tau\\).\nShow that \\(\\hat{\\tau} - \\tau = O_p(n^{-1/2})\\)\nShow that \\(\\sqrt{n}(\\hat{\\tau}-\\tau) \\to^d N(0,\\sigma_\\tau^2)\\) and calculate \\(\\sigma_\\tau^2\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”"
  },
  {
    "objectID": "problemsets/03/ps03.html",
    "href": "problemsets/03/ps03.html",
    "title": "ECON 626: Problem Set 3",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\]\n\nProblem 1\nIn the linear model, \\(Y = \\alpha + \\beta X + \\epsilon\\), assume \\(\\mathrm{Var}(X)&gt;0\\) and \\(\\Er[\\epsilon] = 0\\). Show that without any more assumptions, \\(\\beta\\) is not identified by finding values of \\(\\beta\\) that are observationally equivalent.\n\n\nProblem 2\nSuppose \\(Y_i = m(X_i' \\beta + u_i)\\) for \\(i=1, ... , n\\) with \\(X_i \\in \\R^k\\) and \\(m:\\R \\to \\R\\) is a known function. Throughout, assume that observations are independent across \\(i\\), \\(\\Er[u] =0\\), and \\(u\\) is independent of \\(X\\), and \\(\\Er[XX']\\) is nonsingular. \\(Y\\) and \\(X\\) are observed, but \\(u\\) is not.\n\nIf \\(m\\) is strictly increasing show that \\(\\beta\\) is identified by explicitly writing \\(\\beta\\) as a function of the distribution of \\(X\\) and \\(Y\\).\nSuppose \\(m(z) = 1\\{z \\geq 0\\}\\). For simplicitly, let \\(k=1\\) and \\(X_i \\in \\{-1, 1\\}\\). Show that \\(\\beta\\) is not identified by finding an observationally equivalent \\(\\tilde{\\beta}\\).\n\n\n\nProblem 3\nSong (2021) Chatper 4, exercise 1.3.\nConsider the binary choice model in Example 1.3 and assume that \\(\\tilde{\\beta}_0\\) and \\(\\tilde{\\beta}_1\\) have appropriate estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Provide a sample analogue estimator of the average derivative, replacing \\(\\tilde{\\beta}_0\\) and \\(\\tilde{\\beta}_1\\) with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\n\n\n\n\n\nReferences\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”"
  },
  {
    "objectID": "problemsets/01/ps01.html",
    "href": "problemsets/01/ps01.html",
    "title": "ECON 626: Problem Set 1",
    "section": "",
    "text": "Problem 1\nLet \\(\\Omega = \\{a,b,c,d\\}\\).\n\nIs \\(\\{\\{a\\}, \\{c\\}, \\{a,b\\}, \\emptyset\\}\\) a \\(\\sigma\\)-field?\nWhat is the smallest \\(\\sigma\\)-field containing \\(\\{\\{a,b\\},\\{a,c\\}\\}\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 2\nSong (2021) exercise 4.1.\nFor a collection \\(\\mathscr{C}\\) of sets, we write \\(\\sigma (\\mathscr{C})\\) to denote the smallest \\(\\sigma\\)-field that contains \\(\\mathscr{C}\\), and say that \\(\\sigma (\\mathscr{C})\\) is the \\(\\sigma\\)-field generated by \\(\\mathscr{C}\\).\n\nLet \\(X\\) be a random variable on \\((\\Omega ,\\mathscr{F})\\) and let \\(\\mathscr{G}\\) be the collection of the sets of the form \\(\\{\\omega \\in \\Omega :X(\\omega )\\in B\\}\\) with \\(B\\in \\mathscr{B}(\\mathbf{R})\\). Then show that \\(\\mathscr{G}\\) is a \\(\\sigma\\)-field.\nOptional, more difficult Show that \\(\\{X^{-1}(A):A\\in \\sigma (\\mathscr{C})\\}=\\sigma(\\{X^{-1}(A):A\\in \\mathscr{C}\\})\\) for any subset \\(\\mathscr{C}\\) of \\(\\mathscr{B}(\\mathbf{R})\\).\n\n\n\nProblem 3\nSuppose that \\(f_n(x) \\to f(x)\\) and \\(f_n \\geq 0\\), \\(\\int f_n d\\mu = 1\\) for all \\(n\\) and \\(f \\geq 0\\), \\(\\int f d\\mu = 1\\). Use Jensen’s inequality and the dominated convergence theorem to show that for any measurable set \\(A\\), \\(\\int_A f_n d\\mu \\to \\int_A f d\\mu\\).\n\n\n\n\n\n\nTip\n\n\n\nThe fact that \\[\n|f(x) - f_n(x)| = f_n(x) - f(x) + 2max\\{0, f(x) - f_n(x)\\}\n\\] might be useful.\n\n\n\n\nProblem 4\nShow that if for some \\(u &gt; 0\\), \\(E[|X|^u ] &lt; \\infty\\) and \\(E[|Y|^u ] &lt;\n\\infty\\), then for any \\(q \\in (0, u/2)\\), \\[\n\\lim_{a \\to \\infty} a^{q} P \\left(|XY| &gt; a\\right) = 0\n\\]\n\n\n\n\n\n\nTip\n\n\n\nUse Markov’s inequality and the Cauchy-Schwarz inequality.\n\n\n\n\nProblem 5\nConsider repeatedly performing the same experiment \\(n\\) times and recording some measurement.\nEach trial, the outcome is a random variable \\(X_i\\) with sigma-field \\(\\mathscr{B}(\\mathbf{R})\\) and distribution \\(P_X\\). The outcome of each trial has no influence on any other.\n\nShow that there is a unique measure on \\(\\mathscr{B}(\\mathbf{R}^n)\\) such that \\(P_n(A_1 \\times A_2 \\times \\cdots \\times A_n) = P_X(A_1)P_X(A_n) \\cdots P_X(A_n)\\) for all \\(A_1, ..., A_n \\in \\mathscr{B}(\\mathbf{R})\\).\n\n\n\n\n\n\n\nTip\n\n\n\nUse Carathéodary’s extension theorem.\n\n\n\nSuppose \\(\\mathrm{E}[X_1] = \\mu\\) and \\(P(|X_1-\\mu| &gt; b) = 0\\). Show that \\(P\\left( \\left\\vert \\frac{1}{n} \\sum_{i=1}^n X_i - \\mu \\right\\vert &gt; \\epsilon \\right) \\leq \\frac{b^2}{n \\epsilon^2}\\)\n\n\n\n\n\n\nReferences\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”"
  },
  {
    "objectID": "measure/measure.html#references",
    "href": "measure/measure.html#references",
    "title": "Measure",
    "section": "References",
    "text": "References\n\nSong (2021) chapter 1\nWatt (2022) chapter 8\nSpanos (2019) chapters 1 & 2\nPollard (2002)\nTao (2011)"
  },
  {
    "objectID": "measure/measure.html#why-measure-theory",
    "href": "measure/measure.html#why-measure-theory",
    "title": "Measure",
    "section": "Why Measure Theory?",
    "text": "Why Measure Theory?\n\nSimplifies some arguments\n\nExample from Pollard (2002), define independence as factorization of distribution functions \\[ P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) P(Y \\leq y) \\]\nIf \\(X_1,X_2,X_3, X_4\\) are independent, show that \\[ Y = X_1 X_2 \\log\\left(\\frac{X_1^2 + X_2^3}{|X_1| + |X_2|}\\right) \\] is independent of \\[ Z =  sin\\left(X_3 + X_3^2 + X_3X_4 + X_4^2 \\right) \\]\n\n\n\\[\n\\def\\R{\\mathbb{R}}\n\\def\\B{\\mathscr{B}}\n\\]"
  },
  {
    "objectID": "measure/measure.html#why-measure-theory-1",
    "href": "measure/measure.html#why-measure-theory-1",
    "title": "Measure",
    "section": "Why Measure Theory?",
    "text": "Why Measure Theory?\n\nSimplifies some arguments\nUnifies treatment\n\ndiscrete vs continuous\nuni- vs multi-variate\n\nResolves some difficulties with infinity\n\n\nOne difficulty with infinity in probability is defining probability over uncountably infinite sets. In probability, we start with a sample space, say \\(\\Omega\\), and then say a probability is a mapping from subsets of \\(\\Omega\\) to \\([0,1]\\) satisfying some conditions. One question is can we define such a probability on all subsets of \\(\\Omega\\). If \\(\\Omega\\) is uncountable, then the answer no. We have to limit our attention to certain well behaved subsets. These turn out to be exactly a \\(\\sigma\\)-field, which we now define.\nAnother important difficulty with infinity is that the limit of bounded Riemann integrable functions need not be Riemann integrable. This concerns us because limits of random variables, which are functions, are essential for asymptotic theory and practical statistics."
  },
  {
    "objectID": "measure/measure.html#measure-space",
    "href": "measure/measure.html#measure-space",
    "title": "Measure",
    "section": "Measure Space",
    "text": "Measure Space\n\nA set \\(\\Omega\\)\nA collection of subsets, \\(\\mathscr{F}\\), of \\(\\Omega\\) that is a \\(\\sigma\\)-field (aka \\(\\sigma\\)-algebra) , that is\n\n\\(\\Omega \\in \\mathscr{F}\\)\nIf \\(A \\in \\mathscr{F}\\), then \\(A^c \\in \\mathscr{F}\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\), then \\(\\cup_{j=1}^\\infty A_j \\in \\mathscr{F}\\)\n\nA measure, \\(\\mu: \\mathcal{F} \\to [0, \\infty]\\) s.t.\n\n\\(\\mu(\\emptyset) = 0\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\) are pairwise disjoint, then \\(\\mu\\left(\\cup_{j=1}^\\infty A_j \\right) = \\sum_{j=1}^\\infty \\mu(A_j)\\)\n\n\n\nWhy do we need sigma fields? We need them because we cannot assign a sensible measure to all subsets. Construct the set \\(V\\) as follows. Let \\(x \\sim y\\) iff \\(x-y \\in \\mathbb{Q}\\). Form \\(V\\) by choosing one element from each equivalence class in \\([0,1)\\). For \\(x,y \\in [0,1]\\), let \\(+_1\\) be addition modulo \\(1\\), so \\(x +_1 y = x+y\\) if \\(x+y \\in [0,1]\\) and \\(x+y - 1\\) otherwise. Translation invariance implies \\(\\mu(V +_1 x) = \\mu(V)\\), but \\(\\mu\\left([0,1)\\right) = \\mu\\left( \\cup_{q \\in \\mathbb{Q} \\cap [0,1)} V +_1 q \\right) = \\sum_{q \\in \\mathbb{Q} \\cap [0,1)} \\mu(V +_1 q)\\) and then either \\(\\mu\\left([0,1))\\right) = 0\\) or \\(\\infty\\).\nExamples of sigma fields."
  },
  {
    "objectID": "measure/measure.html#generated-sigma-field",
    "href": "measure/measure.html#generated-sigma-field",
    "title": "Measure",
    "section": "Generated \\(\\sigma\\)-field",
    "text": "Generated \\(\\sigma\\)-field\nGiven \\(\\mathcal{C} \\subseteq 2^\\Omega\\), the \\(\\sigma\\)-field generated by \\(\\mathcal{C}\\) is the intersection of all \\(\\sigma\\)-fields containing \\(\\mathcal{C}\\)\n\n\n\nExercise\n\n\nIf \\(\\Omega = \\{a, b, c\\}\\) and \\(\\mathcal{C} = \\{ \\{a, b\\} \\}\\), what is \\(\\sigma(\\mathcal{C})\\)?"
  },
  {
    "objectID": "measure/measure.html#carathéodarys-extension-theorem",
    "href": "measure/measure.html#carathéodarys-extension-theorem",
    "title": "Measure",
    "section": "Carathéodary’s Extension Theorem",
    "text": "Carathéodary’s Extension Theorem\n\n\n\nTheorem\n\n\nIf \\(\\mathcal{C}\\) is a ring of sets, i.e.\n\n\\(\\emptyset \\in \\mathcal{C}\\)\nFor all \\(A,B \\in \\mathcal{C}\\), \\(A \\cup B \\in \\mathcal{C}\\) and \\(A \\setminus B \\in \\mathcal{C}\\)\n\nand \\(\\mu: \\mathcal{C} \\to \\R\\) is a pre-measure, i.e.\n\n\\(\\mu(\\emptyset) = 0\\)\nFor any \\(A = \\cup_{i=1}^\\infty A_i\\) where \\(A_i\\) are disjoint, \\(\\mu(A) = \\sum_{i=1}^\\infty \\mu(A_i)\\)\n\nthen there exists a measure \\(\\mu': \\sigma(\\mathcal{C}) \\to \\R\\).\n\n\n\n\nThis ensures existence of measures. It is not too difficult to specify a pre-measure on a ring of sets, and this theorem ensures that measures extends to a \\(\\sigma\\)-field.\nIf \\(\\mu\\) is \\(\\sigma\\)-finite, then the extension is unique."
  },
  {
    "objectID": "measure/measure.html#finite-measure",
    "href": "measure/measure.html#finite-measure",
    "title": "Measure",
    "section": "Finite Measure",
    "text": "Finite Measure\n\nMeasure \\(\\mu\\) is finite if \\(\\mu(\\Omega)\\) is finite\n\\(\\mu\\) is \\(\\sigma\\)-finite if \\(\\exists\\) \\(\\{A_n\\}_{n=1}^\\infty \\in\n\\mathscr{F}\\) s.t. \\(\\mu(A_n)\\) is finite \\(\\forall n\\) and \\(\\cup_{n=1}^\\infty A_n = \\Omega\\)\n\n\n\n\nExercise\n\n\nLet \\(\\Omega\\) be countable with any \\(\\mathscr{F}\\), define \\(\\mu(A)\\) as the number of elements of \\(A\\). Show \\(\\mu\\) is \\(\\sigma\\) finite."
  },
  {
    "objectID": "measure/measure.html#lebesgue-measure",
    "href": "measure/measure.html#lebesgue-measure",
    "title": "Measure",
    "section": "Lebesgue Measure",
    "text": "Lebesgue Measure\n\n\n\nTheorem\n\n\nThere exists a unique \\(\\sigma\\)-finite measure \\(\\mu\\) on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that for any \\(a\\leq b\\) with \\(a,b\\in \\mathbf{R}\\), \\[\n\\mu ((a,b])=b-a\n\\]"
  },
  {
    "objectID": "measure/measure.html#measurable-function",
    "href": "measure/measure.html#measurable-function",
    "title": "Measure",
    "section": "Measurable Function",
    "text": "Measurable Function\n\nGiven a topology on \\(\\Omega\\), the Borel \\(\\sigma\\)-field, \\(\\mathscr{B}(\\Omega)\\), is the smallest \\(\\sigma\\)-field containing all open subsets of \\(\\Omega\\)\n\n\nWhen working with \\(R^d\\), we will generally use \\(\\B(\\R^d)\\) as our \\(\\sigma\\)-field.\n\n\n\\(f: \\Omega \\to \\R\\) is (\\(\\mathscr{F}\\)-)measurable if \\(\\forall\\) \\(B \\in \\B(\\R)\\), \\(f^{-1}(B) \\in \\mathscr{F}\\)"
  },
  {
    "objectID": "measure/measure.html#measurability-and-continuity",
    "href": "measure/measure.html#measurability-and-continuity",
    "title": "Measure",
    "section": "Measurability and Continuity",
    "text": "Measurability and Continuity\n\n\n\nLemma\n\n\nIf \\(f: \\R^n \\to \\R^k\\) is continuous, then \\(f\\) is \\(\\B(\\R^n)\\) measurable.\n\n\n\n\nTo prove this, we show another useful claim. Let \\((X,\\mathcal{A})\\) and \\((Y, \\sigma(\\mathcal{C}))\\) be measure spaces. Then, \\(f:X \\to Y\\) is measurable if \\(f^{-1}(C) \\in \\mathcal{A}\\) for all \\(C \\in \\mathcal{C}\\). The lemma on the slide immediately follows since \\(\\B(\\R^k)\\) is generated by open sets and continuous pre-images of open-sets are open.\nTo prove the claim, note that \\(\\mathcal{D} = \\{ B \\in Y : f^{-1}(B) \\in \\mathcal{A}\\}\\) is a \\(\\sigma\\)-field on \\(Y\\). Also, if \\(f^{-1}(C) \\in \\mathcal{A}\\) for all \\(C \\in \\mathcal{C}\\), then \\(\\mathcal{C} \\subseteq \\mathcal{D}\\). Thus, \\(\\sigma(\\mathcal{C}) \\subseteq \\mathcal{D}\\), so \\(f\\) is measurable.\n\n\n\n\n\nCorollary\n\n\nIf \\(f: \\Omega \\to \\R\\) is measurable and \\(g: \\R \\to \\R\\) is continuous, then \\(g \\circ f\\) is measurable.\n\n\n\n\n\n\n\n\nCorollary\n\n\nIf \\(f: \\Omega \\to \\R\\) and \\(g: \\Omega \\to \\R\\) are measurable, then \\(f+g\\) is measurable."
  },
  {
    "objectID": "measure/measure.html#simple-functions",
    "href": "measure/measure.html#simple-functions",
    "title": "Measure",
    "section": "Simple Functions",
    "text": "Simple Functions\n\nAssume \\(\\mu(\\Omega) &lt; \\infty\\)\n\\(f\\) is a simple function if \\(f = \\sum_{j=1}^n a_j 1\\{\\omega \\in E_j \\}\\) for \\(a_j \\in \\R\\) and \\(E_j \\in \\mathscr{F}\\)\nIntegral of simple functions: \\[ \\int f d \\mu = \\sum_{j=1}^n a_j \\mu(E_j) \\]"
  },
  {
    "objectID": "measure/measure.html#bounded-functions",
    "href": "measure/measure.html#bounded-functions",
    "title": "Measure",
    "section": "Bounded Functions",
    "text": "Bounded Functions\n\nLet \\(E\\) be such that \\(\\mu(E)&lt;\\infty\\)\nLet \\(f\\) be bounded function and \\(f(x) = 0 \\forall x \\in E^c\\)\nDefine: \\[\n\\int f d\\mu \\equiv \\sup_{\\varphi \\leq f: \\varphi \\text{ simple}} \\int \\varphi d\\mu =\\inf_{\\varphi \\geq f: \\varphi \\text{ simple}} \\int \\varphi d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#nonnegative-functions",
    "href": "measure/measure.html#nonnegative-functions",
    "title": "Measure",
    "section": "Nonnegative Functions",
    "text": "Nonnegative Functions\n\nIf \\(f \\geq 0\\), define \\[\n\\int fd\\mu =\\sup_{f_n \\leq f \\text{ simple, bounded+}} \\int f_{n}d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#measurable-functions",
    "href": "measure/measure.html#measurable-functions",
    "title": "Measure",
    "section": "Measurable Functions",
    "text": "Measurable Functions\n\nIf \\(f\\) is measurable, let \\(f^{+} = \\max\\{f, 0\\}\\) and \\(f^{-} = \\max\\{-f, 0\\}\\) and define the Lesbegue integral\n\n\\[ \\int f d\\mu = \\int f^{+} d\\mu - \\int f^{-} d\\mu \\]"
  },
  {
    "objectID": "measure/measure.html#sequences-of-sets",
    "href": "measure/measure.html#sequences-of-sets",
    "title": "Measure",
    "section": "Sequences of Sets",
    "text": "Sequences of Sets\n\n\\(\\{E_n\\}_{n \\geq 1} \\in \\mathscr{F}\\)\n\nincreasing if \\(E_1 \\subset E_2 \\subset ...\\)\ndecreasing if \\(E_1 \\supset E_2 \\supset ...\\)\nmonotone if either increasing or decreasing\n\nFor increasing \\(E_n\\), define \\(\\lim_{n \\to \\infty} E_n =\\cup_{n=1}^\\infty E_n\\)\nFor decreasing \\(E_n\\), define \\(\\lim_{n \\to \\infty} E_n =\\cap_{n=1}^\\infty E_n\\)"
  },
  {
    "objectID": "measure/measure.html#continuity-of-measure",
    "href": "measure/measure.html#continuity-of-measure",
    "title": "Measure",
    "section": "Continuity of Measure",
    "text": "Continuity of Measure\n\n\n\nLemma\n\n\nSuppose that \\(\\{E_{n}\\}\\) is a monotone sequence of sets. Then \\[\n\\mu \\left( \\lim_{n\\rightarrow \\infty}E_{n}\\right) =\\lim_{n\\rightarrow \\infty }\\mu (E_{n}).\n\\]\n\n\n\n\nTo see what this has to do with “continuity”, remember that one way to define continuity of functions is that for all \\(x_n \\to x\\), \\(f(x_n) \\to x\\)."
  },
  {
    "objectID": "measure/measure.html#monotone-convergence-theorem",
    "href": "measure/measure.html#monotone-convergence-theorem",
    "title": "Measure",
    "section": "Monotone Convergence Theorem",
    "text": "Monotone Convergence Theorem\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\uparrow f(\\omega )\\), then \\(\\int f_{n}d\\mu \\uparrow \\int fd\\mu\\) as \\(n\\rightarrow \\infty\\)\n\n\n\n\n\n\n\nExercise\n\n\nSuppose \\(g_n: \\Omega \\to \\R\\) are measurable and \\(g_n \\geq 0\\). Show that \\[\n\\int \\sum_{n=1}^\\infty g_n d\\mu = \\sum_{n=1}^\\infty \\int g_n d\\mu\n\\]\n\n\n\n\n\nHow do we know \\(f = \\lim_{n\\to \\infty} f_n\\) is measurable?\nNote that half open intervals \\((-\\infty, c)\\) generate \\(\\B(\\R)\\), so it suffices to show that \\(f^{-1}((-\\infty, c))\\) is in \\(\\mathscr{F}\\). Since \\(f_n\\) are measurable, \\(f_n^{-1}((-\\infty,c)) \\in \\mathscr{F}\\) for all \\(n\\). Also, \\[\nf_n^{-1}((-\\infty,c)) = \\{x: f_n(x) &lt; c\\}\n\\] and since \\(f_n\\) are increasing, \\(f_n^{-1}((-\\infty,c)) \\supset\nf_m^{-1}((-\\infty, c))\\) for an \\(m \\geq n\\). Moreover, \\[\nf^{-1}((-\\infty,c)) = \\{x : f(x) &lt; c \\} = \\cup_{n=1}^\\infty \\{x: f_n(x) &lt; c\\}\n\\] and hence is in \\(\\mathscr{F}\\)."
  },
  {
    "objectID": "measure/measure.html#fatous-lemma",
    "href": "measure/measure.html#fatous-lemma",
    "title": "Measure",
    "section": "Fatou’s Lemma",
    "text": "Fatou’s Lemma\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\R\\) are measurable, \\(f_{n}\\geq 0\\), then \\[\n\\int \\left( \\text{liminf}_{n\\rightarrow \\infty }f_{n}d\\mu \\right) \\leq \\text{liminf}_{n\\rightarrow \\infty }\\int f_{n}d\\mu\n\\]\n\n\n\n\n\\(\\text{liminf}_{n\\rightarrow \\infty} x_n = \\lim_{n \\to \\infty} \\left(\\inf m \\geq n x_m \\right)\\)"
  },
  {
    "objectID": "measure/measure.html#dominated-convergence-theorem",
    "href": "measure/measure.html#dominated-convergence-theorem",
    "title": "Measure",
    "section": "Dominated Convergence Theorem",
    "text": "Dominated Convergence Theorem\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\R\\) are measurable, and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\rightarrow f(\\omega ).\\) Furthermore, for some \\(g\\geq 0\\) such that \\(\\int gd\\mu &lt;\\infty\\), \\(|f_{n}|\\leq g\\) for each \\(n\\geq 1\\). Then, \\(\\int f_{n}d\\mu \\rightarrow \\int fd\\mu\\)\n\n\n\n\n\n\n\nExercise\n\n\nShow that if \\(g_n: \\Omega \\to \\R\\) are measurable and \\(\\sum_{n=1}^\\infty \\int |g_n| d\\mu &lt; \\infty\\), then \\(\\sum_{n=1}^\\infty |g_n(x)| &lt; \\infty\\) almost everywhere and if \\(g(x) = \\sum_{n=1}^\\infty g_n(x)\\), then \\[\n\\lim_{N \\to \\infty} \\int \\left\\vert g(x) - \\sum_{n=1}^N g_n(x) \\right\\vert d\\mu(x) = 0\n\\]"
  },
  {
    "objectID": "measure/measure.html#absolute-continuity",
    "href": "measure/measure.html#absolute-continuity",
    "title": "Measure",
    "section": "Absolute Continuity",
    "text": "Absolute Continuity\n\nMeasure \\(\\nu\\) is absolutely continuous with respect to \\(\\mu\\) if for \\(A \\in \\mathscr{F}\\), \\(\\mu(A) = 0\\) implies \\(\\nu(A) = 0\\)\n\ndenotate as \\(\\nu \\ll \\mu\\)\n\\(\\mu\\) is called a dominating measure"
  },
  {
    "objectID": "measure/measure.html#radon-nikodym-derivative",
    "href": "measure/measure.html#radon-nikodym-derivative",
    "title": "Measure",
    "section": "Radon-Nikodym Derivative",
    "text": "Radon-Nikodym Derivative\n\n\n\nTheorem\n\n\nLet \\((\\Omega,\\mathscr{F},\\mu)\\) be a measure space, and let \\(\\nu\\) and \\(\\mu\\) be \\(\\sigma\\)-finite measures defined on \\(\\mathscr{F}\\) and \\(\\nu \\ll \\mu\\). Then there is a nonnegative measurable function \\(f\\) such that for each set \\(A\\in \\mathscr{F}\\), \\[\n\\nu (A)=\\int_{A}fd\\mu\n\\] For any such \\(f\\) and \\(g\\), \\(\\mu (\\{\\omega \\in \\Omega:f(\\omega )\\neq g(\\omega )\\})=0\\)\n\n\n\n\nDenote \\(f = \\frac{d\\nu}{d\\mu}\\)\nExercise: show coincides with usual derivative?"
  },
  {
    "objectID": "iv/endogeneity.html#reading",
    "href": "iv/endogeneity.html#reading",
    "title": "Endogeneity",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 11\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "iv/endogeneity.html#omitted-variables",
    "href": "iv/endogeneity.html#omitted-variables",
    "title": "Endogeneity",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\nDesired model \\[\nY_i = X_i'\\beta_0 + W_i'\\gamma_0 + u_i\n\\] Assume \\(\\Er[u] = \\Er[Xu] = \\Er[Wu] = 0\\)\nEstimated model \\[\nY_i = X_i'\\beta + u_i\n\\]\nWhat is \\(\\plim \\hat{\\beta}\\)?"
  },
  {
    "objectID": "iv/endogeneity.html#omitted-variables-1",
    "href": "iv/endogeneity.html#omitted-variables-1",
    "title": "Endogeneity",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\n\\(\\plim \\hat{\\beta} \\inprob \\beta_0 + \\Er[X_i X_i']^{-1} \\Er[X_i W_i'] \\gamma_0\\)"
  },
  {
    "objectID": "iv/endogeneity.html#omitted-variables-2",
    "href": "iv/endogeneity.html#omitted-variables-2",
    "title": "Endogeneity",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\nIf \\(\\gamma_0 = 0\\), what is variance of \\(\\hat{\\beta}\\) when \\(W\\) is and is not included in the model?"
  },
  {
    "objectID": "iv/endogeneity.html#errors-in-variables",
    "href": "iv/endogeneity.html#errors-in-variables",
    "title": "Endogeneity",
    "section": "Errors in Variables",
    "text": "Errors in Variables\n\nSee problem set 6"
  },
  {
    "objectID": "iv/endogeneity.html#simultaneity-bias",
    "href": "iv/endogeneity.html#simultaneity-bias",
    "title": "Endogeneity",
    "section": "Simultaneity Bias",
    "text": "Simultaneity Bias\n\nEquilibrium conditions often lead to variables that are simultaneously determined\nDemand and supply: \\[\n\\begin{align*}\nQ_i^D & = P_i \\beta_D + X_D'\\gamma_D + u_{D,i} \\\\\nQ_i^S & = P_i \\beta_S + X_S'\\gamma_S + u_{S,i} \\\\\nQ_i^S & = Q_i^D\n\\end{align*}\n\\]"
  },
  {
    "objectID": "iv/endogeneity.html#simultaneity-bias-1",
    "href": "iv/endogeneity.html#simultaneity-bias-1",
    "title": "Endogeneity",
    "section": "Simultaneity Bias",
    "text": "Simultaneity Bias\n\nStructural equations: (demand and inverse supply): \\[\n\\begin{align*}\nQ_i & = P_i \\beta_D + X_D'\\gamma_D + u_{D,i} \\\\\nP_i & = Q_i \\frac{1}{\\beta_S}  - X_S'\\gamma_D\\frac{1}{\\beta_S} - u_{S,i}\\frac{1}{\\beta_S} \\\\\n\\end{align*}\n\\]\nReduced form: \\[\n\\begin{align*}\nQ_i = & \\frac{\\beta_D}{\\beta_D - \\beta_S} \\left( -X_{D,i}' \\gamma_D + X_{S,i}'\\gamma_S - u_{D,i} + u_{S,i} \\right) + X_{D,i}'\\gamma_D + u_{D,i} \\\\\nP_i = & \\frac{1}{\\beta_D - \\beta_S}\\left(-X_{D,i}' \\gamma_D + X_{S,i}'\\gamma_S - u_{D,i} + u_{S,i} \\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "identification/identification.html#reading",
    "href": "identification/identification.html#reading",
    "title": "Identification",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 4 (which is the basis for these slides)\nRecommended: Lewbel (2019)\nSupplementary: Matzkin (2013), Molinari (2020) , Imbens (2020), V. Chernozhukov et al. (2024)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\]\n\nMotivate by wanting to go from data to parameters …"
  },
  {
    "objectID": "identification/identification.html#example-descriptive-statistics",
    "href": "identification/identification.html#example-descriptive-statistics",
    "title": "Identification",
    "section": "Example: Descriptive Statistics",
    "text": "Example: Descriptive Statistics\n\n\\(\\theta_0 =\\) mean of \\(X\\), then \\(\\theta_0\\) is identified by \\[\n\\psi_\\mu(P) = \\int x dP(x)\n\\] in \\(\\mathcal{P} = \\{P : \\int x dP(x) &lt; \\infty \\}\\)\nGenerally, descriptive statistics identified in a broad probability model with just regularity restrictions to ensure the statistics exist"
  },
  {
    "objectID": "identification/identification.html#example-linear-model",
    "href": "identification/identification.html#example-linear-model",
    "title": "Identification",
    "section": "Example: Linear Model",
    "text": "Example: Linear Model\n\\[\nY = \\alpha + \\beta X + \\epsilon\n\\]\n\n\\(\\mathcal{P} = \\{P_{X,Y}:\\) \\(Y=\\alpha + \\beta X + \\epsilon\\),\n\n\\(| \\mathrm{Cov}(X,Y) | &lt; \\infty\\), \\(0 &lt; \\mathrm{Var}(X) &lt; \\infty\\)\n\\(\\mathrm{Cov}(X, \\epsilon)  = 0\\) \\(\\}\\)\n\n\\(\\beta\\) identified as\n\n\\[\n\\beta = \\frac{\\int (x - \\Er X) (y - \\Er Y ) dP_{X,Y}(x,y)}\n{\\int (x - \\Er X)^2 dP_{X}(x)}  = \\frac{ \\cov(X,Y) }{ \\var(X) }\n\\]"
  },
  {
    "objectID": "identification/identification.html#section",
    "href": "identification/identification.html#section",
    "title": "Identification",
    "section": "",
    "text": "Identification requires:\n\nUsually innocuous regularity conditions\nSubstantive exogeneity restriction\n\nEvaluating plausibility of exogeneity restrictions requires a priori knowledge of data context and related economic theory\n\n\nExamples of production function and returns to schooling here?\n2 is more difficult to get right and more context dependent, so we will first tackle the easier problem of 1."
  },
  {
    "objectID": "identification/identification.html#example-multiple-regression",
    "href": "identification/identification.html#example-multiple-regression",
    "title": "Identification",
    "section": "Example: Multiple Regression",
    "text": "Example: Multiple Regression\n\\[\nY = X'\\beta + \\epsilon\n\\]\n\n\\(\\mathcal{P} = \\{P: \\Er [X \\epsilon] = 0, \\Er [X X'] \\text{ invertible} \\}\\)"
  },
  {
    "objectID": "identification/identification.html#example-binary-choice",
    "href": "identification/identification.html#example-binary-choice",
    "title": "Identification",
    "section": "Example: Binary Choice",
    "text": "Example: Binary Choice\n\\[\nY = 1\\{ \\beta_0 + \\beta_1 X &gt; u \\}\n\\]\n\n\\(\\mathcal{P} = \\{P: u \\sim N(0,1), 0&lt; \\var(X) &lt; \\infty \\}\\)\n\n\n\nIs \\(u \\sim N(0,1)\\) innocuous?\n\n\n\nMore generally, think about importance of statistical assumptions in terms of how they affect identifiable quantities and counterfactuals of interest."
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes",
    "href": "identification/identification.html#example-potential-outcomes",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\nData:\n\nTreatment \\(D_i\\)\nPotential outcomes \\((Y_{i,0}, Y_{i,1})\\), observed outcome \\(Y_i = D_i Y_{i,1} + (1-D_i) Y_{i,0}\\)\nCovarites \\(X_i\\)\n\nParameter: \\(\\theta_0 = \\Er[Y_{i,1} - Y_{i,0}] =\\) average treatment effect\nAssume:\n\nUnconfoundedness: \\((Y_{i,0}, Y_{i,1})\\) conditionally independent of \\(D_i\\) given \\(X_i\\)\nOverlap: \\(\\epsilon &lt; P(D=1|X=x) &lt; 1-\\epsilon\\) for some \\(\\epsilon &gt; 0\\) and all \\(x\\)"
  },
  {
    "objectID": "identification/identification.html#causal-diagrams-1",
    "href": "identification/identification.html#causal-diagrams-1",
    "title": "Identification",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\n\nOriginate with Wright around 1920, e.g. Wright (1934)\nRecently advocated by Pearl, e.g. Pearl (2015), Pearl and Mackenzie (2018)\nRecommended introduction Imbens (2020) or CausalMLBook chapters 7 and 8\nSometimes useful expository tool for explaining identifying restriction, but should not be your only or primary approach\n\ne.g. Victor Chernozhukov, Kasahara, and Schrimpf (2021)"
  },
  {
    "objectID": "identification/identification.html#example-regression",
    "href": "identification/identification.html#example-regression",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    X --&gt; Y\n    ϵ -.-&gt; Y\n\n\n\n\n\n\n\nArrow means \\(X\\) causes \\(Y\\)\nDashed arrow means \\(\\epsilon\\) causes \\(Y\\) and is unobserved (not universal, often dashed box around \\(\\epsilon\\) instead)\nLack of connection between \\(X\\) and \\(\\epsilon\\) means they are independent"
  },
  {
    "objectID": "identification/identification.html#example-regression-1",
    "href": "identification/identification.html#example-regression-1",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\nIf you believe:\n\n\n\n\n\nflowchart LR\n    subgraph \" Y\"\n        Y[Wage]\n    end\n    subgraph X\n       E[Education] --&gt; Y\n       T[SAT] --&gt; E\n       F[Family SES] --&gt; E\n       F --&gt; Y\n    end\n    subgraph ϵ\n        I[Intelligence] -.-&gt; Y\n        L[Luck] -.-&gt; Y\n        I -.-&gt; T\n    end\n\n\n\n\n\n\nthen regression \\(Wage = \\beta_1 Education + \\beta_2 SAT + \\beta_3 FamilySES + \\epsilon\\) identifies causal effect of education on wages"
  },
  {
    "objectID": "identification/identification.html#example-regression-2",
    "href": "identification/identification.html#example-regression-2",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\nBut reality is likely more complex …\n\n\n\n\n\nflowchart LR\n   subgraph \" Y\"\n        Y[Wage]\n   end\n   subgraph X\n       E[Education] --&gt; Y\n       T[SAT] --&gt; E\n       F[Family SES] --&gt; E\n       F --&gt; Y\n   end\n   subgraph ϵ\n        I[Intelligence] -.-&gt; Y\n        L[Luck] -.-&gt; Y\n        G[Grit] -.-&gt; Y\n        G -.-&gt; E\n        L -.-&gt; E\n        I -.-&gt; T\n        I -.-&gt; E\n        I -.-&gt; G\n        G -.-&gt; I\n   end"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-1",
    "href": "identification/identification.html#example-potential-outcomes-1",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\n\n\n\n\nflowchart LR\n    u -.-&gt; D\n    X --&gt; Y\n    X --&gt; D\n    D --&gt; Y\n    ϵ -.-&gt; Y"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-2",
    "href": "identification/identification.html#example-potential-outcomes-2",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\n\n\n\n\nflowchart LR\n    subgraph Treatment\n        D[Naloxone distribution site opens]\n    end\n    subgraph Outcome\n        Y[ER visits for overdose]\n    end\n    u -.-&gt; D\n    D --&gt; Y\n    ϵ -.-&gt; Y\n    subgraph X\n        Income --&gt; Y\n        Unemployment --&gt; Y\n        Income --&gt; D\n        Unemployment --&gt; D\n        OD[OD rate prior to opening] --&gt; Y\n        OD --&gt; D\n        Crime --&gt; Y\n        Crime --&gt; D\n    end"
  },
  {
    "objectID": "identification/identification.html#more-on-causal-graphs",
    "href": "identification/identification.html#more-on-causal-graphs",
    "title": "Identification",
    "section": "More on Causal Graphs",
    "text": "More on Causal Graphs\n\nGiven a graph, what can be identified?\n\n\\(Y\\) and \\(X\\) are d-separated by a collection of nodes \\(S\\) if there are no paths between \\(Y\\) and \\(X\\) except through \\(S\\)\nd-separation implies conditional independence \\(Y \\perp X | S\\)\n\nDoes conditional independence imply d-separation? Can we estimate counditional independence to find out the correct causal graph? (Causal discovery)\n\nConditional independence does not automatically imply d-separation, but exceptions have measure 0, so maybe causal discovery possible\nBut neighborhood of exceptions is large, so causal discovery very difficult\n\nV. Chernozhukov et al. (2024), especially chapters 7 & 8"
  },
  {
    "objectID": "identification/identification.html#reintrepretation-of-estimators-1",
    "href": "identification/identification.html#reintrepretation-of-estimators-1",
    "title": "Identification",
    "section": "Reintrepretation of Estimators",
    "text": "Reintrepretation of Estimators\n\nGeneralized and/or descriptive interpretation of population estimator\nAnalyze familiar estimator under more general assumptions\n\nUnderstand bias when exogeneity assumptions fail\nSometimes give more general interpretation of existing estimator"
  },
  {
    "objectID": "identification/identification.html#example-regression-3",
    "href": "identification/identification.html#example-regression-3",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\nIn linear model \\(Y_i = X_i'\\beta + \\epsilon_i\\), if just assume \\(\\Er[X X']\\) invertible,\nPopulation regression \\[\n\\begin{align*}\n\\theta = & \\Er[ X X']^{-1} \\Er[ X Y]  \\\\\n= & \\Er[X X']^{-1} \\Er[X (X' \\beta + \\epsilon)] \\\\\n= & \\beta + \\Er[X X']^{-1} \\Er[X\\epsilon]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "identification/identification.html#example-regression-4",
    "href": "identification/identification.html#example-regression-4",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\nIf relevant moments exist (no linear model required) population regression solves \\[\n\\Er[ X X']^{-1} \\Er[ X Y]  \\in \\mathrm{arg}\\min_b \\Er[ (X'b - \\Er[Y|X])^2 ]\n\\]"
  },
  {
    "objectID": "identification/identification.html#caution",
    "href": "identification/identification.html#caution",
    "title": "Identification",
    "section": "Caution",
    "text": "Caution\n\nRegression being a linear approximation to \\(\\Er[Y|X]\\) does not mean \\(\\beta = \\Er[X X']^{-1} \\Er[X Y]\\) necessarily has the sign you want\nIn example below, \\(\\Er[Y|x_1=1, x_2] &gt; \\Er[Y|x_1=0,x_2]\\), but \\(\\beta_1 &lt; 0\\)\n\n\n\n3-element Vector{Float64}:\n  0.4859277368803867\n -0.10715764238088142\n -0.3817130280184443"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-3",
    "href": "identification/identification.html#example-potential-outcomes-3",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\nMatching initially studied with a linear regression model, e.g. Cochran (1953) \\[\nY_i = \\alpha D_i + X_i' \\beta + \\epsilon_i\n\\]\nImplies constant treatment effect \\(Y_{i,1} - Y_{i,0} = \\alpha\\)\n\n\nThe LATE interpretation of IV developed along similar lines – replace a constant treatment effect linear model with a heterogenous effect potential outcomes framework, and see what IV does.\nSimilarly, the recent literature on difference in differences with staggered treatment timing arises from moving from a model with a constant treatment effect over time to one where treatment effects vary with time."
  },
  {
    "objectID": "identification/identification.html#observational-equivalence-1",
    "href": "identification/identification.html#observational-equivalence-1",
    "title": "Identification",
    "section": "Observational Equivalence",
    "text": "Observational Equivalence\n\nIdentification sometimes defined without explicit mapping from data to parameters, e.g. Hsiao (1983), Matzkin (2007)\n\n\n\n\nDefinition: Observationally Equivalent\n\n\n\nLet \\(\\mathcal{P} = \\{ P(\\cdot; s) : s \\in S \\}\\), two structures \\(s\\) and \\(\\tilde{s}\\) in \\(S\\) are observationally equivalent if they imply the same distribution for the observed data, i.e. \\[ P(B;s) = P(B; \\tilde{s}) \\] for all \\(B \\in \\sigma(X)\\).\nLet \\(\\lambda: S \\to \\R^k\\), \\(\\theta\\) is observationally equivalent to \\(\\tilde{\\theta}\\) if \\(\\exists s, \\tilde{s} \\in S\\) that are observationally equivalent and \\(\\theta = \\lambda(s)\\) and \\(\\tilde{\\theta} = \\lambda(\\tilde{s})\\)\n\nLet \\(\\Gamma(\\theta, S) = \\{P(\\dot; s) | s \\in S, \\theta = \\lambda(s) \\}\\), then \\(\\theta\\) and \\(\\tilde{\\theta}\\) are observationally equivalent iff \\(\\Gamma(\\theta,S) \\cap \\Gamma(\\tilde{\\theta}, S) \\neq \\emptyset\\)"
  },
  {
    "objectID": "identification/identification.html#non-constructive-identification",
    "href": "identification/identification.html#non-constructive-identification",
    "title": "Identification",
    "section": "Non-constructive Identification",
    "text": "Non-constructive Identification\n\n\n\nDefinition: (Non-Constructive) Identification\n\n\n\n\\(s_0 \\in S\\) is identified if there is no \\(s\\) that is observationally equivalent to \\(s_0\\)\n\\(\\theta_0\\) is identified (in \\(S\\)) if there is no observationally equivalent \\(\\theta \\neq \\theta_0\\)\n\ni.e. \\(\\Gamma(\\theta_0, S) \\cap \\Gamma(\\theta, S) = \\emptyset\\) \\(\\forall \\theta \\neq \\theta_0\\)\n\n\n\n\n\n\nCompared to constructive definition with \\(\\theta_0 = \\psi(P)\\):\n\nLess clear how to use identification to estimate\nEasier to show non-identification\nSet of observationally equivalent structures can be of interest"
  },
  {
    "objectID": "identification/identification.html#example-multiple-regression-1",
    "href": "identification/identification.html#example-multiple-regression-1",
    "title": "Identification",
    "section": "Example: Multiple Regression",
    "text": "Example: Multiple Regression\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]\n\n\\(X = [X_1\\, X_2]'\\), if rank \\(\\Er X X' = 1\\), then \\(\\beta_1, \\beta_2\\) is observationally equivalent to any \\(\\tilde{\\beta}_1, \\tilde{\\beta}_2\\) s.t. \\[\n\\tilde{\\beta}_1 + \\tilde{\\beta}_2 = \\beta_1 + \\beta_2 \\frac{\\cov(X_1, X_2)}{\\var(X_2)}\n\\]\n\\(\\theta_0 = \\lambda( \\beta ) = \\beta_1 + \\beta_2\\) is identified if rank \\(\\Er [X X'] \\geq 1\\)"
  },
  {
    "objectID": "identification/identification.html#example-random-coefficients-logit",
    "href": "identification/identification.html#example-random-coefficients-logit",
    "title": "Identification",
    "section": "Example: Random Coefficients Logit",
    "text": "Example: Random Coefficients Logit\n\n\\(Y_i = 1\\{\\beta_0 + \\beta_i X_i  \\geq U_i \\}\\)\n\n\\(U\\) independent \\(X_i,\\beta_i\\),\n\\(\\beta_i\\) indepedent \\(X_i\\),\n\\(F_u(z) = \\frac{e^z}{1+e^z}\\)\n\n\\(\\Er[Y|X] = \\int \\frac{e^{\\beta_0 + \\beta X_i}} {1+e^{\\beta_0 + \\beta X_i}} dF_\\beta(\\beta)\\)\nNon-constructive and constructive identification of \\(F_\\beta\\) in Fox et al. (2012)\n\n\nHard to give a brief example where non-constructive argument is required. Non constructive identification proofs typically leverage some high level mathematical result.\nChristensen (2015) is another example of non-constructive identification."
  },
  {
    "objectID": "estimation/estimation.html#reading",
    "href": "estimation/estimation.html#reading",
    "title": "Estimation",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 4, sections 1.2 and 2 (which is the basis for these slides)\nSupplemental: Erich Leo Lehmann and Romano (n.d.) , Erich L. Lehmann and Casella (2006)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#estimator",
    "href": "estimation/estimation.html#estimator",
    "title": "Estimation",
    "section": "Estimator",
    "text": "Estimator\n\n\nGiven a parameter of interest \\(\\theta_0\\), an estimator is a measurable function of an observed random vector X, i.e. \\(\\hat{\\theta} = \\tau(X)\\) for some known map \\(\\tau\\)\nAn estimate given \\(X=x\\) is \\(\\tau(x)\\)"
  },
  {
    "objectID": "estimation/estimation.html#sample-analogue-estimation",
    "href": "estimation/estimation.html#sample-analogue-estimation",
    "title": "Estimation",
    "section": "Sample Analogue Estimation",
    "text": "Sample Analogue Estimation\n\ni.i.d. observations from \\(P\\), \\(X = (X_1, ...., X_n)\\)\nconstructively identified parameter \\(\\theta_0 = \\psi(P)\\)\nempirical measure: \\[\n\\hat{P}(B) = \\frac{1}{n} \\sum_{i=1}^n 1\\{X_i \\in B \\}.\n\\]\nSample analogue estimator \\[\n\\hat{\\theta} = \\psi(\\hat{P})\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#sample-analogue-estimation---examples",
    "href": "estimation/estimation.html#sample-analogue-estimation---examples",
    "title": "Estimation",
    "section": "Sample Analogue Estimation - Examples",
    "text": "Sample Analogue Estimation - Examples\n\nMean\nOLS\n\n\nGo over some examples. Introduce empirical expectation.\nMention idea of conditions on \\(\\psi\\) and \\(\\hat{P}\\) such that estimator has good properties."
  },
  {
    "objectID": "estimation/estimation.html#maximum-likelihood-estimation",
    "href": "estimation/estimation.html#maximum-likelihood-estimation",
    "title": "Estimation",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta\n\\in \\Theta \\subset \\R^d \\}\\)\n\\(P_\\theta\\) dominated by \\(\\sigma\\)-finite \\(\\mu\\) with density \\(f_X(\\cdot;\\theta)\\)\nLikelihood \\(\\ell(\\cdot, X): \\Theta \\to [0,\\infty)\\) \\[\n\\ell(\\theta; X)= f(X; \\theta)\n\\]\nMaximum likelihood estimator \\[\n\\hat{\\theta}_{MLE} = \\textrm{arg}\\max_{\\theta \\in \\Theta} \\ell(\\theta;X)\n\\]\n\n\nNormal mean example.\nLog-likelihood."
  },
  {
    "objectID": "estimation/estimation.html#mle-examples",
    "href": "estimation/estimation.html#mle-examples",
    "title": "Estimation",
    "section": "MLE: Examples",
    "text": "MLE: Examples\n\n\\(X_i \\sim N(\\mu, 1)\\)\n\\(Y_i = \\alpha_0 + \\beta_0 X_i + \\epsilon_i\\), \\(\\epsilon_i \\sim N(0, \\sigma_0^2)\\)"
  },
  {
    "objectID": "estimation/estimation.html#mle-equivariance",
    "href": "estimation/estimation.html#mle-equivariance",
    "title": "Estimation",
    "section": "MLE: Equivariance",
    "text": "MLE: Equivariance\n\n\n\nTheorem 1.1\n\n\nIf \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\), then for any function \\(g:\\Theta \\to G\\), the MLE of \\(g(\\theta)\\) is \\(g(\\hat{\\theta})\\)."
  },
  {
    "objectID": "estimation/estimation.html#mean-squared-error",
    "href": "estimation/estimation.html#mean-squared-error",
    "title": "Estimation",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nLoss function \\(L: \\R^d \\times \\Theta \\to [0,\\infty)\\) with \\(L(\\theta,\\theta)=0\\)\nRisk of at \\(\\theta_0\\) \\(\\Er[L(\\hat{\\theta}, \\theta_0)]\\)\nSquared error loss \\(L_2(\\theta, \\theta_0) = (\\theta-\\theta_0)'(\\theta-\\theta_0)\\)\nMean squared error \\[\nMSE(\\hat{\\theta}) = \\Er[ (\\theta-\\theta_0)'(\\theta-\\theta_0) ]\n\\]\nBias-variance decomposition \\[\nMSE(\\hat{\\theta}) = \\textrm{Bias}(\\hat{\\theta})'\\textrm{Bias}(\\hat{\\theta}) + \\textrm{tr}(\\var(\\hat{\\theta}))\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#setup",
    "href": "estimation/estimation.html#setup",
    "title": "Estimation",
    "section": "Setup",
    "text": "Setup\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta\n\\in \\Theta \\subset \\R^d \\}\\), likelihood \\(\\ell(\\theta;x) = f_X(x;\\theta)\\)\nQuestion: if an estimator is unbiased, what is the smallest possible variance?"
  },
  {
    "objectID": "estimation/estimation.html#score-equality",
    "href": "estimation/estimation.html#score-equality",
    "title": "Estimation",
    "section": "Score Equality",
    "text": "Score Equality\n\nIf \\(\\frac{\\partial}{\\partial \\theta} \\int f_X(x;\\theta) d\\mu(x) =\n\\int \\frac{\\partial}{\\partial \\theta} f_X(x;\\theta) d\\mu(x)\\), then \\[\n\\int \\underbrace{\\frac{\\partial \\log \\ell(\\theta;x)}{\\partial \\theta}}_{\\text{\"score\"}=s(x,\\theta)} dP_\\theta(x) = 0\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#information-equality",
    "href": "estimation/estimation.html#information-equality",
    "title": "Estimation",
    "section": "Information Equality",
    "text": "Information Equality\n\nFischer Information \\(I(\\theta) = \\int s(x,\\theta) s(x,\\theta)' dP_\\theta(x)\\)\nIf \\(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} \\int f_X(x;\\theta) d\\mu(x) =\n\\int \\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} f_X(x;\\theta)\nd\\mu(x)\\), then \\[\nI(\\theta) = -\\int \\underbrace{\\frac{\\partial^2 \\ell(\\theta;x)}{\\partial \\theta \\partial \\theta'}}_{\\text{\"Hessian\"}=h(x,\\theta)} dP_\\theta(x)\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#section",
    "href": "estimation/estimation.html#section",
    "title": "Estimation",
    "section": "",
    "text": "If \\(T = \\tau(X)\\) is an unbiased estimator for \\(\\theta\\) and \\[\n\\frac{\\partial}{\\partial \\theta} \\int \\tau(x) f_X(x;\\theta) d\\mu(x) =\n\\int \\tau(x) \\frac{\\partial f_X(x,\\theta)}{\\partial \\theta\\partial \\theta'} d\\mu(x)\n\\] then \\[\n\\int \\tau(x) s(x,\\theta)'dP_\\theta(x) = I\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#cramér-rao-bound",
    "href": "estimation/estimation.html#cramér-rao-bound",
    "title": "Estimation",
    "section": "Cramér-Rao Bound",
    "text": "Cramér-Rao Bound\n\n\n\nCramér-Rao Bound\n\n\nLet \\(T = \\tau(X)\\) be an unbiased estimator, and suppose the condition of the previous slide and of the score equality hold. Then, \\[\n\\var_\\theta(\\tau(X)) \\equiv \\int \\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)\\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)' dP\\theta \\geq I(\\theta)^{-1}\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-1",
    "href": "estimation/estimation.html#hypothesis-testing-1",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(X \\in \\mathcal{X} \\subset \\R^n\\), distribution \\(P_x \\in \\mathcal{P}\\)\nPartition \\(\\mathcal{P} = \\mathcal{P}_0 \\cup \\mathcal{P}_1\\)\nNull and alternative hypotheses:\n\n\\(H_0: \\; P_x \\in \\mathcal{P}_0\\)\n\\(H_1: \\; P_x \\in \\mathcal{P}_1\\)"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-2",
    "href": "estimation/estimation.html#hypothesis-testing-2",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nTest partitions \\(\\mathcal{X} = \\underbrace{C}_{\\text{critical region}} \\cup A\\)\n\nReject null if \\(X \\in C\\)\nOften \\(C = \\{x \\in \\mathcal{X}:\n   \\underbrace{\\tau(x)}_{\\text{test statistic}} &gt;\n   \\underbrace{c}_{\\text{critical value}} \\}\\)"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-3",
    "href": "estimation/estimation.html#hypothesis-testing-3",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_0)\\)=Type I error \\(=P_x(C)\\)\n\\(P(\\text{fail to reject } H_0 | P_x \\in \\mathcal{P}_1)\\)=Type II error\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_1)\\) = power\n\\(\\sup_{P_x \\in \\mathcal{P}_0} P_x(C)\\) = size of test\n\n\nIllustrate schematically how these vary with \\(C\\)"
  },
  {
    "objectID": "estimation/estimation.html#p-value",
    "href": "estimation/estimation.html#p-value",
    "title": "Estimation",
    "section": "p-value",
    "text": "p-value\n\ntest statistic \\(\\tau(X)\\) , define \\[ G_P(t)  =  P(\\tau(X) &gt; t) \\]\np-value is \\[\np= \\sup_{P \\in \\mathcal{P}_0} G_P(\\tau(X))\n\\]\n\n\n\nif \\(\\mathcal{P}_0 = \\{P_0\\}\\), critical value \\(c\\), let \\(\\alpha = G_{P_0}(c)\\), then \\(\\tau(X) &gt; c\\) iff \\(p &lt; \\alpha\\)"
  },
  {
    "objectID": "estimation/estimation.html#testing-in-parametric-family",
    "href": "estimation/estimation.html#testing-in-parametric-family",
    "title": "Estimation",
    "section": "Testing in Parametric Family",
    "text": "Testing in Parametric Family\n\nParametric family \\(\\mathcal{P} = \\{P_\\theta: \\theta  \\in \\Theta\\}\\)\n\n\\(\\Theta_0 = \\{\\theta \\in \\Theta: P_\\theta \\in \\mathcal{P}_0\\}\\)\n\\(\\Theta_1 = \\{\\theta \\in \\Theta: P_\\theta \\in \\mathcal{P}_1\\}\\)\n\nHypotheses\n\n\\(H_0 : \\theta  \\in \\Theta_0\\)\n\\(H_1: \\theta \\in \\Theta_1\\)\n\nPower function of test \\(C\\) \\[\\pi:\\Theta \\to [0,1] , \\;\\;  \\pi(\\theta) = P_\\theta(C)\\]\nSize \\(= \\sup_{\\theta \\in \\Theta_0} \\pi(\\theta)\\)"
  },
  {
    "objectID": "estimation/estimation.html#more-powerful",
    "href": "estimation/estimation.html#more-powerful",
    "title": "Estimation",
    "section": "More Powerful",
    "text": "More Powerful\n\n\n\nDefinition\n\n\n\nFor test \\(C_1\\) and \\(C_2\\) with same size, \\(C_1\\) is more powerful at \\(\\theta \\in \\Theta_1\\) than \\(C_2\\) if \\(P_\\theta(C_1) \\geq P_\\theta(C_2)\\)\n\\(C\\) is most powerful at \\(\\theta \\in \\Theta_1\\) if is more powerfull than any test of the same size\n\\(C\\) is uniformly most powerful if it is most powerful at any \\(\\theta \\in \\Theta_1\\)"
  },
  {
    "objectID": "estimation/estimation.html#neyman-pearson",
    "href": "estimation/estimation.html#neyman-pearson",
    "title": "Estimation",
    "section": "Neyman-Pearson",
    "text": "Neyman-Pearson\n\n\n\nLemma (Neyman-Pearson)\n\n\nLet \\(\\Theta = \\{0, 1\\}\\), \\(f_0\\) and \\(f_1\\) be densities of \\(P_0\\) and \\(P_1\\), \\(\\tau(x) =f_1(x)/f_0(x)\\) and \\(C^* =\\{x \\in X: \\tau(x) &gt; c\\}\\). Then among all tests \\(C\\) s.t. \\(P_0(C) = P_0(C^*)\\), \\(C^*\\) is most powerful."
  },
  {
    "objectID": "estimation/estimation.html#example",
    "href": "estimation/estimation.html#example",
    "title": "Estimation",
    "section": "Example",
    "text": "Example\n\n\\(X_i \\sim N(\\mu, 1)\\)\n\\(H_0: \\mu = 0\\) against \\(H_1: \\mu = 1\\)\nFind a most powerful test\n\n\n\nWhat is the most powerful test if \\(H_1: \\mu = a\\) for \\(a&gt;0\\) instead?\n\n\n\n\nWhat is the uniformly most powerful test if \\(H_1: \\mu &gt; 0\\) ?\n\n\n\n\nWhat is the uniformly most powerful test if \\(H_1: \\mu \\neq 0\\) ?"
  },
  {
    "objectID": "asymptotics/ols.html#reading",
    "href": "asymptotics/ols.html#reading",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 10\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#consistency",
    "href": "asymptotics/ols.html#consistency",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Consistency",
    "text": "Consistency\n\\[\n\\begin{align*}\n\\hat{\\beta} = & (X'X)^{-1} X' y \\\\\n= & (X'X)^{-1} X' (X \\beta + \\epsilon) \\\\\n= & \\beta + (X'X)^{-1} X' \\epsilon\n\\end{align*}\n\\]\nConsistent, \\(\\hat{\\beta} \\inprob \\beta\\), if\n\n\\((X'X)^{-1} X' \\epsilon \\inprob 0\\) (“high level assumption”), or\n\\(\\frac{1}{n} X'X \\inprob C\\) and \\(\\frac{1}{n} X' \\epsilon \\inprob 0\\), or"
  },
  {
    "objectID": "asymptotics/ols.html#consistency-1",
    "href": "asymptotics/ols.html#consistency-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Consistency",
    "text": "Consistency\n\\[\n\\begin{align*}\n\\hat{\\beta} = & (X'X)^{-1} X' y \\\\\n= & (X'X)^{-1} X' (X \\beta + \\epsilon) \\\\\n= & \\beta + (X'X)^{-1} X' \\epsilon\n\\end{align*}\n\\]\nConsistent, \\(\\hat{\\beta} \\inprob \\beta\\), if\n\nUsing non-iid WLLN from convergence in distribution slides (“low level assumption”):\n\nFor both \\(Z_i = X_i'\\) and \\(Z_i = \\epsilon_i\\),\n\n\\(\\Er\\left[\\left((X_i Z_i) - \\Er[X_i Z_i'] \\right) \\left((X_j\n  Z_j) - \\Er[X_j Z_j]\\right)' 1\\{\\right] = 0\\) for \\(i \\neq j\\) and\n\\(\\frac{1}{n} \\max_{1  \\leq i \\leq n} \\Er[ (X_i Z_i - \\Er[X_i Z_i]) (X_i Z_i - \\Er[X_i Z_i])'] \\to 0\\)\n\n\\(\\Er[X_i \\epsilon_i] = 0\\) for all \\(i\\)\n\\(\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\Er[X_i X_i'] = C\\) is invertible"
  },
  {
    "objectID": "asymptotics/ols.html#asymptotic-distribution",
    "href": "asymptotics/ols.html#asymptotic-distribution",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\beta} - \\beta) = & \\sqrt{n} (X'X)^{-1} (X' \\epsilon) \\\\\n= & (\\frac{1}{n} X'X)^{-1} \\frac{1}{\\sqrt{n}} X' \\epsilon\n\\end{align*}\n\\]\n\\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\) if:\n\n\\(\\frac{1}{n} X'X \\inprob C\\) nonsingular, and \\(\\frac{1}{\\sqrt{n}} X' \\epsilon \\indist N(0,V)\\), or\nUsing i.i.d. CLT and WLLN:\n\n\\((X_i, \\epsilon_i)\\) i.i.d.\n\\(\\Er[X_i X_i' ]\\) is nonsingular, \\(\\Er[X_i \\epsilon_i] = 0\\), and \\(\\var(X_i \\epsilon_i) = \\Omega &gt; 0\\)\nExercise: what is \\(\\Sigma\\) under these assumptions?"
  },
  {
    "objectID": "asymptotics/ols.html#asymptotic-distribution-1",
    "href": "asymptotics/ols.html#asymptotic-distribution-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\beta} - \\beta) = & \\sqrt{n} (X'X)^{-1} (X' \\epsilon) \\\\\n= & (\\frac{1}{n} X'X)^{-1} \\frac{1}{\\sqrt{n}} X' \\epsilon\n\\end{align*}\n\\]\n\\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\) if:\n\nUsing Lindeberg’s CLT and non-iid WLLN:\n\n\\((X_i, \\epsilon_i) \\perp (X_j, \\epsilon_j)\\) if \\(i \\neq j\\), and\n\\(\\frac{1}{n} \\max_{1  \\leq i \\leq n} \\Er[ (X_i X_i' - \\Er[X_i X_i']) (X_i X_i' - \\Er[X_i X_i])'] \\to 0\\), and\n\\(\\frac{1}{n} \\sum_{i=1}^n \\Er[X_i \\epsilon_i^2 X_i'] = \\Omega_n\\) is non singular, and \\(\\Omega_n \\to \\Omega\\), and\n\\(\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\Er\\left[ c'( X_i\n\\epsilon_i^2 X_i') c 1\\{ | c'( X_i \\epsilon_i)| &gt; \\delta\n\\sqrt{n} \\} \\right]  = 0\\) for all \\(\\delta &gt; 0\\) and \\(c \\in \\R^k\\)\nExercise: what is \\(\\Sigma\\) under these assumptions?"
  },
  {
    "objectID": "asymptotics/ols.html#estimated-variance",
    "href": "asymptotics/ols.html#estimated-variance",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Estimated Variance",
    "text": "Estimated Variance\n\nKnowing \\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\) isn’t useful, unless we know or can estimate \\(\\Sigma\\)\n\n\n\n\nLemma\n\n\nIf \\(\\hat{\\Sigma} \\inprob \\Sigma\\), \\(\\Sigma\\) is nonsingular, and \\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\), then \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta)\\hat{\\Sigma}^{-1/2} \\indist N(0, I)\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedasticity",
    "href": "asymptotics/ols.html#heteroskedasticity",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWith i.i.d. data, \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\overbrace{\\Er[X_i X_i']^{-1} \\var(X_i \\epsilon_i) \\Er[X_i X_i']^{-1}}^{\\Sigma} )\n\\]\n\\(Var(X_i \\epsilon_i) = \\Er[\\epsilon_i^2 X_i X_i'] = \\Er[\\Er[\\epsilon_i^2 |X_i] X_i X_i']\\)\n\n\n\n\nDefinition\n\n\n\\(\\epsilon\\) is homoskedastic if \\(\\Er[\\epsilon_i^2 | X_i] = \\sigma^2\\), otherwise \\(\\epsilon\\) is heteroskedastic."
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedasticity-1",
    "href": "asymptotics/ols.html#heteroskedasticity-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWith homoskedasticity, \\(\\Sigma = \\Er[X_i X_i']^{-1} \\sigma^2\\)\nWith heteroskedasticity, \\(\\Sigma = \\Er[X_i X_i']^{-1} \\var(X_i \\epsilon_i) \\Er[X_i X_i']^{-1}\\) and can be (with appropriate assumptions) consistently estimated by \\[\n\\hat{\\Sigma}^{robust} = (\\frac{1}{n} X ' X)^{-1} \\left(\\frac{1}{n} \\sum_{i=1}^n X_i X_i' \\epsilon_i^2 \\right) (\\frac{1}{n} X'X)^{-1}\n\\]\nEven with homoskedasticity, there is little downside to using \\(\\hat{\\Sigma}^{robust}\\), so always used in practice"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data\n\nusing Statistics, LinearAlgebra\n\nfunction sim(n,k; β=ones(k), σ = x-&gt;1)\n  X = randn(n,k)\n  ϵ = randn(n).*mapslices(σ, X, dims=[2])\n  y = X*β + ϵ\n  return(X,y)\nend\n\nfunction ols(X,y)\n  XXfactored = cholesky(X'*X)\n  β̂ = XXfactored \\ X'*y\n  Vr, Vh = olsvar(X, y - X*β̂, XXfactored)\n  return(β̂, Vr, Vh)\nend\n\nfunction olsvar(X, ϵ, XXf)\n  n, k = size(X)\n  iXX = inv(XXf)\n  @views @inbounds Vr = n/(n-k)*iXX*sum(X[i,:]*X[i,:]'*ϵ[i]^2 for i ∈ axes(X)[1])*iXX\n  Vh = n/(n-k)*iXX*var(ϵ)\n  return(Vr,Vh)\nend\n\nolsvar (generic function with 1 method)"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-1",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data\n\n\nCode\nusing PlotlyLight, Distributions, Cobweb\nfunction simsize(n,k,S; β=ones(k), σ=x-&gt;1)\n  z = zeros(S,2)\n  for s ∈ 1:S\n    X,y = sim(n,k,β=β,σ=σ)\n    β̂, Vr, Vh = ols(X,y)\n    z[s,1] = (β̂[1] - β[1])/sqrt(Vr[1,1])\n    z[s,2] = (β̂[1] - β[1])/sqrt(Vh[1,1])\n  end\n  p = cdf.(Normal(),z)\n  return(p, z)\nend\nfunction makeplot(p,z,n;   u = range(0,1,length=100))\n  plt = Plot()\n  plt.layout = Config()\n  plt.layout.title=\"N=$n\"\n  plt.layout.yaxis.title.text=\"x - P(asymptotic p-value &lt; x)\"\n  plt.layout.xaxis.title.text=\"x\"\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,1].&lt;=x)).(u)), name=\"Heteroskedasticity Robust\")\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,2].&lt;=x)).(u)), name=\"Homoskedasticity\")\n  return(plt())\nend\nN = [100, 500, 2500, 10_000]\nk = 2\nfor n ∈ N\n  if !isfile(\"size_$n.html\")\n    fig = makeplot(simsize(n,k,10_000)...,n)\n    PlotlyLight.save(fig, \"size_$n.html\")\n  end\nend"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-2",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-3",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-3",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-4",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-4",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#plot-of-residuals-vs-predictions",
    "href": "asymptotics/ols.html#plot-of-residuals-vs-predictions",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Plot of Residuals vs Predictions",
    "text": "Plot of Residuals vs Predictions\n\n\nCode\nn = 250\nk = 3\nXh,yh = sim(n,k)\nbh, _, _ = ols(Xh,yh)\neh = yh - Xh*bh\nX,y = sim(n,k, σ=x-&gt;(0.1 + norm(x'*ones(k) .+ 3)/3))\nb, _, _ = ols(X,y)\ne = y - X*b\n\nplt = Plot()\nplt.layout = Config()\nplt(x = vec(Xh*bh), y=vec(eh), name=\"Homoskedastic\", mode=\"markers\", type=\"scatter\")\nfig =plt(x = vec(X*b), y = vec(e), name=\"Heteroskedastic\", mode=\"markers\")\nPlotlyLight.save(fig, \"resid.html\")\nHTML(\"&lt;iframe src=\\\"resid.html\\\" width=\\\"1000\\\"  height=\\\"650\\\"&gt;&lt;/iframe&gt;\\n\")"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data\n\n\nCode\nN = [100, 500, 2500, 10_000]\nk = 2\nfor n ∈ N\n  if !isfile(\"size_het_$n.html\")\n    fig = makeplot(simsize(n,k,10_000, σ=x-&gt;(0.1 + norm(x'*ones(k) .+ 3)/3))...,n)\n    PlotlyLight.save(fig, \"size_het_$n.html\")\n  end\nend"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-1",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-2",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-3",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-3",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#dependence",
    "href": "asymptotics/ols.html#dependence",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Dependence",
    "text": "Dependence\n\n\\(\\sqrt{n} (\\hat{\\beta} - \\beta) \\indist N(0, \\Er[X_iX_i']^{-1} V\n\\Er[X_i X_i']^{-1})\\) if \\(\\frac{1}{n} X'X \\inprob \\Er[X_iX_i']\\) and \\(\\frac{1}{n} X'\\epsilon \\indist N(0,V)\\)\nGenerally, \\(V = \\lim_{n \\to \\infty} \\var\\left( \\frac{1}{n} X'\\epsilon \\right)\\) \\[\n\\var\\left( \\frac{1}{n} X'\\epsilon \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\cov(X_i \\epsilon_i, X_j \\epsilon_j)\n\\]\n\\(n(n-1)/2\\) different \\(\\cov(X_i \\epsilon_i, X_j \\epsilon_j)\\), so need some restriction"
  },
  {
    "objectID": "asymptotics/ols.html#clustered-standard-errors",
    "href": "asymptotics/ols.html#clustered-standard-errors",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustered Standard Errors",
    "text": "Clustered Standard Errors\n\nPartition data into \\(G\\) groups, \\(\\{g_h\\}_{h=1}^G\\), denote group of \\(i\\) as \\(g(i)\\)\nAssume \\(\\cov(X_i \\epsilon_i, X_j \\epsilon_j) = 0\\) if \\(g(i) \\neq\ng(j)\\), and \\(\\Er[X_i \\epsilon_i] = 0\\)\n\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n \\sum_{j=1}^n \\cov(X_i \\epsilon_i, X_j \\epsilon_j) = & \\sum_{h=1}^G \\sum_{i \\in g_h} \\sum_{j \\in g_h} \\cov(X_i \\epsilon_i, X_j, \\epsilon_j) \\\\\n= & \\sum_{h=1}^G \\Er\\left[\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)^2 \\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#clustered-standard-errors-1",
    "href": "asymptotics/ols.html#clustered-standard-errors-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustered Standard Errors",
    "text": "Clustered Standard Errors\n\nStrengthening some assumptions to apply Lindeberg’s CLT to \\(\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)\\) we get\n\n\\[\n\\frac{1}{\\sqrt{G}} \\left(\\sum_{h = 1}^G \\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right) \\right) \\left( \\frac{1}{G} \\sum_{h=1}^G \\Er\\left[\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)^2 \\right] \\right)^{-1/2} \\indist N(0, I)\n\\]\n\nThus,\n\n\\[\n\\sqrt{G} (\\hat{\\beta} - \\beta) \\indist N\\left(0, \\Er[X_i X_i']^{-1} \\left(\\lim_{G \\to \\infty} \\frac{1}{G} \\sum_{h=1}^G \\Er\\left[\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)^2 \\right] \\right)  \\Er[X_i X_i']^{-1} \\right)\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#clustering",
    "href": "asymptotics/ols.html#clustering",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering\n\nfunction olscl(X,y, group=group)\n  XXfactored = cholesky(X'*X)\n  β̂ = XXfactored \\ X'*y\n  V = olsclvar(X, y - X*β̂, XXfactored, group=group)\n  return(β̂, V)\nend\n\nfunction olsclvar(X, ϵ, XXf; group=axes(X)[1])\n  n, k = size(X)\n  groups=unique(group)\n  G = length(groups)\n  iXX = inv(XXf)\n  @views @inbounds Vr = G/(G-k)*iXX*\n    sum(X[group.==g,:]'*ϵ[group.==g]*ϵ[group.==g]'*X[group.==g,:] for  g in groups)*iXX\n  return(Vr)\nend\n\nfunction simcluster(n,k,G; β=ones(k), σ = x-&gt;1, ρ=0.7)\n  X = randn(n,k)\n  group = rand(1:G,n)\n  for g ∈ 1:G # ensure all groups included\n    if sum(group.==g)==0\n      group[g]=g\n    end\n  end\n  X[:,1] .+= (group.&gt;(G/2))\n  u = randn(G)\n  ϵ = (ρ*u[group] + sqrt(1-ρ^2)*randn(n)).*mapslices(σ, X, dims=[2])\n  y = X*β + ϵ\n  return(X,y, group)\nend\n\nfunction simsizecluster(n,k,G,S; β=ones(k), σ=x-&gt;1, ρ=0.7)\n  z = zeros(S,2)\n  for s ∈ 1:S\n    X,y,group = simcluster(n,k,G,β=β,σ=σ, ρ=ρ)\n    β̂, Vcr = olscl(X,y, group)\n    _, Vr, _ = ols(X,y)\n    z[s,1] = (β̂[1] - β[1])/sqrt(Vcr[1,1])\n    z[s,2] = (β̂[1] - β[1])/sqrt(Vr[1,1])\n  end\n  p = hcat(cdf.(Normal(),z), cdf.(TDist(G-k),z))\n  return(p, z)\nend\n\nfunction makeplotcl(p,z,n,g; u = range(0,1,length=100))\n  plt = Plot()\n  plt.layout = Config()\n  plt.layout.title=\"N=$n, G=$g\"\n  plt.layout.yaxis.title.text=\"x - P(asymptotic p-value &lt; x)\"\n  plt.layout.xaxis.title.text=\"x\"\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,1].&lt;=x)).(u)), name=\"Clustered (Normal Distribution)\")\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,2].&lt;=x)).(u)), name=\"Heteroskedasticity Robust\")\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,3].&lt;=x)).(u)), name=\"Clustered (t-Distribution)\")\n  return(plt())\nend\n\nN = [200, 200, 5000, 5000]\nG = [10, 100, 10, 100]\nk = 2\nfor (n,g) ∈ zip(N,G)\n  if !isfile(\"size_cluster_$(n)_$g.html\")\n    fig = makeplotcl(simsizecluster(n,k,g,10_000)...,n,g)\n    PlotlyLight.save(fig, \"size_cluster_$(n)_$g.html\")\n  end\nend"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-1",
    "href": "asymptotics/ols.html#clustering-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-2",
    "href": "asymptotics/ols.html#clustering-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-3",
    "href": "asymptotics/ols.html#clustering-3",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-4",
    "href": "asymptotics/ols.html#clustering-4",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence",
    "href": "asymptotics/ols.html#time-dependence",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence",
    "text": "Time Dependence\n\nReferences:\n\nMikusheva and Schrimpf (2007) lectures 2 and 3 and recitation 2\nDedecker et al. (2007) for comprehensive treatment"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---lln",
    "href": "asymptotics/ols.html#time-dependence---lln",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - LLN",
    "text": "Time Dependence - LLN\n\nRecall simplest LLN proof via Markov’s inequality and showing \\(\\var\\left(\\frac{1}{n} \\sum_{i=1}^n x_i\\right) \\to 0\\)\nWith dependence,\n\n\\[\n\\begin{align*}\n\\var\\left(\\frac{1}{n} \\sum_{i=1}^n x_i\\right) = & \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\cov(x_i,x_j) \\\\\n& \\text{assume covariance stationarity:} \\cov(x_i, x_j) = \\gamma_{|i-j|} \\\\\n= & \\frac{1}{n^2} \\left(n \\gamma_0 + 2(n-1) \\gamma_1 + \\cdots\\right) \\\\\n= & \\frac{1}{n} \\left[ \\gamma_0 + 2 \\sum_{k=1}^n \\gamma_k \\left(1 - \\frac{k}{n} \\right)  \\right]\n\\end{align*}\n\\]\nif \\(\\sum_{j=-\\infty}^{\\infty} |\\gamma_j| &lt; \\infty\\), then \\(\\frac{1}{n}\n\\left[ \\gamma_0 + 2 \\sum_{k=1}^n \\gamma_k \\left(1 - \\frac{k}{n}\n  \\right)  \\right]\\to 0\\) and LLN holds"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---clt",
    "href": "asymptotics/ols.html#time-dependence---clt",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - CLT",
    "text": "Time Dependence - CLT\n\nLong run variance: \\[\n\\begin{align*}\n\\frac{1}{\\sqrt{n}} \\var\\left( \\sum_{i=1}^n x_i\\right) = &  \\gamma_0 + 2 \\sum_{k=1}^n \\gamma_k \\left(1 - \\frac{k}{n} \\right)  \\\\\n\\to & \\gamma_0 + 2 \\sum_{k=1}^\\infty \\gamma_k \\equiv \\mathcal{J} = \\text{long-run variance}\n\\end{align*}\n\\]\nWith appropriate assumptions if \\(\\Er[x_i] = 0\\) and \\(\\mathcal{J} &lt; \\infty\\), then \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n x_i \\indist N(0,\\mathcal{J})\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---gordins-clt",
    "href": "asymptotics/ols.html#time-dependence---gordins-clt",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - Gordin’s CLT",
    "text": "Time Dependence - Gordin’s CLT\n\n\n\nCLT\n\n\nAssume \\(\\Er[y_t] = 0\\). Let \\(I_t\\) be the sigma-algebra generated by \\(\\{y_j\\}_{j=-\\infty}^t\\). Further assume:\n\n\\(y_t\\) is strictly stationary: the distribution of \\(y_{t_1}, ... , y_{t_k}\\) equals the distribution of \\(y_{t_1 + s} , ... y_{t_k+s}\\) for all \\(t_j\\) and \\(s\\)\n\\(y_t\\) is ergodic \\(\\lim_{s\\to\\infty} \\cov(g(y_t,..., y_{t+k}),\nh(y_{t+k+s}, ..., y_{t+k+s+l})) = 0\\) for all bounded \\(g\\), \\(h\\)\n\\(\\sum_{j=1}^\\infty \\left(\\Er\\left[ (\\Er[y_t|I_{t-j}] -\n\\Er[y_t|I_{t-j-1}])^2\\right] \\right)^{1/2} &lt; \\infty\\)\n\\(\\Er[y_t | I_{t-j}] \\to 0\\) as \\(j \\to \\infty\\)\n\nThen, \\[\n\\frac{1}{\\sqrt{T}} \\sum_{t=1}^T y_t \\indist N(0,\\mathcal{J})\n\\]\n\n\n\n\nMany variations of assumptions possible, see e.g. Dedecker et al. (2007) for more"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---ols",
    "href": "asymptotics/ols.html#time-dependence---ols",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - OLS",
    "text": "Time Dependence - OLS\n\nIf LLN applies \\(X'X\\) and CLT to \\(X \\epsilon\\), then \\[\n\\sqrt{n} (\\hat{\\beta} - \\beta) \\indist N(0, M^{-1} \\mathcal{J} M^{-1})\n\\] with \\(M = \\plim \\frac{1}{n} X'X\\) and \\(\\mathcal{J} = \\var(X_i \\epsilon_i) + 2\\sum_{k=1}^\\infty \\cov(X_i \\epsilon_i, X_{i+k} \\epsilon_{i+k})\\)\nConsistently estimate \\(\\mathcal{J}\\) by (Newey and West (1987)) \\[\n\\hat{\\mathcal{J}} = \\sum_{-S_n}^{S_n}k_n(j) \\underbrace{\\hat{\\gamma}_j}_{=\\frac{1}{n} \\sum_{i=1}^{n-j} (X_i \\hat{\\epsilon}_i) (X_{i+j} \\hat{\\epsilon}_{i+j})'}\n\\] with \\(k_n(j) \\to 1\\), and \\(S_n \\to \\infty\\) and \\(S_n^3/n \\to 0\\)"
  },
  {
    "objectID": "asymptotics/ols.html#wald",
    "href": "asymptotics/ols.html#wald",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Wald",
    "text": "Wald\n\nIf \\(\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\indist N(0, V)\\), then \\[\n\\sqrt{n}(R \\hat{\\beta} - \\underbrace{R\\beta_0}_{=r} ) \\indist N(0, RVR')\n\\] and \\[\nW \\equiv n(R \\hat{\\beta} - r )' (RVR')^{-1} (R \\hat{\\beta} - r ) \\indist \\chi^2_{rank(R)}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#restricted-mle",
    "href": "asymptotics/ols.html#restricted-mle",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Restricted MLE",
    "text": "Restricted MLE\n\nRestricted MLE with \\(\\epsilon \\sim N(0, \\sigma^2 I_n)\\) \\[\n\\hat{\\beta}_R = \\mathrm{arg}\\max_{b: Rb = r, \\sigma} -\\frac{n}{2}(\\log 2\\pi + \\log \\sigma^2) + \\sum_{i=1}^n \\frac{-1}{2\\sigma^2} (y_i - X_i b)^2\n\\]\nFOC \\[\n\\begin{align*}\n-X'y/\\sigma^2 + X'X\\hat{\\beta}_R/\\sigma^2 + R'\\hat{\\lambda} & = 0\\\\\nR\\hat{\\beta}_R - r & = 0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#lagrange-multiplier",
    "href": "asymptotics/ols.html#lagrange-multiplier",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Lagrange Multiplier",
    "text": "Lagrange Multiplier\n\nUnder \\(H_0\\), \\(\\lambda = 0\\), so form test statistic based on \\(\\hat{\\lambda} \\approx 0\\)\nFrom FOC: \\[\nR'\\hat{\\lambda} = X'(y - X'\\hat{\\beta}_R)/\\hat{\\sigma}^2_R = X'\\hat{\\epsilon}_r/\\hat{\\sigma}^2_R\n\\]\nTo find distribution, note that \\[\n\\begin{pmatrix}\n\\hat{\\beta}_R \\\\\n\\hat{\\lambda}/2\n\\end{pmatrix} =\n\\begin{pmatrix}\nX'X/\\sigma^2 & R' \\\\\n-R & 0\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\nX'y/\\sigma^2 \\\\\n-r\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#lagrange-multiplier-1",
    "href": "asymptotics/ols.html#lagrange-multiplier-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Lagrange Multiplier",
    "text": "Lagrange Multiplier\n\nso (using partitioned inverse), \\[\n\\hat{\\lambda} = (R\\hat{\\sigma}_R^2 (X'X)^{-1} R')^{-1} (R \\hat{\\beta} - r)\n\\] and \\[\n\\hat{\\beta}_R = \\hat{\\beta} - (X'X)^{-1}(R (X'X)^{-1} R)^{-1} (R \\hat{\\beta} - r)\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#lagrange-multiplier-2",
    "href": "asymptotics/ols.html#lagrange-multiplier-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Lagrange Multiplier",
    "text": "Lagrange Multiplier\n\nNote that \\[\n\\begin{align*}\n\\hat{\\lambda} & = (R\\hat{\\sigma}_R^2 (X'X)^{-1} R')^{-1} (R \\hat{\\beta} - r) \\\\\n& = (R\\hat{\\sigma}_R^2 (X'X)^{-1} R')^{-1} (R (X'X)^{-1} X'\\epsilon)\n\\end{align*}\n\\]\nso, with homoskedasticity, \\[\nLM = \\hat{\\lambda}'R \\hat{\\sigma}^2_R (X'X)^{-1} R' \\hat{\\lambda} \\indist \\chi^2_{rank(R)}\n\\]\nCan modify for heteroskedasticity and/or dependence"
  },
  {
    "objectID": "asymptotics/ols.html#likelihood-ratio",
    "href": "asymptotics/ols.html#likelihood-ratio",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Likelihood Ratio",
    "text": "Likelihood Ratio\n\nIf \\(\\epsilon_i \\sim N(0, \\sigma^2)\\), twice the log likelihood ratio for \\(H_0: R\\beta = r\\), \\[\n\\begin{align*}\n2\\max_{b,\\sigma} & \\left[-\\frac{n}{2} \\log \\sigma^2 + \\sum_{i=1}^n \\frac{-1}{2\\sigma^2} (y_i - X_i b)^2 \\right] - \\\\\n& - 2\\max_{b,\\sigma: Rb = r} \\left[-\\frac{n}{2} \\log \\sigma^2 + \\sum_{i=1}^n \\frac{-1}{2\\sigma^2} (y_i - X_i b)^2 \\right] = \\\\\n= & -n\\log \\hat{\\sigma}^2 + n \\log\\hat{\\sigma}_R^2 \\\\\n= & -n\\log\\left(\\frac{1}{n} \\norm{y-X\\hat{\\beta}}^2\\right) + n \\log \\left(\\frac{1}{n}\\norm{y - X \\hat{\\beta}_R}^2 \\right) \\\\\n= & n \\log \\left(\\frac{\\frac{1}{n}\\norm{y-X\\hat{\\beta}}^2 + \\frac{1}{n}(\\hat{\\beta}_R-\\hat{\\beta}) X'X (\\hat{\\beta}_R-\\hat{\\beta})}{\\frac{1}{n}\\norm{y-X\\hat{\\beta}}^2}\\right) \\\\\n= & n \\log (1 + W/n) \\indist \\chi^2_{rank(R)} (\\text{with homoskedasticity})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#reading",
    "href": "asymptotics/indistribution.html#reading",
    "title": "Convergence in Distribution",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 10\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#convergence-in-distribution-1",
    "href": "asymptotics/indistribution.html#convergence-in-distribution-1",
    "title": "Convergence in Distribution",
    "section": "Convergence in Distribution",
    "text": "Convergence in Distribution\n\n\n\nDefinition\n\n\nRandom vectors \\(X_1, X_2, ...\\) converge in distribution to the random vector \\(X\\) if for all \\(f \\in \\underbrace{\\mathcal{C}_b}\\) (continuous and bounded) \\[\n\\Er[ f(X_n) ] \\to \\Er[f(X)]\n\\] denoted by \\(X_n \\indist X\\)\n\n\n\n\n\nrelate to convergence of the linear operator from \\(\\mathcal{C}_b \\to \\mathbbm{R}\\)\nNote important implication for test statistics"
  },
  {
    "objectID": "asymptotics/indistribution.html#relation-to-convergence-in-probability",
    "href": "asymptotics/indistribution.html#relation-to-convergence-in-probability",
    "title": "Convergence in Distribution",
    "section": "Relation to Convergence in Probability",
    "text": "Relation to Convergence in Probability\n\n\n\nTheorem 1.4\n\n\n\nIf \\(X_n \\indist X\\), then \\(X_n = O_p(1)\\)\nIf \\(c\\) is a constant, then \\(X_n \\inprob c\\) iff \\(X_n \\indist c\\)\nIf \\(Y_n \\inprob c\\) and \\(X_n \\indist X\\), then \\((Y_n, X_n) \\indist (c, X)\\)\nIf \\(X_n \\inprob X\\), then \\(X_n \\indist X\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#slutskys-lemma",
    "href": "asymptotics/indistribution.html#slutskys-lemma",
    "title": "Convergence in Distribution",
    "section": "Slutsky’s Lemma",
    "text": "Slutsky’s Lemma\n\n\n\nTheorem 1.5 (Generalized Slutsky’s Lemma)\n\n\nIf \\(Y_n \\inprob c\\), \\(X_n \\indist X\\), and \\(g\\) is continuous, then \\[\ng(Y_n, X_n) \\indist g(c,X)\n\\]\n\n\n\n\nImplies:\n\n\\(Y_n + X_n \\indist c + X\\)\n\\(Y_n X_n \\indist c X\\)\n\\(X_n/Y_n \\indist X/c\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#levys-continuity-theorem",
    "href": "asymptotics/indistribution.html#levys-continuity-theorem",
    "title": "Convergence in Distribution",
    "section": "Levy’s Continuity Theorem",
    "text": "Levy’s Continuity Theorem\n\n\n\nLemma 2.1 (Levy’s Continuity Theorem)\n\n\n\\(X_n \\indist X\\) iff \\(\\Er[e^{i t'X_n} ] \\to \\Er[e^{i t' X} ]\\) for all \\(t \\in \\R^d\\)\n\n\n\n\nsee Döbler (2022) for a short proof\n\\(\\Er[e^{i t' X}] \\equiv \\varphi(t)\\) is the characteristic function of \\(X\\)\n\n\n\nhttp://theanalysisofdata.com/probability/8_8.html\nDöbler (2022)\nhttps://terrytao.wordpress.com/2010/01/05/254a-notes-2-the-central-limit-theorem/#berry"
  },
  {
    "objectID": "asymptotics/indistribution.html#law-of-large-numbers-revisited",
    "href": "asymptotics/indistribution.html#law-of-large-numbers-revisited",
    "title": "Convergence in Distribution",
    "section": "Law of Large Numbers Revisited",
    "text": "Law of Large Numbers Revisited\n\n\n\nLemma 2.2 (Weak Law of Large Numbers)\n\n\nIf \\(X_1, ..., X_n\\) are i.i.d. with \\(\\Er[|X_1|] &lt; \\infty\\), then \\(\\frac{1}{n} \\sum_{i=1}^n X_i \\inprob \\Er[X_1]\\)\n\n\n\n\n\n\n\nTheorem 2.2 (non-iid WLLN)\n\n\nIf \\(\\Er[X_i]=0\\), \\(\\Er[X_i X_j] = 0\\) for all \\(i \\neq j\\) and \\(\\frac{1}{n} \\max_{1 \\leq\n  j \\leq n} \\Er[X_j^2] \\to 0\\), then \\(\\frac{1}{n} \\sum_{i=1}^n X_i \\inprob 0\\)\n\n\n\n\n\nApply to OLS again and show \\(\\hat{\\sigma}^2\\) is consistent"
  },
  {
    "objectID": "asymptotics/indistribution.html#central-limit-theorem-1",
    "href": "asymptotics/indistribution.html#central-limit-theorem-1",
    "title": "Convergence in Distribution",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\nTheorem 2.3\n\n\nSuppose \\(X_1, ..., X_n \\in \\R\\) are i.i.d. with \\(\\Er[X_1] = \\mu\\) and \\(\\var(X_1) = \\sigma^2\\), then \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma} \\indist N(0,1)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#cdf",
    "href": "asymptotics/indistribution.html#cdf",
    "title": "Convergence in Distribution",
    "section": "CDF",
    "text": "CDF\n\n\nplotting code\nplt = Plot()\nplt.layout = Config()\nfor i in axes(N)[1]\n  plt(x=x, y=Fn[i].(x), name=\"N=$(N[i])\")\nend\nplt(x=x, y=cdf.(Normal(),x), name=\"Normal CDF\")\nfig = plt()\nfig"
  },
  {
    "objectID": "asymptotics/indistribution.html#size-distortion",
    "href": "asymptotics/indistribution.html#size-distortion",
    "title": "Convergence in Distribution",
    "section": "Size Distortion",
    "text": "Size Distortion\n\n\nplotting code\np = range(0,1,length=200)\nplt = Plot()\nplt.layout = Config()\nplt.layout.yaxis.title.text=\"p - Fn(Φ^{-1}(p))\"\nplt.layout.xaxis.title.text=\"p\"\nfor i in axes(N)[1]\n  plt(x=p,y=p - Fn[i].(quantile.(Normal(),p)), name=\"N=$(N[i])\")\nend\nfig = plt()\nfig"
  },
  {
    "objectID": "asymptotics/indistribution.html#histogram",
    "href": "asymptotics/indistribution.html#histogram",
    "title": "Convergence in Distribution",
    "section": "Histogram",
    "text": "Histogram\n\n\nplotting code\nbins = range(-2.5,2.5,length=21)\nx = range(-2.5,2.5, length=1000)\nplt = Plot();\nplt.layout=Config()\nfunction Hn(x, F)\n  if (x &lt;= bins[1] || x&gt;bins[end])\n    return 0.0\n  end\n  j = findfirst( x .&lt;= bins )\n  return (F(bins[j]) - F(bins[j-1]))\nend\nplt(x=x,y=Hn.(x,x-&gt;cdf(Normal(),x)), name=\"Normal\")\nfor i in axes(N)[1]\n  plt(x=x,y=Hn.(x, Fn[i]), name=\"N=$(N[i])\")\nend\nfig = plt()\nfig"
  },
  {
    "objectID": "asymptotics/indistribution.html#cramér-wold-device",
    "href": "asymptotics/indistribution.html#cramér-wold-device",
    "title": "Convergence in Distribution",
    "section": "Cramér-Wold Device",
    "text": "Cramér-Wold Device\n\n\n\nLemma 2.2\n\n\nFor \\(X_n, X \\in \\R^d\\), \\(X_n \\indist X\\) iff \\(t' X_n \\indist t' X\\) for all \\(t \\in \\R^d\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#multivariate-central-limit-theorem",
    "href": "asymptotics/indistribution.html#multivariate-central-limit-theorem",
    "title": "Convergence in Distribution",
    "section": "Multivariate Central Limit Theorem",
    "text": "Multivariate Central Limit Theorem\n\n\n\nTheorem 2.4\n\n\nSuppose \\(X_1, ..., X_n\\) are i.i.d. with \\(\\Er[X_1] = \\mu \\in \\R^d\\) and \\(\\var(X_1) = \\Sigma &gt; 0\\), then \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n (X_i - \\mu) \\indist N(0,\\Sigma)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#delta-method",
    "href": "asymptotics/indistribution.html#delta-method",
    "title": "Convergence in Distribution",
    "section": "Delta Method",
    "text": "Delta Method\n\n\n\nTheorem 3.1 (Delta Method)\n\n\nSuppose that \\(\\hat{\\theta}\\) is a sequence of estimators of \\(\\theta_0 \\in \\R^d\\), and \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist S\n\\] Also, assume that \\(h: \\R^d \\to \\R^k\\) is differentiable at \\(\\theta_0\\), then \\[\n\\sqrt{n} \\left( h(\\hat{\\theta}) - h(\\theta_0) \\right) \\indist Dh(\\theta_0) S\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#delta-method-example",
    "href": "asymptotics/indistribution.html#delta-method-example",
    "title": "Convergence in Distribution",
    "section": "Delta Method: Example",
    "text": "Delta Method: Example\n\nWhat is the asymptotic distribution of \\[\n\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\n\\sum_{i=1}^n \\left(x_i - \\frac{1}{n} \\sum_{j=1}^n x_j \\right)^2}?\n\\]\n\n\nWe need some additional assumptions for this to have good answer. At a high level, we want \\(\\frac{1}{n} \\sum x_i \\inprob \\Er[x]\\), and \\(\\frac{1}{n} \\sum_{i=1}^n \\left(x_i - \\frac{1}{n} \\sum_j=1^n x_j \\right)^2 - \\sigma^2 \\indist W\\) for some known random variable \\(W\\). Sufficient more primitive assumptions would be that \\(x_i\\) is i.i.d. with mean \\(\\mu\\), variance \\(\\sigma^2&gt;0\\), and finite fourth moment."
  },
  {
    "objectID": "asymptotics/indistribution.html#continuous-mapping-theorem",
    "href": "asymptotics/indistribution.html#continuous-mapping-theorem",
    "title": "Convergence in Distribution",
    "section": "Continuous Mapping Theorem",
    "text": "Continuous Mapping Theorem\n\n\n\nContinuous Mapping Theorem\n\n\nLet \\(X_n \\indist X\\) and \\(g\\) be continuous on a set \\(C\\) with \\(P(X \\in C) = 1\\), then \\[\ng(X_n) \\indist g(X)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#continuous-mapping-theorem-example",
    "href": "asymptotics/indistribution.html#continuous-mapping-theorem-example",
    "title": "Convergence in Distribution",
    "section": "Continuous Mapping Theorem: Example",
    "text": "Continuous Mapping Theorem: Example\n\nIn linear regression, \\[\ny_i = x_i'\\beta_0 + \\epsilon_i\n\\]\nWhat is the asymptotic distribution of \\[\nM(\\beta) = \\left\\Vert \\frac{1}{\\sqrt{n}} \\sum_{i=1} x_i (y_i - x_i'\\beta) \\right\\Vert^2\n\\] when \\(\\beta=\\beta_0\\)?"
  },
  {
    "objectID": "asymptotics/indistribution.html#i.-non-i.d.-central-limit-theorem",
    "href": "asymptotics/indistribution.html#i.-non-i.d.-central-limit-theorem",
    "title": "Convergence in Distribution",
    "section": "i. non i.d. Central Limit Theorem",
    "text": "i. non i.d. Central Limit Theorem\n\nTriangular array \\[\n\\begin{array}{ccc}\nX_{1,1}, & ..., & X_{1,k(1)} \\\\\nX_{2,1}, & ..., & X_{2,k(2)} \\\\\n\\vdots & & \\vdots \\\\\nX_{n,1}, & ..., & X_{n,k(n)}\n\\end{array}\n\\] with \\(k(n) \\to \\infty\\) as \\(n \\to \\infty\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#i.-non-i.d.-central-limit-theorem-1",
    "href": "asymptotics/indistribution.html#i.-non-i.d.-central-limit-theorem-1",
    "title": "Convergence in Distribution",
    "section": "i. non i.d. Central Limit Theorem",
    "text": "i. non i.d. Central Limit Theorem\n\n\n\nTheorem 2.5 (Lindeberg’s Theorem)\n\n\nAssume that for each \\(n\\), \\(X_{n,1}, ..., X_{n,k(n)}\\) are independent with \\(\\Er[X_{nj}] = 0\\), and \\(\\frac{1}{k(n)} \\sum_{j=1}^{k(n)} \\Er[X_{nj}^2]  = 1\\) and for any \\(\\epsilon&gt;0\\), \\[\n\\lim_{n \\to \\infty} \\frac{1}{k(n)} \\sum_{j=1}^{k(n)} \\Er\\left[ X_{nj}^2 1\\{|X_{nj}|&gt;\\epsilon \\sqrt{k(n)}  \\right]  = 0\n\\] Then, \\[\n\\frac{1}{\\sqrt{k(n)}} \\sum_{j=1}^{k(n)} X_{n,j} \\indist N(0,1)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#characterizing-convergence-in-distribution-1",
    "href": "asymptotics/indistribution.html#characterizing-convergence-in-distribution-1",
    "title": "Convergence in Distribution",
    "section": "Characterizing Convergence in Distribution",
    "text": "Characterizing Convergence in Distribution\n\n\n\nLemma 1.2\n\n\n\\(X_n \\indist X\\) iff for any open \\(G \\subset \\R^d\\), \\[\n\\liminf P(X_n \\in G) \\geq P(X \\in G)\n\\]\n\n\n\n\nThis and additional characterizations of convergence in distribution are called the Portmanteau Theorem"
  },
  {
    "objectID": "asymptotics/indistribution.html#characterizing-convergence-in-distribution-2",
    "href": "asymptotics/indistribution.html#characterizing-convergence-in-distribution-2",
    "title": "Convergence in Distribution",
    "section": "Characterizing Convergence in Distribution",
    "text": "Characterizing Convergence in Distribution\n\n\n\nTheorem 1.1\n\n\nIf \\(X_n \\indist X\\) if and only if \\(P(X_n \\leq t) \\to P(X \\leq t)\\) for all \\(t\\) where \\(P(X \\leq t)\\) is continuous\n\n\n\n\n\n\n\nTheorem 1.2\n\n\nIf \\(X_n \\indist X\\) and \\(X\\) is continuous, then \\[\n\\sup_{t \\in \\R^d} | P(X_n \\leq t) - P(X \\leq t) | \\to 0\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#weak-berry-esseen-theorem",
    "href": "asymptotics/indistribution.html#weak-berry-esseen-theorem",
    "title": "Convergence in Distribution",
    "section": "Weak Berry-Esseen Theorem",
    "text": "Weak Berry-Esseen Theorem\n\n\n\n\nWeak Berry-Esseen Theorem\n\n\nLet \\(X_i\\) be i.i.d with \\(\\Er[X]=0\\), \\(\\Er[X^2]=1\\) and \\(\\Er[|X|^3]\\) finite. Let \\(\\varphi\\) be smooth with its first three derivatives uniformly bounded, and let \\(G \\sim N(0,1)\\). Then \\[\n\\left\\vert \\Er\\left[ \\varphi\\left( \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i \\right) \\right] -\n\\Er\\left[\\varphi(G)\\right]\n\\right\\vert \\leq C \\frac{\\Er[|X|^3]}{\\sqrt{n}} \\sup_{x \\in \\R} |\\varphi'''(x)|\n\\]\n1\n\n\n\n\n\nThis is based on Tao (2010) https://terrytao.wordpress.com/2010/01/05/254a-notes-2-the-central-limit-theorem/#berry\nProof\nLet \\(G_i \\sim N(0,1)\\). Define \\[\nZ_{n,i} = \\frac{X_1 + \\cdots X_i + G_{i+1} + \\cdots G_n}{\\sqrt{n}},\n\\] so that $Z_{n,n} = _{i=1}^n X_i $ and \\(Z_{n,0} \\equiv G\\).\nWith this notation we have \\[\n\\Er\\left[\\varphi(Z_{n,n}) - \\varphi(Z_{n,0}) \\right] = - \\sum_{i=0}^{n-i} \\Er\\left[ \\varphi(Z_{n,i}) - \\varphi(Z_{n,i+1})\\right]\n\\] and we will show that \\(\\Er\\left[ \\varphi(Z_{n,i}) - \\varphi(Z_{n,i+1})\\right]\\) is small.\nLet \\(S_{n,i} = Z_{n,i} - \\frac{G_{i+1}}{\\sqrt{n}} = Z_{n,i+1} - \\frac{X_{i+1}}/\\sqrt{n}\\) and take a second order Taylor expansion of \\(\\varphi\\) around \\(S_{n,i}\\), giving \\[\n\\varphi(Z_{n,i}) = \\varphi(S_{n,i}) + \\varphi'(S_{n,i}) \\frac{G_{i+1}}{\\sqrt{n}} + \\frac{1}{2}\\varphi''(S_{n,i}) \\frac{G_{i+1}^2}{n} + O\\left( \\frac{|G_{i+1}|^3}{n^{3/2}} \\sup \\varphi'''(x) \\right)\n\\] and \\[\n\\varphi(Z_{n,i+i}) = \\varphi(S_{n,i}) + \\varphi'(S_{n,i}) \\frac{X_{i+1}}{\\sqrt{n}} + \\frac{1}{2}\\varphi''(S_{n,i}) \\frac{X_{i+1}^2}{n} + O\\left( \\frac{|X_{i+1}|^3}{n^{3/2}} \\sup \\varphi'''(x) \\right).\n\\]\nWe have assumed that \\(\\Er[X_i] = \\Er[G_i]\\) and \\(\\Er[X_i^2] = \\Er[G_i^2]\\), so \\[\n\\left\\vert \\Er\\left[ \\varphi(Z_{n,i}) - \\varphi(Z_{n,i+1})\\right] \\right\\vert \\leq C( \\frac{|G_{i+1}|^3 + |X_{i+1}|^3}{n^{3/2}} \\sup \\varphi'''(x) \\right)\n\\] for some constant \\(C\\) that depends on \\(\\varphi\\), but not on \\(n\\) or \\(i\\). We can conclude that \\[\n\\Er\\left[\\varphi(Z_{n,n}) - \\varphi(Z_{n,0}) \\right] \\leq n C( \\frac{|G_{i+1}|^3 + |X_{i+1}|^3}{n^{3/2}} \\sup \\varphi'''(x) \\right).\n\\]\n\nFrom Tao (2010)"
  },
  {
    "objectID": "asymptotics/indistribution.html#berry-esseen-theorem",
    "href": "asymptotics/indistribution.html#berry-esseen-theorem",
    "title": "Convergence in Distribution",
    "section": "Berry-Esseen Theorem",
    "text": "Berry-Esseen Theorem\n\n\n\n\nBerry-Esseen Theorem\n\n\nIf \\(X_i\\) are i.i.d. with \\(\\Er[X] = 0\\) and \\(\\var(X)=1\\), then \\[\n\\sup_{z \\in \\R} \\left\\vert\nP\\left(\\left[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i\\right] \\leq z \\right) - \\Phi(z) \\right\\vert \\leq 0.5 \\Er[|X|^3]/\\sqrt{n}\n\\] where \\(\\Phi\\) is the normal CDF.\n\n\n\n\n\n\n\n\n\nMultivariate Berry-Esseen Theorem\n\n\nIf \\(X_i \\in \\R^d\\) are i.i.d. with \\(\\Er[X] = 0\\) and \\(\\var(X)=I_d\\), then \\[\n\\sup_{A \\subset \\R^d, \\text{convex}} \\left\\vert\nP\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n X_i \\in A \\right) - P(N(0,I_d) \\in A) \\right\\vert \\leq\n(42 d^{1/4} + 16) \\Er[\\Vert X \\Vert ^3]/\\sqrt{n}\n\\]\n1\n\n\n\n\n\nSee Raič (2019) for details."
  },
  {
    "objectID": "asymptotics/indistribution.html#simulated-illustration-of-berry-esseen-clt",
    "href": "asymptotics/indistribution.html#simulated-illustration-of-berry-esseen-clt",
    "title": "Convergence in Distribution",
    "section": "Simulated Illustration of Berry-Esseen CLT",
    "text": "Simulated Illustration of Berry-Esseen CLT\n\n\nplotting code\nusing Plots, Distributions\n\nfunction dgp(n, xhi=2)\n  p = 1/(1+xhi^2)\n  xlo = -p*xhi/(1-p)\n  hi = rand(n) .&lt; p\n  x = ifelse.(hi, xhi, xlo)\nend\n\nfunction Ex3(xhi)\n  p = 1/(1+xhi^2)\n  xlo = -p*xhi/(1-p)\n  p*xhi^3 + (1-p)*-xlo^3\nend\n\nfunction plotcdfwithbounds(dgp,  e3, n=[10,100,1000], S=9999)\n  cmap = palette(:tab10)\n  x = range(-2.5, 2.5, length=200)\n  cdfx=x-&gt;cdf(Normal(), x)\n  fig=Plots.plot(x, cdfx, label=\"Normal\", color=\"black\", linestyle=:dash)\n  for (i,ni) in enumerate(n)\n    truedist = [mean(dgp(ni))*sqrt(ni) for _ in 1:S]\n    ecdf = x-&gt; mean(truedist .&lt;= x)\n    Plots.plot!(x, ecdf, label=\"n=$ni\", color=cmap[i])\n    Plots.plot!(x, cdfx.(x), ribbon = 0.5*e3/√ni, fillalpha=0.2, label=\"\", color=cmap[i])\n  end\n  xlims!(-2.5,2.5)\n  ylims!(0,1)\n  title!(\"Distribution of Scaled Sample Mean\")\n  return(fig)\nend\nxhi = 2.5\nplotcdfwithbounds(n-&gt;dgp(n,xhi), Ex3(xhi))\n\n\n\n\nThis simulation is constructed so that the bound is close to the actual error. For many distributions, the bound is large and slack. It is usually not as informative as it appears in this example."
  },
  {
    "objectID": "asymptotics/indistribution.html#simulated-illustration-of-berry-esseen-clt-slack-bounds",
    "href": "asymptotics/indistribution.html#simulated-illustration-of-berry-esseen-clt-slack-bounds",
    "title": "Convergence in Distribution",
    "section": "Simulated Illustration of Berry-Esseen CLT : Slack Bounds",
    "text": "Simulated Illustration of Berry-Esseen CLT : Slack Bounds"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for ECON 626: Econometric Theory I at UBC in 2022. The course aims to equip students with introductory knowledge in probability, statistics, and econometrics so that students become familiar with basic concepts underlying empirical and methodological research in economics.\nThis site was created using Quarto.\nSource code is available on github with a CC-by-SA license."
  },
  {
    "objectID": "asymptotics/inprobability.html#reading",
    "href": "asymptotics/inprobability.html#reading",
    "title": "Convergence in Probability",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 9\nIndependent study: Song (2021) chapters 6-7"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-in-probability-1",
    "href": "asymptotics/inprobability.html#convergence-in-probability-1",
    "title": "Convergence in Probability",
    "section": "Convergence in Probability",
    "text": "Convergence in Probability\n\n\n\nDefinition\n\n\nRandom vectors \\(X_1, X_2, ...\\) converge in probability to the random vector \\(Y\\) if for all \\(\\epsilon&gt;0\\) \\[\n\\lim_{n \\to \\infty} P\\left( \\norm{X_n - Y} &gt; \\epsilon \\right) = 0\n\\] denoted by \\(X_n \\inprob Y\\) or \\(\\plim_{n \\to \\infty} X_n = Y\\)\n\n\n\n\n\nTypical use: \\(X_n = \\hat{\\theta}_n\\), estimator from \\(n\\) observations, \\(Y=\\theta_0\\), a constant.\nHow to show that \\(\\hat{\\theta}_n \\inprob \\theta_0\\)?"
  },
  {
    "objectID": "asymptotics/inprobability.html#lp-convergence",
    "href": "asymptotics/inprobability.html#lp-convergence",
    "title": "Convergence in Probability",
    "section": "\\(L^p\\) convergence",
    "text": "\\(L^p\\) convergence\n\n\n\nDefinition\n\n\nRandom vectors \\(X_1, X_2, ...\\) converge in \\(L^p\\) to the random vector \\(Y\\) if \\[\n\\lim_{n \\to \\infty} \\Er\\left[ \\norm{X_n-Y}^p \\right] \\to 0\n\\]\n\n\n\n\n\\(p=2\\) called convergence in mean square"
  },
  {
    "objectID": "asymptotics/inprobability.html#markovs-inequality",
    "href": "asymptotics/inprobability.html#markovs-inequality",
    "title": "Convergence in Probability",
    "section": "Markov’s Inequality",
    "text": "Markov’s Inequality\n\n\n\nMarkov’s Inequality\n\n\n\\(P(|X|&gt;\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)\n\n\n\n\n\\(P\\left( \\norm{X_n - Y} &gt; \\epsilon \\right) \\leq \\frac{\\Er[ \\norm{X_n - Y}^k]} {\\epsilon_k}\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-in-lp-implies-convergence-in-probability",
    "href": "asymptotics/inprobability.html#convergence-in-lp-implies-convergence-in-probability",
    "title": "Convergence in Probability",
    "section": "Convergence in \\(L^p\\) implies convergence in probability",
    "text": "Convergence in \\(L^p\\) implies convergence in probability\n\n\n\nTheorem 1.1\n\n\nIf \\(X_n\\) converges in \\(L^p\\) to \\(Y\\), then \\(X_n \\inprob Y\\)."
  },
  {
    "objectID": "asymptotics/inprobability.html#application-to-estimators",
    "href": "asymptotics/inprobability.html#application-to-estimators",
    "title": "Convergence in Probability",
    "section": "Application to Estimators",
    "text": "Application to Estimators\n\nAn estimator, \\(\\hat{\\theta}\\) is consistent if \\(\\hat{\\theta} \\inprob \\theta_0\\)\nImplication for estimators: \\[\n\\begin{aligned}\nMSE(\\hat{\\theta}_n) = & \\Er[ \\norm{\\hat{\\theta}_n - \\theta_0}^2 ] \\\\\n= & tr[\\var(\\hat{\\theta}_n)] + Bias(\\hat{\\theta}_n)'Bias(\\hat{\\theta}_n)\n\\end{aligned}\n\\]\nIf \\(MSE(\\hat{\\theta}_n) \\to 0\\), then \\(\\hat{\\theta}_n \\inprob \\theta_0\\)\nIf \\(\\lim_{n \\to \\infty} \\Er[\\hat{\\theta}_n]  \\neq \\theta_0\\), then \\(\\plim \\hat{\\theta}_n \\neq \\theta_0\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#consistency-of-least-squares",
    "href": "asymptotics/inprobability.html#consistency-of-least-squares",
    "title": "Convergence in Probability",
    "section": "Consistency of Least-Squares",
    "text": "Consistency of Least-Squares\n\nIn \\(y = X \\beta_0 + u\\), when does \\(\\hat{\\beta} = (X'X)^{-1} X' y\\) \\(\\inprob \\beta_0\\)?\nSufficient that \\(MSE(\\hat{\\beta}) = tr[\\var(\\hat{\\beta})] +\nBias(\\hat{\\beta})'Bias(\\hat{\\beta}) \\to 0\\)\n\\(\\var(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}\\) (treating \\(X\\) as non-stochastic)\n\\(tr((X'X)^{-1}) \\leq \\frac{k}{\\lambda_{ min}(X'X)}\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-in-probability-of-functions",
    "href": "asymptotics/inprobability.html#convergence-in-probability-of-functions",
    "title": "Convergence in Probability",
    "section": "Convergence in Probability of Functions",
    "text": "Convergence in Probability of Functions\n\n\n\nTheorem 2.2\n\n\nIf \\(X_n \\inprob X\\), and \\(f\\) is continuous, then \\(f(X_n) \\inprob f(X)\\)\n\n\n\n\n\n\nSlutsky’s Lemma\n\n\nIf \\(Y_n \\inprob c\\) and \\(W_n \\inprob d\\), then\n\n\\(Y_n + W_n \\inprob c+ d\\)\n\\(Y_n W_n \\inprob cd\\)\n\\(Y_n / W_n \\inprob c/d\\) if \\(d \\neq 0\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#weak-law-of-large-numbers",
    "href": "asymptotics/inprobability.html#weak-law-of-large-numbers",
    "title": "Convergence in Probability",
    "section": "Weak Law of Large Numbers",
    "text": "Weak Law of Large Numbers\n\n\n\nWeak Law of Large Numbers\n\n\nIf \\(X_1, ..., X_n\\) are i.i.d. and \\(\\Er[X^2]\\) exists, then \\[\n\\frac{1}{n} \\sum X_i \\inprob \\Er[X]\n\\]\n\n\n\n\nProof: use Markov’s inequality\nThis is the simplest to prove WLLN, but there are many variants with alternate assumptions that also imply \\(\\frac{1}{n} \\sum X_i \\inprob \\Er[X]\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#consistency-of-least-squares-revisited",
    "href": "asymptotics/inprobability.html#consistency-of-least-squares-revisited",
    "title": "Convergence in Probability",
    "section": "Consistency of Least Squares Revisited",
    "text": "Consistency of Least Squares Revisited\n\nIn \\(y = X \\beta_0 + u\\), when does \\(\\hat{\\beta} \\inprob \\beta_0\\)?\nTreat \\(X\\) as stochastic\n\\(\\hat{\\beta} = \\left(\\frac{1}{n} \\sum_{i=1}^n X_i X_i' \\right)^{-1} \\left(\\frac{1}{n} \\sum_{i=1}^n X_i y_i \\right)\\)\nIf WLLN applies to \\(\\frac{1}{n} \\sum_{i=1}^n X_i X_i'\\) and \\(\\frac{1}{n} \\sum_{i=1}^n X_i y_i\\) (and \\(\\Er[X_i X_i']^{-1}\\) exists)\nSufficient for i.i.d, \\(\\Er[X_i u_i] = 0\\), 4th moments of \\(X_i\\) to exist, \\(\\Er[u_i^2]\\) to exist"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-rates-1",
    "href": "asymptotics/inprobability.html#convergence-rates-1",
    "title": "Convergence in Probability",
    "section": "Convergence Rates",
    "text": "Convergence Rates\n\n\n\nDefinition\n\n\nGiven a sequence of random variables, \\(X_1, X_2, ...\\) and constants \\(b_1, b_2, ...\\), then\n\n\\(X_n = O_p(b_n)\\) if for all \\(\\epsilon &gt; 0\\) there exists \\(M_\\epsilon\\) s.t. \\[\n\\lim\\sup P\\left(\\frac{\\norm{X_n}}{b_n} \\geq  M_\\epsilon \\right) &lt; \\epsilon\n\\]\n\\(X_n = o_p(b_n)\\) if \\(\\frac{X_n}{b_n} \\inprob 0\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#example-little-o_p",
    "href": "asymptotics/inprobability.html#example-little-o_p",
    "title": "Convergence in Probability",
    "section": "Example: Little \\(o_p\\)",
    "text": "Example: Little \\(o_p\\)\n\nReal valued \\(X_1, ..., X_n\\) i.i.d., with \\(\\Er[X] = \\mu\\), \\(\\var(X_i) = \\sigma^2\\)\nMarkov’s inequality \\[\nP\\left( |\\overbrace{\\En X_i}^{\\equiv \\frac{1}{n} \\sum_{i=1}^n X_i} - \\mu | &gt; a \\right) \\leq \\frac{\\var(\\En X_i - \\mu)}{a^2} \\leq  \\frac{\\sigma^2}{n a^2}\n\\]\nLet \\(a = \\epsilon n^{-\\alpha}\\), then \\[\nP\\left( \\frac{|\\En X_i - \\mu |}{n^{-\\alpha}} &gt; \\epsilon \\right) \\leq \\frac{\\sigma^2}{n^{1 - 2\\alpha}\\epsilon^2}\n\\]\n\\(|\\En X_i - \\mu | = o_p(n^{-\\alpha})\\) for \\(\\alpha \\in (0, 1/2)\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#example-big-o_p",
    "href": "asymptotics/inprobability.html#example-big-o_p",
    "title": "Convergence in Probability",
    "section": "Example: Big \\(O_p\\)",
    "text": "Example: Big \\(O_p\\)\n\nReal valued \\(X_1, ..., X_n\\) i.i.d., with \\(\\Er[X] = \\mu\\), \\(\\var(X_i) = \\sigma^2\\)\nMarkov’s inequality \\[\nP\\left( |\\overbrace{\\En X_i}^{\\equiv \\frac{1}{n} \\sum_{i=1}^n X_i} - \\mu | &gt; a \\right) \\leq  \\frac{\\var(\\En X_i - \\mu)}{a^2}  \\leq   \\frac{\\sigma^2}{n a^2}\n\\]\nLet \\(a = \\sigma \\epsilon^{-1/2} n^{-1/2}\\), \\[\nP\\left( \\frac{|\\En X_i - \\mu |}{n^{-1/2}} &gt; \\underbrace{\\sigma \\epsilon^{-1/2}}_{M_\\epsilon} \\right) \\leq \\epsilon\n\\] so \\(|\\En X_i - \\mu | = O_p(n^{-\\alpha})\\) for \\(\\alpha \\in (0, 1/2]\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#non-asymptotic-bounds-1",
    "href": "asymptotics/inprobability.html#non-asymptotic-bounds-1",
    "title": "Convergence in Probability",
    "section": "Non-Asymptotic Bounds",
    "text": "Non-Asymptotic Bounds\n\nLet \\(\\Er[X] = \\mu\\)\nMarkov’s inequality: \\(P(|X - \\mu |&gt;\\epsilon) \\leq \\frac{\\Er[|X - \\mu|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)\nIdea: minimize left side over \\(k\\) to get a tighter bound"
  },
  {
    "objectID": "asymptotics/inprobability.html#non-asymptotic-bounds-2",
    "href": "asymptotics/inprobability.html#non-asymptotic-bounds-2",
    "title": "Convergence in Probability",
    "section": "Non-Asymptotic Bounds",
    "text": "Non-Asymptotic Bounds\n\nMarkov’s inequality for \\(e^{\\lambda(X-\\mu)}\\) \\[\nP(X-\\mu&gt;\\epsilon) = P\\left( e^{\\lambda (X - \\mu)} &gt; e^{\\lambda \\epsilon} \\right) \\leq e^{-\\lambda \\epsilon} \\Er\\left[e^{\\lambda (X-\\mu)}\\right]\n\\]\n\\(M_X(\\lambda)=\\Er\\left[e^{\\lambda (X-\\mu)}\\right]\\) is the (centered) moment generating function\n\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(\\Er\\left[e^{\\lambda (X-\\mu)}\\right] = e^{\\frac{\\lambda^2 \\sigma^2}{2}}\\)\nIf \\(|X| \\leq b\\), then \\(\\Er\\left[e^{\\lambda (X-\\mu)}\\right] \\leq e^{\\lambda^2 b^2}\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#non-asymptotic-bounds-3",
    "href": "asymptotics/inprobability.html#non-asymptotic-bounds-3",
    "title": "Convergence in Probability",
    "section": "Non-Asymptotic Bounds",
    "text": "Non-Asymptotic Bounds\n\nSuppose \\(\\Er\\left[e^{\\lambda (X-\\mu)}\\right] \\leq e^{\\frac{\\lambda^2 \\sigma^2}{2}}\\), then \\[\nP(X-\\mu&gt;\\epsilon) \\leq \\inf_{\\lambda \\geq 0} e^{-\\lambda \\epsilon + \\lambda^2 \\sigma^2/2} = e^{-\\frac{\\epsilon^2}{2 \\sigma^2}}\n\\]\nSuppose \\(\\Er\\left[e^{\\lambda (X_i-\\mu)}\\right] \\leq e^{\\frac{\\lambda^2 \\sigma^2}{2}}\\) and \\(X_i\\) are independent, then \\[\nP\\left(\\frac{1}{n} \\sum_{i=1}^n X_i-\\mu &gt;\\epsilon\\right) \\leq e^{-\\frac{\\epsilon^2 n}{2 \\sigma^2}}\n\\]"
  },
  {
    "objectID": "did/did.html#setup",
    "href": "did/did.html#setup",
    "title": "Difference in Diffferences",
    "section": "Setup",
    "text": "Setup\n\nTwo periods, binary treatment in second period\nPotential outcomes \\(\\{y_{it}(0),y_{it}(1)\\}_{t=0}^1\\) for \\(i=1,...,N\\)\nTreatment \\(D_{it} \\in \\{0,1\\}\\),\n\n\\(D_{i0} = 0\\) \\(\\forall i\\)\n\\(D_{i1} = 1\\) for some, \\(0\\) for others\n\nObserve \\(y_{it} = y_{it}(0)(1-D_{it}) + D_{it} y_{it}(1)\\)"
  },
  {
    "objectID": "did/did.html#identification",
    "href": "did/did.html#identification",
    "title": "Difference in Diffferences",
    "section": "Identification",
    "text": "Identification\n\nAverage treatment effect on the treated: \\[\n\\begin{align*}\nATT & = \\Er[y_{i1}(1) - \\color{red}{y_{i1}(0)} | D_{i1} = 1] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] \\\\\n& \\text{ assume } \\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) | D_{i1}=1] =  \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1}(1) - y_{i0}(0) | D_{i1} = 1] - \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0] \\\\\n& = \\Er[y_{i1} - y_{i0} | D_{i1}=1, D_{i0}=0] - \\Er[y_{i1} - y_{i0} | D_{i1}=0, D_{i0}=0]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "did/did.html#important-assumptions",
    "href": "did/did.html#important-assumptions",
    "title": "Difference in Diffferences",
    "section": "Important Assumptions",
    "text": "Important Assumptions\n\nNo anticipation: \\(D_{i1}=1\\) does not affect \\(y_{i0}\\)\n\nbuilt into the potential outcomes notation we used, relaxing would be allowing potential outcomes given sequence of \\(D\\) – \\(y_{it}(D_{i0},D_{i1})\\)\n\nParallel trends: \\(\\Er[\\color{red}{y_{i1}(0)} - y_{i0}(0) |D_{i1}=1,D_{i0}=0] =  \\Er[y_{i1}(0) - y_{i0}(0) | D_{i1}=0], D_{i0}=0]\\)\n\nnot invariant to tranformations of \\(y\\)"
  },
  {
    "objectID": "did/did.html#estimation",
    "href": "did/did.html#estimation",
    "title": "Difference in Diffferences",
    "section": "Estimation",
    "text": "Estimation\n\n\nPlugin: \\[\n\\widehat{ATT} = \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})D_{i1}(1-D_{i0})}{\\sum_{i=1}^n D_{i1}(1-D_{i0})} -  \\frac{ \\sum_{i=1}^n (y_{i1} - y_{i0})(1-D_{i1})(1-D_{i0})}{\\sum_{i=1}^n (1-D_{i1})(1-D_{i0})}\n\\]\nRegression: \\[\ny_{it} = \\delta_t + \\alpha 1\\{D_{i1}=1\\} + \\beta D_{it} + \\epsilon_{it}\n\\] then \\(\\hat{\\beta} = \\widehat{ATT}\\)\nFixed effects: \\[\ny_{it} = \\delta_t + \\alpha_i + \\beta D_{it} + u_{it}\n\\] then \\(\\hat{\\beta} = \\widehat{ATT}\\)"
  },
  {
    "objectID": "did/did.html#identification-1",
    "href": "did/did.html#identification-1",
    "title": "Difference in Diffferences",
    "section": "Identification",
    "text": "Identification\n\nSame logic as before, \\[\n\\begin{align*}\nATT_{t,t-s} & = \\Er[y_{it}(1) - \\color{red}{y_{it}(0)} | D_{it} = 1, D_{it-s}=0] \\\\\n& = \\Er[y_{it}(1) - y_{it-s}(0) | D_{it} = 1, D_{it-s}=0] - \\\\\n& \\;\\; -  \\Er[\\color{red}{y_{it}(0)} - y_{t-s}(0) | D_{it}=1, D_{it-s}=0]\n\\end{align*}\n\\]\n\nassume \\(\\Er[\\color{red}{y_{it}(0)} - y_{it-s}(0) | D_{it}=1,  D_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | D_{it}=0, D_{it-s}=0]\\)\n\n\n\\[\n\\begin{align*}\nATT_{t,t-s}& = \\Er[y_{it} - y_{it-s} | D_{it}=1, D_{it-s}=0] - \\Er[y_{it} - y_{it-s} | D_{it}=0, D_{it-s}=0]\n\\end{align*}\n\\] - Similarly, can identify various other interpretable average treatment effects conditional on being treated at some times and not others"
  },
  {
    "objectID": "did/did.html#estimation-1",
    "href": "did/did.html#estimation-1",
    "title": "Difference in Diffferences",
    "section": "Estimation",
    "text": "Estimation\n\nPlugin\nFixed effects? \\[\ny_{it} = \\beta D_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\n\\] When will \\(\\hat{\\beta}^{FE}\\) consistently estimate some interpretable conditional average of treatment effects?"
  },
  {
    "objectID": "did/did.html#fixed-effects",
    "href": "did/did.html#fixed-effects",
    "title": "Difference in Diffferences",
    "section": "Fixed Effects",
    "text": "Fixed Effects\n\nAs on problem set 6, \\[\n\\begin{align*}\n\\hat{\\beta} = & \\sum_{i=1,t=1}^{n,T} y_{it} \\overbrace{\\frac{\\tilde{D}_{it}}{ \\sum_{i,t} \\tilde{D}_{it}^2 }}^{\\hat{\\omega}_{it}(D_it)} \\\\\n= & \\sum_{i=1,t=1}^{n,T} y_{it}(0) \\hat{\\omega}_{it}(D_it) + \\sum_{i=1,t=1}^{n,T} D_{it} (y_{it}(1) - y_{it}(0)) \\hat{\\omega}_{it}(D_it)\n\\end{align*}\n\\] where \\[\n\\begin{align*}\n\\tilde{D}_{it} & = D_{it} - \\frac{1}{n} \\sum_{j=1}^n (D_{jt} - \\frac{1}{T} \\sum_{s=1}^T D_{js}) - \\frac{1}{T} \\sum_{s=1}^T D_{is} \\\\\n& = D_{it} - \\frac{1}{n} \\sum_{j=1}^n D_{jt} - \\frac{1}{T} \\sum_{s=1}^T D_{is} + \\frac{1}{nT} \\sum_{j,s} D_{js}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "did/did.html#simulation",
    "href": "did/did.html#simulation",
    "title": "Difference in Diffferences",
    "section": "Simulation",
    "text": "Simulation\n\n\\(T\\) periods\nOnce \\(i\\) treated, remains treated"
  },
  {
    "objectID": "did/did.html#weights",
    "href": "did/did.html#weights",
    "title": "Difference in Diffferences",
    "section": "Weights",
    "text": "Weights\n\n\nCode\nusing Statistics\nfunction assigntreat(n,T;portiontreated=vcat(zeros(T ÷ 2), 0.5, zeros(T - (T ÷ 2) - 1)))\n  treated = falses(n,T)\n  for t=2:T\n    treated[:,t] = treated[:,t-1]\n    if (portiontreated[t]&gt;0)\n      treated[:,t] = (treated[:,t] .|| rand(n) .&lt; portiontreated[t])\n    end\n  end\n  return(treated)\nend\n\nfunction weights(D)\n  D̃ = D .- mean(D,dims=1) .- mean(D,dims=2) .+ mean(D)\n  ω = D̃./sum(D̃.^2)\nend\n\nn = 1000\nT = 9\nD = assigntreat(n,T)\ny = randn(n,T)\nsum(y.*weights(D))\n\n\n0.007077771753723077\n\n\n\n\nCode\nn,T = size(D)\nusing DataFrames, FixedEffectModels, RegressionTables\ndf = DataFrame(id = vec((1:n)*ones(Int,T)'), t = vec(ones(Int,n)*(1:T)'), y = vec(y), D=vec(D))\nm=reg(df, @formula(y ~ D + fe(t) + fe(id)))\nregtable(m, render=AsciiTable())\n\n\n\n--------------------------\n                      y   \n--------------------------\nD                    0.007\n                   (0.042)\n--------------------------\nt Fixed Effects        Yes\nid Fixed Effects       Yes\n--------------------------\nN                    9,000\nR2                   0.112\nWithin-R2            0.000\n--------------------------"
  },
  {
    "objectID": "did/did.html#weights-with-single-treatment-time",
    "href": "did/did.html#weights-with-single-treatment-time",
    "title": "Difference in Diffferences",
    "section": "Weights with Single Treatment Time",
    "text": "Weights with Single Treatment Time\n\n\nCode\nusing PlotlyLight\n\nfunction plotp(D; width=900, height=300)\n  n,T=size(D)\n  plt=Plot()\n  plt.layout=Config(xaxis=Config(title=\"time\",tickvals=1:T),\n                    yaxis=Config(title=\"Portion Treated\"),\n                    autosize=false,\n                    width=width,\n                    height=height)\n  plt(x=1:T,y=vec(mean(D,dims=1)))\n  plt()\nend\n\npfig = plotp(D)\npfig"
  },
  {
    "objectID": "did/did.html#weights-with-single-treatment-time-1",
    "href": "did/did.html#weights-with-single-treatment-time-1",
    "title": "Difference in Diffferences",
    "section": "Weights with Single Treatment Time",
    "text": "Weights with Single Treatment Time\n\n\nCode\nfunction plotweights(D; width=900, height=300)\n  n,T = size(D)\n  ω = weights(D)\n  groups = unique(eachrow(D))\n  plt = Plot()\n  plt.layout=Config(xaxis=Config(title=\"time\",tickvals=1:T),\n                    yaxis=Config(title=\"weight\"),\n                    autosize=false,\n                    width=width,\n                    height=height)\n\n  for g in groups\n    i = findfirst(d == g for d in eachrow(D))\n    wt = ω[i,:]\n    plt(x=1:T,y=wt,name=\"Treated $(sum(g)) times\", mode=\"markers\",type=\"scatter\")\n    end\n  fig=plt()\n  return(fig)\nend\nfig = plotweights(D)\nfig"
  },
  {
    "objectID": "did/did.html#weights-with-uniform-treatment-time",
    "href": "did/did.html#weights-with-uniform-treatment-time",
    "title": "Difference in Diffferences",
    "section": "Weights with Uniform Treatment Time",
    "text": "Weights with Uniform Treatment Time\n\n\nCode\nD = assigntreat(n,T,portiontreated=vcat(0,fill(0.5/(T-1),T-1)))\npfig = plotp(D)\npfig"
  },
  {
    "objectID": "did/did.html#weights-with-uniform-treatment-time-1",
    "href": "did/did.html#weights-with-uniform-treatment-time-1",
    "title": "Difference in Diffferences",
    "section": "Weights with Uniform Treatment Time",
    "text": "Weights with Uniform Treatment Time\n\n\nCode\nfig = plotweights(D)\nfig"
  },
  {
    "objectID": "did/did.html#weights-with-early-and-late-treated",
    "href": "did/did.html#weights-with-early-and-late-treated",
    "title": "Difference in Diffferences",
    "section": "Weights with Early and Late Treated",
    "text": "Weights with Early and Late Treated\n\n\nCode\npt = zeros(T)\npt[2] = 1/3\npt[end-1]=1/3\nD = assigntreat(n,T,portiontreated=pt)\npfig = plotp(D)\npfig"
  },
  {
    "objectID": "did/did.html#weights-with-early-and-late-treatde",
    "href": "did/did.html#weights-with-early-and-late-treatde",
    "title": "Difference in Diffferences",
    "section": "Weights with Early and Late Treatde",
    "text": "Weights with Early and Late Treatde\n\n\nCode\nfig = plotweights(D)\nfig"
  },
  {
    "objectID": "did/did.html#sign-reversal-with-fixed-effects",
    "href": "did/did.html#sign-reversal-with-fixed-effects",
    "title": "Difference in Diffferences",
    "section": "Sign Reversal with Fixed Effects",
    "text": "Sign Reversal with Fixed Effects\n\nTrue Treatment Effects\n\n\n\nCode\npt = zeros(T)\npt[2] = 1/3\npt[end-1]=1/3\n\nfunction simulate(n,T,portiontreated, ATT, σ=0.01)\n  D = assigntreat(n,T,portiontreated=portiontreated)\n  y = randn(n,T)*σ\n  for i in axes(y)[1]\n    timetreated=cumsum(D[i,:])\n    y[i,:] .+= (tt&gt;0 ? ATT[tt] : 0.0 for tt in timetreated)\n  end\n  DataFrame(id = vec((1:n)*ones(Int,T)'), t = vec(ones(Int,n)*(1:T)'), y = vec(y), D=vec(D))\nend\n\nATT =  vcat(ones(T-3),10*ones(3))\ndf = simulate(n,T, pt,ATT)\n\nfunction plotGAT(ATT,D; width=900, height=300)\n  n,T = size(D)\n  groups = unique(eachrow(D))\n  plt = Plot()\n  plt.layout=Config(xaxis=Config(title=\"time\",tickvals=1:T),\n                    yaxis=Config(title=\"ATT\"),\n                    autosize=false,\n                    width=width,\n                    height=height)\n\n  for g in groups\n    t = findfirst(g)\n    if (isnothing(t))\n      t=T+1\n    end\n    ate = vcat(zeros(t-1), ATT[1:(T-t+1)])\n    plt(x=1:T,y=ate,name=\"Treated $(sum(g)) times\", mode=\"markers\",type=\"scatter\")\n    end\n  fig=plt()\n  return(fig)\nend\n\nplotGAT(ATT,D)"
  },
  {
    "objectID": "did/did.html#sign-reversal-with-fixed-effects-1",
    "href": "did/did.html#sign-reversal-with-fixed-effects-1",
    "title": "Difference in Diffferences",
    "section": "Sign Reversal with Fixed Effects",
    "text": "Sign Reversal with Fixed Effects\n\nFixed Effects Estimate\n\n\nm=reg(df, @formula(y ~ D + fe(t) + fe(id)))\nregtable(m, render=AsciiTable())\n\n\n----------------------------\n                       y    \n----------------------------\nD                  -0.461***\n                     (0.087)\n----------------------------\nt Fixed Effects          Yes\nid Fixed Effects         Yes\n----------------------------\nN                      9,000\nR2                     0.543\nWithin-R2              0.004\n----------------------------"
  },
  {
    "objectID": "did/did.html#when-to-worry",
    "href": "did/did.html#when-to-worry",
    "title": "Difference in Diffferences",
    "section": "When to worry",
    "text": "When to worry\n\nIf multiple treatment times and treatment heterogeneity\nEven if weights do not have wrong sign, the fixed effects estimate is hard to interpret\nSame logic applies more generally – not just to time\n\nE.g. if have group effects, some treated units in multiple groups, and \\(E[y(1) - y(0) | group]\\) varies"
  },
  {
    "objectID": "did/did.html#plug-in-estimator",
    "href": "did/did.html#plug-in-estimator",
    "title": "Difference in Diffferences",
    "section": "Plug-in Estimator",
    "text": "Plug-in Estimator\n\nFollow identification \\[\n\\begin{align*}\nATT_{t,t-s}& = \\Er[y_{it} - y_{it-s} | D_{it}=1, D_{it-s}=0] - \\Er[y_{it} - y_{it-s} | D_{it}=0, D_{it-s}=0]\n\\end{align*}\n\\] and estimate \\[\n\\begin{align*}\n\\widehat{ATT}_{t,t-s} = & \\frac{\\sum_i y_{it} D_{it}(1-D_{it-s})}{\\sum_i D_{it}(1-D_{it-s})} \\\\\n& - \\frac{\\sum_i y_{it} (1-D_{it})(1-D_{it-s})}{\\sum_i (1-D_{it})(1-D_{it-s})}\n\\end{align*}\n\\] and perhaps some average, e.g. (there are other reasonable weighted averages) \\[\n\\sum_{t=1}^T \\frac{\\sum_i D_{it}}{\\sum_{i,s} D_{i,s}} \\frac{1}{t-1} \\sum_{s=1}^{t-1} \\widehat{ATT}_{t,t-s}\n\\]\n\nInference? Optimal?"
  },
  {
    "objectID": "did/did.html#what-to-do-1",
    "href": "did/did.html#what-to-do-1",
    "title": "Difference in Diffferences",
    "section": "What to Do?",
    "text": "What to Do?\n\nProblem is possible correlation of \\((y_{it}(1) - y_{it}(0))D_{it}\\) with \\(\\tilde{D}_{it}\\)\n\n\\(\\tilde{D}_{it}\\) is function of \\(t\\) and \\((D_{i1}, ..., D_{iT})\\)\nEstimating separate coefficient for each combination of \\(t\\) and \\((D_{i1}, ..., D_{iT})\\) will eliminate correlation / flexibly model treatment effect heterogeneity"
  },
  {
    "objectID": "did/did.html#cohorts",
    "href": "did/did.html#cohorts",
    "title": "Difference in Diffferences",
    "section": "Cohorts",
    "text": "Cohorts\n\nCohorts = unique sequences of \\((D_{i1}, ..., D_{iT})\\)\n\nIn last simulated example, three cohorts\n\n\\((0, 0, 0, 0, 0, 0, 0, 0, 0)\\)\n\\((0, 0, 0, 0, 0, 0, 0, 1, 1)\\)\n\\((0, 1, 1, 1, 1, 1, 1, 1, 1)\\)\n\n\n\n\n\nCode\nusing CategoricalArrays\n\nfunction createCohorts(df)\n  n = length(unique(df.id))\n  T = length(unique(df.t))\n  sorted = sort(df, [:id, :t])\n  D = reshape(sorted.D, T,n)'\n  groups = sort(unique(eachrow(D)))\n  cohorts = [findfirst(d == g for g in groups) for d in eachrow(D)]\n  df=leftjoin(sorted, DataFrame(cohort=categorical(cohorts), id=unique(sorted.id)), on=:id)\n  df.DCt .= \"untreated\"\n  for r in 1:T\n    for c in unique(df.cohort)\n      dct = (df.t .== r) .& (df.cohort .== c) .& df.D\n      if (any(dct))\n        df.DCt[dct] .= \"c$(c),t$(r)\"\n        df[!,\"Dc$(c)t$(r)\"] .= false\n        df[!,\"Dc$(c)t$(r)\"][dct] .= true\n      end\n    end\n  end\n  df.ct = categorical(df.t)\n  df\nend\n\ndfc = createCohorts(df);"
  },
  {
    "objectID": "did/did.html#regression-with-cohort-interactions",
    "href": "did/did.html#regression-with-cohort-interactions",
    "title": "Difference in Diffferences",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\n\nCode\nm=reg(dfc, @formula(y ~ DCt + fe(id) + fe(t)))\nregtable(m, render=AsciiTable())\n\n\n\n----------------------------\n                       y    \n----------------------------\nDCt: c2,t9             0.002\n                     (0.001)\nDCt: c3,t2            -0.000\n                     (0.001)\nDCt: c3,t3             0.001\n                     (0.001)\nDCt: c3,t4            -0.000\n                     (0.001)\nDCt: c3,t5            -0.000\n                     (0.001)\nDCt: c3,t6            -0.001\n                     (0.001)\nDCt: c3,t7            -0.000\n                     (0.001)\nDCt: c3,t8          9.000***\n                     (0.001)\nDCt: c3,t9          9.001***\n                     (0.001)\nDCt: untreated     -0.999***\n                     (0.001)\n----------------------------\nid Fixed Effects         Yes\nt Fixed Effects          Yes\n----------------------------\nN                      9,000\nR2                     1.000\nWithin-R2              1.000\n----------------------------"
  },
  {
    "objectID": "did/did.html#regression-with-cohort-interactions-1",
    "href": "did/did.html#regression-with-cohort-interactions-1",
    "title": "Difference in Diffferences",
    "section": "Regression with Cohort Interactions",
    "text": "Regression with Cohort Interactions\n\n\nCode\nm=reg(dfc, @formula(y ~ -1 + cohort*ct + fe(id)), save=:fe)\nregtable(m, render=AsciiTable())\n\n\n[ Info: RHS-variable cohort: 2 is collinear with the fixed effects.\n[ Info: RHS-variable cohort: 3 is collinear with the fixed effects.\n\n\n\n-----------------------------\n                        y    \n-----------------------------\ncohort: 2               0.000\n                        (NaN)\ncohort: 3               0.000\n                        (NaN)\nct: 2                  0.002*\n                      (0.001)\nct: 3                  -0.000\n                      (0.001)\nct: 4                   0.000\n                      (0.001)\nct: 5                   0.001\n                      (0.001)\nct: 6                   0.001\n                      (0.001)\nct: 7                   0.000\n                      (0.001)\nct: 8                   0.001\n                      (0.001)\nct: 9                  -0.000\n                      (0.001)\ncohort: 2 & ct: 2      -0.001\n                      (0.001)\ncohort: 3 & ct: 2    0.999***\n                      (0.001)\ncohort: 2 & ct: 3       0.001\n                      (0.001)\ncohort: 3 & ct: 3    1.000***\n                      (0.001)\ncohort: 2 & ct: 4       0.001\n                      (0.001)\ncohort: 3 & ct: 4    0.999***\n                      (0.001)\ncohort: 2 & ct: 5      -0.000\n                      (0.001)\ncohort: 3 & ct: 5    0.999***\n                      (0.001)\ncohort: 2 & ct: 6       0.000\n                      (0.001)\ncohort: 3 & ct: 6    0.998***\n                      (0.001)\ncohort: 2 & ct: 7      -0.000\n                      (0.001)\ncohort: 3 & ct: 7    0.999***\n                      (0.001)\ncohort: 2 & ct: 8    0.999***\n                      (0.001)\ncohort: 3 & ct: 8    9.999***\n                      (0.001)\ncohort: 2 & ct: 9    1.001***\n                      (0.001)\ncohort: 3 & ct: 9   10.000***\n                      (0.001)\n-----------------------------\nid Fixed Effects          Yes\n-----------------------------\nN                       9,000\nR2                      1.000\nWithin-R2               1.000\n-----------------------------"
  },
  {
    "objectID": "did/did.html#what-to-do-2",
    "href": "did/did.html#what-to-do-2",
    "title": "Difference in Diffferences",
    "section": "What to Do?",
    "text": "What to Do?\n\nUnderstand existing methods: read reviews Clément de Chaisemartin and D’Haultfœuille (2022), Roth et al. (2023), C. de Chaisemartin and D’Haultfœuille (2023)\nUse an appropriate package:\n\npartial list on https://asjadnaqvi.github.io/DiD/"
  },
  {
    "objectID": "did/did.html#pre-trends-1",
    "href": "did/did.html#pre-trends-1",
    "title": "Difference in Diffferences",
    "section": "Pre-trends",
    "text": "Pre-trends\n\nParallel trends assumption\n\n\\[\n\\Er[\\color{red}{y_{it}(0)} - y_{it-s}(0) | D_{it}=1,  D_{it-s}=0] = \\Er[y_{it}(0) - y_{it-s}(0) | D_{it}=0, D_{it-s}=0]\n\\]\n\nMore plausible if there are parallel pre-trends\n\n\\[\n\\begin{align*}\n& \\Er[y_{it-r}(0) - y_{it-s}(0) | D_{it}=1, D_{it-r}=0,  D_{it-s}=0] = \\\\\n& = \\Er[y_{it-r}(0) - y_{it-s}(0) | D_{it}=0, D_{it-r}=0, D_{it-s}=0]\n\\end{align*}\n\\]\n\nAlways at least plot pre-trends"
  },
  {
    "objectID": "did/did.html#testing-for-pre-trends",
    "href": "did/did.html#testing-for-pre-trends",
    "title": "Difference in Diffferences",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nIs it a good idea to test\n\n\\[\n\\begin{align*}\nH_0 : & \\Er[y_{it-r} - y_{it-s} | D_{it}=1, D_{it-r}=0,  D_{it-s}=0] = \\\\\n& = \\Er[y_{it-r} - y_{it-s} | D_{it}=0, D_{it-r}=0, D_{it-s}=0]?\n\\end{align*}\n\\] - Even if not testing formally, we do it informally by plotting"
  },
  {
    "objectID": "did/did.html#testing-for-pre-trends-1",
    "href": "did/did.html#testing-for-pre-trends-1",
    "title": "Difference in Diffferences",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nAssume: \\((y_{i1}, ..., y_{iT}, D_{i1},..., D_{iT})\\) i.i.d. across \\(i\\) with finite second moments\nLet \\[\n\\tau^{1t}_{r,s} = \\Er[y_{ir}|D_{it}=1, D_{ir}=0, D_{is}=0]\n\\] \\[\n\\tau^{0t}_{r,s} = \\Er[y_{ir}|D_{it}=0, D_{ir}=0, D_{is}=0]\n\\]\nPlugin estimators \\[\n\\hat{\\tau}^{1t}_{r,s} = \\frac{\\sum_i y_{ir} D_{it}(1-D_{ir})(1-D_{is})} {\\sum_i D_{it}(1-D_{ir})(1-D_{is})}\n\\] \\[\n\\hat{\\tau}^{0t}_{r,s} = \\frac{\\sum_i y_{ir} (1-D_{it})(1-D_{ir})(1-D_{is})} {\\sum_i (1-D_{it})(1-D_{ir})(1-D_{is})}\n\\]"
  },
  {
    "objectID": "did/did.html#testing-for-pre-trends-2",
    "href": "did/did.html#testing-for-pre-trends-2",
    "title": "Difference in Diffferences",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nNote: \\[\n\\begin{align*}\n\\hat{\\tau}^{1t}_{r,s} - \\tau^{1t}_{r,s} = & \\frac{\\frac{1}{n}\\sum_i y_{ir} D_{it}(1-D_{ir})(1-D_{is}) - \\Er[y_{ir} D_{it}(1-D_{ir})(1-D_{is})]}{\\Er[D_{it}(1-D_{ir})(1-D_{is})]} + \\\\\n& + \\frac{\\Er[y_{it} D_{it}(1-D_{ir})(1-D_{is})]}{\\Er[D_{it}(1-D_{ir})(1-D_{is})]^2} \\left(\\frac{1}{n} \\sum_i D_{it}(1-D_{ir})(1-D_{is}) - \\Er[D_{it}(1-D_{ir})(1-D_{is})] \\right)\n\\end{align*}\n\\] and similar expression for \\(\\hat{\\tau}^{0t}_{r,s}\\)\nUse CLT to get joint asymptotic distribution of pre-trends and \\(ATT_{t,t-s}\\)"
  },
  {
    "objectID": "did/did.html#testing-for-pre-trends-3",
    "href": "did/did.html#testing-for-pre-trends-3",
    "title": "Difference in Diffferences",
    "section": "Testing for Pre-trends",
    "text": "Testing for Pre-trends\n\nE.g. \\(T=3\\), \\(D_{i3}=1\\) for some \\(i\\), other \\(D_{it}=0\\) \\[\n\\begin{align*}\n\\sqrt{n} \\left[\n\\begin{pmatrix} \\hat{\\tau}^{13}_{2,1} - \\hat{\\tau}^{03}_{2,1} \\\\\n\\underbrace{\\hat{\\tau}^{13}_{3,2} - \\hat{\\tau}^{03}_{3,2}}_{\\widehat{ATT}_{3,2}}\n\\end{pmatrix} -\n\\begin{pmatrix}\n\\tau^{13}_{2,1} - \\tau^{03}_{2,1} \\\\\nATT_{3,2}\n\\end{pmatrix}\n\\right] \\indist N(0, \\Sigma)\n\\end{align*}\n\\]\nDistribution of \\(\\widehat{ATT}_{3,2}\\) conditional on fail to reject \\(\\tau^{13}_{2,1} - \\tau^{03}_{2,1} = 0\\) is a truncated normal\nRoth (2022) : test can have low power, and in plausible violations, \\(\\widehat{ATT}_{3,2}\\) conditional on failing to reject is biased"
  },
  {
    "objectID": "did/did.html#bounds-from-pre-trends",
    "href": "did/did.html#bounds-from-pre-trends",
    "title": "Difference in Diffferences",
    "section": "Bounds from Pre-trends",
    "text": "Bounds from Pre-trends\n\nLet \\(\\Delta\\) be violation of parallel trends \\[\n\\Delta = \\Er[\\color{red}{y_{it}(0)} - y_{it-1}(0) | D_{it}=1,  D_{it-1}=0] - \\Er[y_{it}(0) - y_{it-1}(0) | D_{it}=0, D_{it-1}=0]\n\\]\nAssume \\(\\Delta\\) is bounded by deviation from parallel of pre-trends \\[\n|\\Delta| \\leq M \\max_{r} \\left\\vert \\tau^{1t}_{t-r,t-r-1} - \\tau^{0t}_{t-r,t-r-1} \\right\\vert\n\\] for some chosen \\(M\\)\nSee Rambachan and Roth (2023)"
  },
  {
    "objectID": "did/did.html#reading",
    "href": "did/did.html#reading",
    "title": "Difference in Diffferences",
    "section": "Reading",
    "text": "Reading\n\nBook: C. de Chaisemartin and D’Haultfœuille (2023)\nRecent reviews: Roth et al. (2023), Clément de Chaisemartin and D’Haultfœuille (2022), Arkhangelsky and Imbens (2023)\nEarly work pointing to problems with fixed effects:\n\nLaporte and Windmeijer (2005), Wooldridge (2005)\n\nExplosion of papers written just before 2020, published just after:\n\nBorusyak and Jaravel (2018)\nClément de Chaisemartin and D’Haultfœuille (2020)\nCallaway and Sant’Anna (2021)\nGoodman-Bacon (2021)\nSun and Abraham (2021)"
  },
  {
    "objectID": "did/did.html#references",
    "href": "did/did.html#references",
    "title": "Difference in Diffferences",
    "section": "References",
    "text": "References\n\n\n\n\nArkhangelsky, Dmitry, and Guido Imbens. 2023. “Causal Models for Longitudinal and Panel Data: A Survey.”\n\n\nBorusyak, Kirill, and Xavier Jaravel. 2018. “Revisiting Event Study Designs.” https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics 225 (2): 200–230. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nChaisemartin, C de, and X D’Haultfœuille. 2023. Credible Answers to Hard Questions: Differences-in-Differences for Natural Experiments. https://dx.doi.org/10.2139/ssrn.4487202.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\n———. 2022. “Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nLaporte, Audrey, and Frank Windmeijer. 2005. “Estimation of Panel Data Models with Binary Indicators When Treatment Effects Are Not Constant over Time.” Economics Letters 88 (3): 389–96. https://doi.org/https://doi.org/10.1016/j.econlet.2005.04.002.\n\n\nRambachan, Ashesh, and Jonathan Roth. 2023. “A More Credible Approach to Parallel Trends.” The Review of Economic Studies 90 (5): 2555–91. https://doi.org/10.1093/restud/rdad018.\n\n\nRoth, Jonathan. 2022. “Pretest with Caution: Event-Study Estimates After Testing for Parallel Trends.” American Economic Review: Insights 4 (3): 305–22. https://doi.org/10.1257/aeri.20210236.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/https://doi.org/10.1016/j.jeconom.2023.03.008.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics 225 (2): 175–99. https://doi.org/https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2005. “Fixed-Effects and Related Estimators for Correlated Random-Coefficient and Treatment-Effect Panel Data Models.” The Review of Economics and Statistics 87 (2): 385–90. https://doi.org/10.1162/0034653053970320."
  },
  {
    "objectID": "gmm/gmm.html#reading",
    "href": "gmm/gmm.html#reading",
    "title": "Generalized Method of Moments",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 13\nSuggested:\n\nOriginated in Hansen (1982), building on method of moments which has a longer history\nImportant early application Hansen and Singleton (1982)\nReview from Hansen (2010) ungated version\n\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\def\\arg{{\\mathrm{arg}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#moment-conditions",
    "href": "gmm/gmm.html#moment-conditions",
    "title": "Generalized Method of Moments",
    "section": "Moment Conditions",
    "text": "Moment Conditions\n\\[\n\\Er\\left[g(Z_i,\\theta_0) \\right] = 0\n\\]\n\nParameter \\(\\theta_0 \\in \\R^d\\)\nData \\(\\tilde{Z}_i \\in \\R^m\\)\nMoment function \\(g: \\R^m \\times \\R^d \\to \\R^k\\)\nIdentification: \\(\\Er\\left[g(Z_i,\\theta) \\right] = 0\\) iff \\(\\theta=\\theta_0\\)"
  },
  {
    "objectID": "gmm/gmm.html#example-iv",
    "href": "gmm/gmm.html#example-iv",
    "title": "Generalized Method of Moments",
    "section": "Example: IV",
    "text": "Example: IV\n\\[\nY_i = X_i' \\beta_0 + u_i\n\\] \\[\n\\Er\\left[Z_i(Y_i - X_i'\\beta_0) \\right] = 0\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-iv-for-nonlinear-regression",
    "href": "gmm/gmm.html#example-iv-for-nonlinear-regression",
    "title": "Generalized Method of Moments",
    "section": "Example: IV for Nonlinear Regression",
    "text": "Example: IV for Nonlinear Regression\n\\[\n\\Er\\left[Z_i(Y_i - h(X_i,\\beta_0)) \\right] = 0\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-binary-choice",
    "href": "gmm/gmm.html#example-binary-choice",
    "title": "Generalized Method of Moments",
    "section": "Example: Binary Choice",
    "text": "Example: Binary Choice\n\n\\(D_i = 1\\{X_i'\\beta_0 &gt; u_i\\}\\)\n\\(u_i \\sim N(0,1)\\) \\[\n\\Er\\left[ \\left(D_i - \\Phi(X_i'\\beta_0) \\right) h(X_i) \\right] = 0\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-consumption-and-assets",
    "href": "gmm/gmm.html#example-consumption-and-assets",
    "title": "Generalized Method of Moments",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nHansen and Singleton (1982)\nModel \\[\n\\begin{align*}\n\\max_{c_t, q_t} & \\Er\\left[ \\sum_{t=0}^\\infty \\beta^t u(c_t) | \\mathcal{I}_0 \\right] \\\\\n\\text{s.t. } & \\;\\; p_t q_t + c_t \\leq (p_t + d_t)q_{t-1} + y_t\n\\end{align*}\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-consumption-and-assets-1",
    "href": "gmm/gmm.html#example-consumption-and-assets-1",
    "title": "Generalized Method of Moments",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nCleverly rearrange first order conditions: \\[\n\\Er\\left[\\beta \\frac{u'(c_{t+1})}{u'(c_t)} \\underbrace{\\frac{p_{t+1} + d_{t+1}}{p_t}}_{R_t} | \\mathcal{I}_s \\right] = 1 \\text{ for } s \\leq t\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-consumption-and-assets-2",
    "href": "gmm/gmm.html#example-consumption-and-assets-2",
    "title": "Generalized Method of Moments",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nAssume \\(u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}\\) \\[\n\\Er\\left[\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t | \\mathcal{I}_s \\right] = 1 \\text{ for } s \\leq t\n\\]\nModel implies \\[\n\\Er\\left[\\left(\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t -1 \\right)Z_t \\right] = 0\n\\] for any \\(Z_t \\in \\mathcal{I}_t\\)"
  },
  {
    "objectID": "gmm/gmm.html#identification",
    "href": "gmm/gmm.html#identification",
    "title": "Generalized Method of Moments",
    "section": "Identification",
    "text": "Identification\n\nAssume:\n\n\\(\\exists \\theta_0 \\in \\Theta\\) s.t. \\(\\forall \\epsilon&gt;0\\), \\[\n\\inf_{\\theta: \\norm{\\theta-\\theta_0} &gt; \\epsilon} \\norm{\\Er[g(Z_i,\\theta)]} &gt; \\norm{\\Er[g(Z_i,\\theta_0)]}\n\\]\n\nSlightly easier assumption to verify is (loosely) \\(g(z,\\theta)\\) continuous and \\(\\theta_0\\) is unique minimizer of \\(\\norm{\\Er[g(Z_i,\\theta)]}\\), see lemma 1 in Song (2021)"
  },
  {
    "objectID": "gmm/gmm.html#estimation",
    "href": "gmm/gmm.html#estimation",
    "title": "Generalized Method of Moments",
    "section": "Estimation",
    "text": "Estimation\n\nPopulation objective function \\[\nQ^{GMM}(\\theta) = \\frac{1}{2} \\norm{\\Er[g(Z_i,\\theta)]}^2_s = \\frac{1}{2} \\Er[g(Z_i,\\theta)]'S'S\\Er[g(Z_i,\\theta)]\n\\]\nSample objective function \\[\n\\hat{Q}^{GMM}(\\theta) = \\frac{1}{2} \\En[g(Z_i,\\theta)]'S_n'S_n\\En[g(Z_i,\\theta)]\n\\]\nEstimator \\[\n\\hat{\\theta} = \\arg\\min_{\\theta \\in \\Theta}\\hat{Q}^{GMM}(\\theta)\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#consistency",
    "href": "gmm/gmm.html#consistency",
    "title": "Generalized Method of Moments",
    "section": "Consistency",
    "text": "Consistency\nSuppose:\n\n\\(\\exists \\theta_0 \\in \\Theta\\) s.t. \\(\\forall \\epsilon&gt;0\\), \\(\\inf_{\\theta: \\norm{\\theta-\\theta_0} &gt; \\epsilon} \\norm{\\Er[g(Z_i,\\theta)]} &gt; \\norm{\\Er[g(Z_i,\\theta_0)]}\\)\n\\(\\sup_{\\theta \\in \\Theta} \\norm{\\En[g(Z_i,\\theta)] - \\Er[g(Z_i,\\theta)]} \\inprob 0\\)\n\\(S_n \\inprob S\\)\n\nThen \\(\\hat{\\theta} \\inprob \\theta_0\\)"
  },
  {
    "objectID": "gmm/gmm.html#asymptotic-normality",
    "href": "gmm/gmm.html#asymptotic-normality",
    "title": "Generalized Method of Moments",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\nSuppose:\n\n\\(\\theta_0 \\in int(\\Theta)\\), & \\(g(z,\\theta)\\) is twice continuously differentiable\n\\(\\sqrt{n} \\frac{\\partial}{\\partial \\theta} \\hat{Q}^{GMM}(\\theta_0)\n\\indist N(0,\\Omega)\\)\n\\(\\sup_{\\theta \\in int(\\Theta)} \\norm{\\frac{\\partial^2}{\\partial\n\\theta \\partial \\theta'} \\hat{Q}^{GMM}(\\theta) - B(\\theta)}\n\\inprob 0\\) with \\(B(\\cdot)\\) continuous at \\(\\theta_0\\) and \\(B(\\theta_0) &gt; 0\\)\n\nThen, \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0, B_0^{-1} \\Omega_0 B_0^{-1})\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#optimal-weighting-matrix",
    "href": "gmm/gmm.html#optimal-weighting-matrix",
    "title": "Generalized Method of Moments",
    "section": "Optimal Weighting Matrix",
    "text": "Optimal Weighting Matrix\nLet \\[\n\\begin{align*}\nM & = (\\Gamma'C\\Gamma)^{-1} \\Gamma' C \\Sigma C \\Gamma (\\Gamma'C\\Gamma)^{-1} \\\\\nM^* & = (\\Gamma' \\Sigma^{-1} \\Gamma)^{-1}\n\\end{align*}\n\\] then \\(M \\geq M^*\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 626: Econometric Theory I",
    "section": "",
    "text": "Part I: Basics\n\nMeasure slides\nProbability slides\n\nRandom Variables, and Distributions\nConditional Expectations and Conditional Distributions\n\nFamily of Distributions : we will not cover in class, read and refer to Song (2021) Chapter 3 as needed\nBasics of Inference\n\nIdentification\nEstimation\nHypothesis Testing\n\n\n\n\nPart II: Generalized Linear Model\n\nPreliminaries of Projection Geometry\nGeneralized Linear Models and Gauss-Markov Theorem\nTests of Linear Hypothesis\n\nMidterm Review\n\n\nPart III: Tools of Asymptotic Theory\n\nConvergence in probability\nConvergence in distribution\nDelta Methods\nAsymptotics of Least Squares\n\n\n\nPart IV\n\nDifference in differences\n\n\n\nPart V: Linear Models with Endogeneity\n\nEndogeneity\nIdentification through IV and Inference\nPreview of GMM\n\n\n\n\n\n\nReferences\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”"
  },
  {
    "objectID": "iv/iv.html#reading",
    "href": "iv/iv.html#reading",
    "title": "Instrumental Variables Estimation",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 12\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\def\\indep{{\\perp\\!\\!\\!\\perp}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "iv/iv.html#model",
    "href": "iv/iv.html#model",
    "title": "Instrumental Variables Estimation",
    "section": "Model",
    "text": "Model\n\\[\nY_i = \\underbrace{X_i}_{\\in \\R^k}' \\beta_0 + u_i\n\\]\n\n\\(\\Er[u_i] = 0\\), but \\(\\Er[X_i u_i] \\neq 0\\)\nInstrument \\(Z_i \\in \\R^d\\) s.t.\n\nRelevant \\(rank(\\Er[Z_i X_i']) = k\\)\nExogenous \\(\\Er[Z_i u_i] = 0\\)"
  },
  {
    "objectID": "iv/iv.html#identification",
    "href": "iv/iv.html#identification",
    "title": "Instrumental Variables Estimation",
    "section": "Identification",
    "text": "Identification\n\nExogeneity implies \\[\n\\Er[Z_i Y_i] = \\Er[Z_i X_i']\\beta_0\n\\]\n\n\n\nIf \\(d=k\\) (exactly identified), then relevance implies \\(\\Er[Z_i X_i']\\) invertible, so \\[\n\\beta_0 = \\Er[Z_i X_i']^{-1} \\Er[Z_i Y_i]\n\\]\n\n\n\n\nFor \\(d&gt;k\\), relevance implies \\(\\Er[Z_iX_i']'\\Er[Z_iX_i']\\) invertible, so \\[\n\\beta_0 = (\\Er[Z_i X_i]' \\Er[Z_i X_i'])^{-1} \\Er[Z_i X_i']' \\Er[Z_i Y_i]\n\\]"
  },
  {
    "objectID": "iv/iv.html#method-of-moments-estimation",
    "href": "iv/iv.html#method-of-moments-estimation",
    "title": "Instrumental Variables Estimation",
    "section": "Method of Moments Estimation",
    "text": "Method of Moments Estimation\n\nWe assume \\(\\Er[Z_i u_i] = 0\\), so \\[\n\\Er[Z_i(Y_i - X_i'\\beta_0)] = 0\n\\]\nEstimate by replacing \\(\\Er\\) with \\(\\frac{1}{n}\\sum_{i=1}^n\\)"
  },
  {
    "objectID": "iv/iv.html#method-of-moments-estimation-1",
    "href": "iv/iv.html#method-of-moments-estimation-1",
    "title": "Instrumental Variables Estimation",
    "section": "Method of Moments Estimation",
    "text": "Method of Moments Estimation\n\n\\(d\\) equations, \\(k \\geq d\\) unknowns, so find \\[\n\\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\hat{\\beta}^{IV}) \\approx 0\n\\] by solving \\[\n\\begin{align*}\n\\hat{\\beta}^{IV} & = \\mathrm{arg}\\min_\\beta \\norm{ \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta) }_{W}^2 \\\\\n& = \\mathrm{arg}\\min_\\beta \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)' W \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "iv/iv.html#method-of-moments-estimation-2",
    "href": "iv/iv.html#method-of-moments-estimation-2",
    "title": "Instrumental Variables Estimation",
    "section": "Method of Moments Estimation",
    "text": "Method of Moments Estimation\n\\[\n\\hat{\\beta}^{IV}\n= \\mathrm{arg}\\min_\\beta \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)' W \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)\n\\]\n\n\n\\(\\hat{\\beta}^{IV}_W = (X'Z W Z'W)^{-1}(X'Z W Z'y)\\)"
  },
  {
    "objectID": "iv/iv.html#consistency",
    "href": "iv/iv.html#consistency",
    "title": "Instrumental Variables Estimation",
    "section": "Consistency",
    "text": "Consistency\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV}_W - \\beta_0 = & (X'Z W Z'W)^{-1}(X'Z W Z'u) \\\\\n= & \\left[ \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\right) \\right]^{-1}\n    \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i u_i\\right)\n\\end{align*}\n\\]\n\nConsistent if LLN applies to \\(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\) and \\(\\frac{1}{n}\\sum_{i=1}^n Z_i u_i\\)\n\nE.g. if i.i.d. with \\(\\Er[\\norm{X_i}^4]\\) and \\(\\Er[\\norm{Z_i}^4]\\) finite and \\(\\Er[u_i^2|Z_i=z] = \\sigma^2\\) 1\n\n\nthese are stronger than needed, for LLN, but needed for CLT"
  },
  {
    "objectID": "iv/iv.html#asymptotic-normality",
    "href": "iv/iv.html#asymptotic-normality",
    "title": "Instrumental Variables Estimation",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV}_W - \\beta_0 = & (X'Z W Z'W)^{-1}(X'Z W Z'u) \\\\\n= & \\left[ \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\right) \\right]^{-1}\n    \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i u_i\\right)\n\\end{align*}\n\\]\n\n\\(\\sqrt{n}(\\hat{\\beta}^{IV} - \\beta_0) \\indist N(0, V)\\) if LLN applies to \\(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\) and CLT to \\(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n Z_i u_i\\)\n\nE.g. if i.i.d. with \\(\\Er[\\norm{X_i}^4]\\) and \\(\\Er[\\norm{Z_i}^4]\\) finite and \\(\\Er[u_i^2|Z_i=z] = \\sigma^2\\)\nthen \\(\\frac{1}{\\sqrt{n}} \\sum Z_i u_i \\indist N(0, \\sigma^2 \\Er[Z_iZ_i'])\\)\n\\(V = \\sigma^2 (\\Er[Z_iX_i']' W \\Er[Z_iX_i'])^{-1} (\\Er[Z_iX_i']' W \\Er[Z_i Z_i'] W \\Er[Z_i X_i']) (\\Er[Z_iX_i']' W \\Er[Z_iX_i'])^{-1}\\)"
  },
  {
    "objectID": "iv/iv.html#optimal-w",
    "href": "iv/iv.html#optimal-w",
    "title": "Instrumental Variables Estimation",
    "section": "Optimal \\(W\\)",
    "text": "Optimal \\(W\\)\n\n\n\nTheorem 2.1\n\n\n\\(W^* = \\Er[Z_iZ_i']^{-1}\\) minimizes the asymptotic variance of \\(\\hat{\\beta}^{IV}_W\\)\n\n\n\n\nEstimate \\(\\hat{W}^* = \\left(\\frac{1}{n} Z'Z\\right)^{-1}\\) \\[\n\\hat{\\beta}^{IV}  = (X'Z (Z'Z)^{-1} Z' X)^{-1} (X'Z(Z'Z)^{-1}Z'y)\n\\]"
  },
  {
    "objectID": "iv/iv.html#two-stage-least-squares",
    "href": "iv/iv.html#two-stage-least-squares",
    "title": "Instrumental Variables Estimation",
    "section": "Two Stage Least Squares",
    "text": "Two Stage Least Squares\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV} & = (X'Z (Z'Z)^{-1} Z' X)^{-1} (X'Z(Z'Z)^{-1}Z'y) \\\\\n& = (X'P_Z X)^{-1} (X' P_Z y) \\\\\n& = ((P_Z X)'(P_Z X))^{-1} ((P_Z X)'y)\n\\end{align*}\n\\]\n\nRegress \\(X\\) on \\(Z\\), let \\(\\hat{X} = P_Z X\\)\nRegress \\(y\\) on \\(\\hat{X}\\)"
  },
  {
    "objectID": "iv/iv.html#testing-overidentifying-restrictions",
    "href": "iv/iv.html#testing-overidentifying-restrictions",
    "title": "Instrumental Variables Estimation",
    "section": "Testing Overidentifying Restrictions",
    "text": "Testing Overidentifying Restrictions\n\n\\(H_0: \\Er[Z_i(Y_i - X_i'\\beta_0)] = 0\\)\n\\(k=d\\), have \\(\\En[Z_i(Y_i - X_i'\\hat{\\beta}^{IV})] = 0\\) exactly, and \\(H_0\\) is untestable\n\\(k&gt;d\\), can test\nTest statistic \\[\nJ = n \\left(\\frac{1}{n} Z'(y-X\\hat{\\beta}^{IV}) \\right)' \\hat{C} \\left(\\frac{1}{n} Z'(y-X\\hat{\\beta}^{IV}) \\right)\n\\]"
  },
  {
    "objectID": "iv/iv.html#testing-overidentifying-restrictions-1",
    "href": "iv/iv.html#testing-overidentifying-restrictions-1",
    "title": "Instrumental Variables Estimation",
    "section": "Testing Overidentifying Restrictions",
    "text": "Testing Overidentifying Restrictions\n\n\n\nTheorem 2.3\n\n\nLet \\(\\hat{C} = \\left(\\frac{1}{n} \\sum_{i=1}^n Z_iZ_i' \\hat{u}_i^2\\right)^{-1}\\). Assume:\n\n\\(\\Er[ \\norm{X_i}^4]  + \\Er[\\norm{Z_i}^4] &lt; \\infty\\)\n\\(\\Er[u|Z] = \\sigma^2\\)\n\\(\\Er[Z_i Z_i']\\) is positive definite\n\nThen, \\[\nJ \\indist \\chi^2_{d-k}\n\\]"
  },
  {
    "objectID": "iv/iv.html#over-identifying-test",
    "href": "iv/iv.html#over-identifying-test",
    "title": "Instrumental Variables Estimation",
    "section": "Over-identifying Test",
    "text": "Over-identifying Test\n\nOnly has power when instruments have different covariances with \\(u\\)\n\n\n\nCode\nusing Distributions, LinearAlgebra\nimport PlotlyLight\nfunction sim(n; d=3, EZu = zeros(d), Exu = 0.5, beta = 1, gamma = ones(d))\n  zu = randn(n,d)\n  Z = randn(n,d) + mapslices(x-&gt;x.*EZu, zu, dims=2)\n  xu = randn(n)\n  X = Z*gamma + xu*Exu\n  u = vec(sum(zu,dims=2) + xu + randn(n))\n  y = X*beta + u\n  return(y,X,Z)\nend\n\nbiv(y,X,Z) = (X'*Z*inv(Z'*Z)*Z'*X) \\ (X'*Z*inv(Z'*Z)*Z'*y)\n\nfunction J(y,X,Z)\n  n = length(y)\n  bhat = biv(y,X,Z)\n  uhat = y - X*bhat\n  C = inv(1/n*sum(z*z'*u^2 for (z,u) in zip(eachrow(Z),uhat)))\n  Zu = Z'*uhat/n\n  J = n*Zu'*C*Zu\nend\n\nS = 1_000\nn = 100\nj0s = [J(sim(n)...) for _ in 1:S]\nj1s = [J(sim(n,EZu=[0.,0., 3.])...) for _ in 1:S]\nj2s = [J(sim(n,EZu=[1.,1., 1.])...) for _ in 1:S]\n\nplt = PlotlyLight.Plot()\nplt(x=j0s, type=\"histogram\", name=\"E[Zu] = 0\")\nplt(x=j1s, type=\"histogram\", name=\"E[Zu] = [0,0,3]\")\nfig=plt(x=j2s, type=\"histogram\", name=\"E[Zu] = [1,1,1]\")\n\nfig"
  },
  {
    "objectID": "iv/iv.html#simulated-distribution-of-hatbetaiv",
    "href": "iv/iv.html#simulated-distribution-of-hatbetaiv",
    "title": "Instrumental Variables Estimation",
    "section": "Simulated Distribution of \\(\\hat{\\beta}^{IV}\\)",
    "text": "Simulated Distribution of \\(\\hat{\\beta}^{IV}\\)\n\nFirst stage \\(X = Z\\gamma + e\\), simulation with \\(\\Er[Z_i Z_i] = I\\) and \\(e \\sim N(0,0.25)\\), so first stage \\(t \\approx \\sqrt{n}\\gamma/0.5\\)\nDistribution of \\(\\hat{\\beta}^{IV}\\) with \\(\\gamma = 1\\), \\(\\gamma=0.2\\), and \\(\\gamma=0.1\\)"
  },
  {
    "objectID": "iv/iv.html#simulated-distribution-of-hatbetaiv-1",
    "href": "iv/iv.html#simulated-distribution-of-hatbetaiv-1",
    "title": "Instrumental Variables Estimation",
    "section": "Simulated Distribution of \\(\\hat{\\beta}^{IV}\\)",
    "text": "Simulated Distribution of \\(\\hat{\\beta}^{IV}\\)\n\n\nCode\nfunction tiv(y,X,Z; b0 = ones(size(X,2)))\n  b = biv(y,X,Z)\n  u = y - X*b\n  V = var(u)*inv(X'*Z*inv(Z'*Z)*Z'*X)\n  (b - b0)./sqrt.(diag(V))\nend\nn = 100\nS = 10_000\nplt = PlotlyLight.Plot()\nfor g in [1, 0.2, 0.1]\n  b = [tiv(sim(n,d=1,EZu=0,gamma=g)...)[1] for _ in 1:S]\n  # crop outliers so figure looks okay\n  b .= max.(b, -4)\n  b .= min.(b, 4)\n  plt(x=b, type=\"histogram\",name=\"γ=$g\")\nend\nfig=plt(x=randn(S), type=\"histogram\", name=\"Normal\")\n\nfig"
  },
  {
    "objectID": "iv/iv.html#weak-instruments-1",
    "href": "iv/iv.html#weak-instruments-1",
    "title": "Instrumental Variables Estimation",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nLessons from simulation:\n\nWhen \\(\\Er[Z_i X_i']\\) is small, usual asymptotic distribution is a poor approximation for the finite sample distribution of \\(\\hat{\\beta}^{IV}\\)\nThe approximation can be poor even when \\(H_0: \\gamma = 0\\) in \\(X = Z\\gamma + e\\) would be rejected\n\nCan we find a better approximation to the finite sample distribution when \\(\\Er[Z_i X_i']\\) is small?"
  },
  {
    "objectID": "iv/iv.html#irrelevant-instrument-asymptotics",
    "href": "iv/iv.html#irrelevant-instrument-asymptotics",
    "title": "Instrumental Variables Estimation",
    "section": "Irrelevant Instrument Asymptotics",
    "text": "Irrelevant Instrument Asymptotics\n\nSuppose \\(\\Er[Z_i X_i'] = 0\\)\nCLT \\[\n\\frac{1}{\\sqrt{n}} \\begin{pmatrix}\nvec(Z'X) \\\\\nZ'u\n\\end{pmatrix} \\indist \\begin{pmatrix} \\zeta_1 \\\\ \\zeta_2 \\end{pmatrix} \\sim N(0, \\Sigma)\n\\]\nThen \\[\n\\begin{align*}\n\\hat{\\beta}^{IV} - \\beta_0 = & \\left((Z'X)'(Z'Z)^{-1}(Z'X)\\right)^{-1} (Z'X)'(Z'Z)^{-1}(Z'u) \\\\\n\\indist & \\left(H' \\Er[Z_i Z_i]^{-1} H\\right)^{-1} \\left(H \\Er[Z_i Z_i']^{-1} \\zeta_2\\right)\n\\end{align*}\n\\] where \\(vec(H) = \\zeta_1\\)"
  },
  {
    "objectID": "iv/iv.html#weak-instrument-asymptotics",
    "href": "iv/iv.html#weak-instrument-asymptotics",
    "title": "Instrumental Variables Estimation",
    "section": "Weak Instrument Asymptotics",
    "text": "Weak Instrument Asymptotics\n\nLet \\(\\Er[Z_i X_i'] = \\frac{1}{\\sqrt{n}} \\Gamma\\)\nThen \\(\\frac{1}{\\sqrt{n}} Z' X = \\Gamma + H\\)\nand \\[\n\\hat{\\beta}^{IV} - \\beta_0 \\indist\n\\left((\\Gamma + H)' \\Er[Z_i Z_i]^{-1} (\\Gamma + H)\\right)^{-1} \\left((\\Gamma + H) \\Er[Z_i Z_i']^{-1} \\zeta_2\\right)\n\\]\n\\(\\Gamma\\) cannot be estimated, but we can try to develop estimators and inference methods for \\(\\beta\\) that work for any \\(\\Gamma\\)"
  },
  {
    "objectID": "iv/iv.html#testing-for-relevance",
    "href": "iv/iv.html#testing-for-relevance",
    "title": "Instrumental Variables Estimation",
    "section": "Testing for Relevance",
    "text": "Testing for Relevance\n\nModel , assume \\(\\Er[W_i u_i] = 0\\) and \\(\\Er[Z_i u_i] = 0\\) \\[\nY_i = X_i'\\beta + W_i'\\beta_W + u_i\n\\]\nFirst stage \\[\nX_i = Z_i' \\pi_z + W_i' \\pi_W + \\nu_i\n\\]\nCan test \\(H_0 : \\pi_z = 0\\) vs \\(H_1 : \\pi_z \\neq 0\\) using F-test\n\nWith one instrument, \\(F = t^2\\)\nRejecting \\(H_0\\) at usual significance level is not enough for \\(\\hat{\\beta}^{IV}\\) to be well aproximated by its asymptotic normal distribution"
  },
  {
    "objectID": "iv/iv.html#testing-for-relevance-1",
    "href": "iv/iv.html#testing-for-relevance-1",
    "title": "Instrumental Variables Estimation",
    "section": "Testing for Relevance",
    "text": "Testing for Relevance\n\nStock and Yogo (2002) (table from Stock, Wright, and Yogo (2002)): first stage F &gt; threshold \\(\\approx 10\\) implies \\(Bias(\\hat{\\beta}^{IV}) &lt; 10\\% Bias(\\hat{\\beta}^{OLS})\\) and size of 5% test &lt; 15%\n\n\nswy-tab1.png"
  },
  {
    "objectID": "iv/iv.html#testing-for-relevance-2",
    "href": "iv/iv.html#testing-for-relevance-2",
    "title": "Instrumental Variables Estimation",
    "section": "Testing for Relevance",
    "text": "Testing for Relevance\n\nLee et al. (2022) : F\\(&gt;&gt;10\\) is needed in practice1\n\nThe argument here is that practitioners misuse the \\(F&gt;10\\) threshold, not that Stock and Yogo (2002) is wrong."
  },
  {
    "objectID": "iv/iv.html#identification-robust-inference",
    "href": "iv/iv.html#identification-robust-inference",
    "title": "Instrumental Variables Estimation",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\nOpinion: always do this, testing for relevance not needed\nTest \\(H_0: \\beta = \\beta_0\\) vs \\(\\beta \\neq \\beta_0\\) with Anderson-Rubin test \\[\nAR(\\beta) = n\\left(\\frac{1}{n} Z'(y-X\\beta) \\right)' \\Sigma(\\beta)^{-1} \\left(\\frac{1}{n} Z'(y - X\\beta)\\right)\n\\] where \\(\\Sigma(\\beta) = \\frac{1}{n} \\sum_{i=1}^n Z_iZ_i' (y_i - X_i'\\beta)^2\\)\n\\(AR(\\beta) \\indist \\chi^2_d\\) (under either weak instrument or usual asymptotics)\nSee my other notes for simulations and references"
  },
  {
    "objectID": "iv/iv.html#identification-robust-inference-1",
    "href": "iv/iv.html#identification-robust-inference-1",
    "title": "Instrumental Variables Estimation",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\nTwo downsides of AR test:\n\nAR statistic is similar to over-identifying test (\\(AR(\\hat{\\beta}^{IV}) = J\\))\n\n\nSmall (even empty) confidence region if model is misspecified\n\n\nOnly gives confidence region for all of \\(\\beta\\), not confidence intervals for single co-ordinates\n\nKleibergen’s LM and Moreira CLR tests address 1, see my other notes for simulations and references"
  },
  {
    "objectID": "iv/iv.html#identification-robust-inference-2",
    "href": "iv/iv.html#identification-robust-inference-2",
    "title": "Instrumental Variables Estimation",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\nVarious approaches to 2 see Andrews, Stock, and Sun (2019) for a review\n\nLondschien and Bühlmann (2024) seems like a promising approach, implemented in ivmodels python package (assumes homoscedasticity)\nGuggenberger, Kleibergen, and Mavroeidis (2024) and Tuvaandorj (2024) allow heteroscedasticity\n\nIf you want something close to the usual t-test and have 1 endogenous regression and 1 instrument, the tF test from Lee et al. (2022), or better yet, recently improved VtF test in Lee et al. (2023)"
  },
  {
    "objectID": "iv/iv.html#further-reading",
    "href": "iv/iv.html#further-reading",
    "title": "Instrumental Variables Estimation",
    "section": "Further Reading",
    "text": "Further Reading\n\nRecent reviews:\n\nAndrews, Stock, and Sun (2019)\nKeane and Neal (2023)"
  },
  {
    "objectID": "iv/iv.html#model-1",
    "href": "iv/iv.html#model-1",
    "title": "Instrumental Variables Estimation",
    "section": "Model",
    "text": "Model\n\n\\(Z_i \\in \\{0,1\\}\\)\n\\(D_i \\in \\{0,1\\}\\)\nPotential treatments \\(D_i(z)\\)\nPotential outcomes \\(Y_i(d)\\)\nExogenous instruments \\(Y_i(0),Y_i(1), D_i(0), D_i(1) \\indep Z_i\\)"
  },
  {
    "objectID": "iv/iv.html#late",
    "href": "iv/iv.html#late",
    "title": "Instrumental Variables Estimation",
    "section": "LATE",
    "text": "LATE\n\nWald estimator \\[\n\\frac{\\Er[Y_i | Z_i=1] - \\Er[Y_i|Z_i=0]}{\\Er[D_i|Z_i=1] - \\Er[D_i|Z_i=0]} =  \\frac{\\Er[Y_i(D_i(1))] - \\Er[Y_i(D_i(0))]}{\\Er[D_i(1)] - \\Er[D_i(0)]} \\text{ (exogeneity)\n\\]\n\n\\[\n= \\frac{\\Er[Y_i(D_i(1)) - Y_i(D_i(0)) | D_i(1) \\neq D_i(0)] P(D_i(1) \\neq D_i(0))} { P(D_i(1) &gt; D_i(0)) - P(D_i(1) &lt; D_i(0))}\n\\]\n\nAssume monotonicity \\(P(D_i(1)&lt;D_i(0)) = 0\\), then \\[\n\\frac{\\Er[Y_i | Z_i=1] - \\Er[Y_i|Z_i=0]}{\\Er[D_i|Z_i=1] - \\Er[D_i|Z_i=0]} = \\Er[Y_i(1) - Y_i(0) | D_i(1)=1, D_i(0) = 0 ]\n\\]\nlocal average treatment effect"
  },
  {
    "objectID": "iv/iv.html#ivlate",
    "href": "iv/iv.html#ivlate",
    "title": "Instrumental Variables Estimation",
    "section": "IV=LATE",
    "text": "IV=LATE\n\nWith single binary \\(Z\\) and \\(D\\), \\[\n\\begin{align*}\n\\hat{\\beta}^{IV} & = \\frac{\\sum Y_i(Z_i - \\bar{Z})} {\\sum D_i(Z_i - \\bar{Z})} \\\\\n& \\inprob \\Er[Y_i(1) - Y_i(0) | D_i(1)=1, D_i(0) = 0 ]\n\\end{align*}\n\\]\n\n\n\n\n\nHow general is this interpretation?\n\nMulti-valued \\(D\\)?\nMulti-values or multiple \\(Z\\)?\nExogenous controls \\(X\\)?\n\nCan salvage some LATE like intrepretation with multiple treatments or instruments, but monotonocity assumption needs to be stronger\n\nMogstad and Torgovitsky (2024) for comprehensive review"
  },
  {
    "objectID": "iv/iv.html#controls",
    "href": "iv/iv.html#controls",
    "title": "Instrumental Variables Estimation",
    "section": "Controls",
    "text": "Controls\n\nConditional exogeneity: \\(Y_i(0),Y_i(1), D_i(0), D_i(1) \\indep Z_i | X_i\\)\nEstimate \\[\ny_i = D_i \\beta + X_i'\\gamma + \\epsilon_i\n\\] by 2SLS\nPartial out \\(X\\) to show \\[\n\\hat{\\beta}^{IV} = \\frac{\\sum y_i \\tilde{Z}_i}{\\sum D_i \\tilde{Z}_i}\n\\] where \\(\\tilde{Z}_i = Z_i - X_i' (X'X)^{-1} X'Z\\)"
  },
  {
    "objectID": "iv/iv.html#sls-with-controls",
    "href": "iv/iv.html#sls-with-controls",
    "title": "Instrumental Variables Estimation",
    "section": "2SLS with Controls",
    "text": "2SLS with Controls\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV} = & \\frac{\\sum y_i \\tilde{Z}_i}{\\sum D_i \\tilde{Z}_i} \\\\\n\\inprob & \\frac{\\Er[Y_i \\tilde{Z}_i]}{\\Er[D_i \\tilde{Z}_i]} \\\\\n= & \\frac{\\Er[\\cov(Y_i \\tilde{Z}_i|X_i)] + \\Er[\\Er[Y_i|X_i]\\Er[\\tilde{Z}_i|X_i]]}{\\Er[D_i \\tilde{Z}_i]}\n\\end{align*}\n\\] - If \\(\\Er[\\Er[Y_i|X_i]\\Er[\\tilde{Z}_i|X_i]] = 0\\), we get average of \\(X\\) specific LATEs - But unless \\(\\Er[Z_i|X_i]\\) is linear, \\(\\Er[\\Er[Y_i|X_i]\\Er[\\tilde{Z}_i|X_i]] \\neq 0\\)"
  },
  {
    "objectID": "iv/iv.html#sls-with-controls-is-not-late",
    "href": "iv/iv.html#sls-with-controls-is-not-late",
    "title": "Instrumental Variables Estimation",
    "section": "2SLS with Controls is not LATE",
    "text": "2SLS with Controls is not LATE\n\nBlandhol et al. (2022) show \\[\n\\begin{align*}\n\\beta^{IV} \\inprob & \\Er\\left[\\omega(cp,X)\\Er[Y(1) - Y(0) |D(1)&gt;D(0), X] \\right] + \\\\\n&  + \\Er\\left[\\omega(at,X)\\Er[Y(1) - Y(0) |D(1)=D(0)=1, X] \\right]\n\\end{align*}\n\\] with \\[\n\\begin{align*}\n\\omega(cp,X) = & \\Er[Z|X](1 - L[Z|X])P(D(1)&gt;D(0)|X)\\Er[\\tilde{Z}D]^{-1} \\\\\n\\omega(at,X) = & \\Er[\\tilde{Z}|X] P(D(1)=D(0)=1|X)\\Er[\\tilde{Z}D]^{-1}\n\\end{align*}\n\\]\n\n\\(\\Er[\\tilde{Z}] = 0\\), so unless \\(\\Er[\\tilde{Z}|X] = 0\\), it will sometimes be negative"
  },
  {
    "objectID": "iv/iv.html#simulation-low-bias",
    "href": "iv/iv.html#simulation-low-bias",
    "title": "Instrumental Variables Estimation",
    "section": "Simulation: Low Bias",
    "text": "Simulation: Low Bias\n\nusing Plots, Statistics, Distributions, Printf\n\nfunction sim(n; ezx = x-&gt;cdf(Normal(),x), Δ = x-&gt;x^2, covde=1, vare=2)\n  xd = randn(n)\n  x = randn(n) + xd\n  de = randn(n)\n  z = rand(n) .&lt; ezx.(x)\n  derr = randn(n)\n  d = (xd + derr + z + de .&gt; 0)\n  d1 = (xd + derr .+ 1 + de .&gt; 0)\n  d0 = (xd + derr .+ 0 + de .&gt; 0)\n  ϵ = de*covde + randn(n)*sqrt(vare-covde^2)\n  y = (Δ.(x) + de).*d + ϵ\n\n  return(y=y,x=x,z=z,d=d,Δ=(Δ.(x) + de), d0=d0, d1=d1)\nend\n\nfunction bols(y,d,x)\n  n = length(y)\n  X = hcat(ones(n), d, x)\n  return((X'*X) \\ X'*y)\nend\n\nfunction b2sls(y,d,x,z)\n  n = length(y)\n  Z = hcat(ones(n), z, x)\n  X = hcat(ones(n), d, x)\n  iZZ = inv(Z'*Z)\n  XZ = X'*Z\n  return((XZ*iZZ*XZ') \\ (XZ*iZZ*(Z'*y)))\nend\n\n\nfunction plotTE(y,d,x,z,Δ,d0,d1; ezx=x-&gt;cdf(Normal(),x))\n  te=scatter(x,Δ, group=[(t0,t1) for (t0,t1) in zip(d0,d1)], alpha=1.0, markersize=1,markerstrokewidth=0)\n  xlabel!(\"x\")\n  ylabel!(\"Treatment Effect\")\n  title!(\"Treatment Effects\")\n  xy=scatter(x,y,group=d,markersize=1,markerstrokewidth=0)\n  xlabel!(\"x\")\n  ylabel!(\"y\")\n  title!(\"Observed Data\")\n  xs = sort(x)\n  pz=plot(xs,ezx.(xs), xlabel=\"x\",ylabel=\"P(Z=1|X)\",title=\"P(Z|X)\",legend=:none)\n  n = length(z)\n  X = hcat(ones(n),x)\n  lzx = X*inv(X'*X)*X'*z\n  scatter!(x,lzx,label=\"L[Z|X]\",markersize=1,markerstrokewidth=0,alpha=0.5)\n\n  bo = bols(y,d,x)[2]\n  bi = b2sls(y,d,x,z)[2]\n  LATE = mean(Δ[d1.&gt;d0])\n  numbers=plot(xlims=(0,1),ylims=(0,1), axis=([], false))\n  annotate!([(0,0.8,(@sprintf(\"E[y1-y0|d1&gt;d0] = %.2f\",LATE),:left)),\n             (0,0.6,(@sprintf(\"βols = %.2f\",bo),:left)),\n             (0,0.4,(@sprintf(\"βiv = %.2f\",bi),:left))])\n\n\n  plot(xy,te,pz,numbers)\nend\n\ny,x,z,d,Δ,d0,d1 = sim(5_000, Δ=x-&gt;1)\nplotTE(y,d,x,z,Δ,d0,d1)"
  },
  {
    "objectID": "iv/iv.html#simulation-low-bias-output",
    "href": "iv/iv.html#simulation-low-bias-output",
    "title": "Instrumental Variables Estimation",
    "section": "Simulation: Low Bias",
    "text": "Simulation: Low Bias"
  },
  {
    "objectID": "iv/iv.html#simulation-low-bias-1",
    "href": "iv/iv.html#simulation-low-bias-1",
    "title": "Instrumental Variables Estimation",
    "section": "Simulation: Low Bias",
    "text": "Simulation: Low Bias\n\nezx = x-&gt;cdf(Normal(),x/10)\ny,x,z,d,Δ,d0,d1 = sim(5_000, Δ=x-&gt;1+x^3/10, ezx = ezx)\nplotTE(y,d,x,z,Δ,d0,d1,  ezx = ezx)"
  },
  {
    "objectID": "iv/iv.html#simulation-high-bias",
    "href": "iv/iv.html#simulation-high-bias",
    "title": "Instrumental Variables Estimation",
    "section": "Simulation: High Bias",
    "text": "Simulation: High Bias\n\ny,x,z,d,Δ,d0,d1 = sim(5_000, Δ=x-&gt;1+x^3/10)\nplotTE(y,d,x,z,Δ,d0,d1)"
  },
  {
    "objectID": "iv/iv.html#observations",
    "href": "iv/iv.html#observations",
    "title": "Instrumental Variables Estimation",
    "section": "Observations",
    "text": "Observations\n\nNonlinearity in \\(\\Er[Z|X]\\) and \\(\\Er[Y|X]\\) can lead to substantial bias in 2SLS"
  },
  {
    "objectID": "iv/iv.html#what-to-do",
    "href": "iv/iv.html#what-to-do",
    "title": "Instrumental Variables Estimation",
    "section": "What to do?",
    "text": "What to do?\n\nFlexibly control for \\(X\\)\nIf discrete, saturated regression\nOtherwise, double robust estimator for average conditional LATE\n\nChernozhukov et al. (2024) chapter 13, doubleml python & R package"
  },
  {
    "objectID": "iv/iv.html#further-reading-1",
    "href": "iv/iv.html#further-reading-1",
    "title": "Instrumental Variables Estimation",
    "section": "Further Reading",
    "text": "Further Reading\n\nMogstad and Torgovitsky (2024)"
  },
  {
    "objectID": "probability/probability.html#reading",
    "href": "probability/probability.html#reading",
    "title": "Probability",
    "section": "Reading",
    "text": "Reading\n\nSong (2021) chapter 2 (which is the basis for these slides)\nSpanos (2019) chapters 2 & 3\nPollard (2002)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\R{{\\mathbb{R}}}\n\\]"
  },
  {
    "objectID": "probability/probability.html#probability-space",
    "href": "probability/probability.html#probability-space",
    "title": "Probability",
    "section": "Probability Space",
    "text": "Probability Space\n\n\n\nDefinitions\n\n\n\nGiven a measure space \\((\\Omega ,\\mathscr{F})\\), a probability (or probability measure)\\(\\ P\\) is a measure s.t. \\(P(\\Omega )=1\\)\n\\((\\Omega ,\\mathscr{F}, P)\\) is a probability space\n\\(\\Omega\\) is a sample space\n\\(\\omega \\in \\Omega\\) is an outcome\n\\(A \\in \\mathscr{F}\\) is an event\n\n\n\n\n\nEmphasize that development of probability was motivated by practical problems.\nGive an example with coins or dice.\nEvents needed both because intrinsically interesting and to describe measurability."
  },
  {
    "objectID": "probability/probability.html#random-variable",
    "href": "probability/probability.html#random-variable",
    "title": "Probability",
    "section": "Random Variable",
    "text": "Random Variable\n\n\n\nDefinition\n\n\nA random variable \\(X\\) is a measurable function from \\(\\Omega\\) to \\(\\R\\)\n\n\n\n\nA sample space can be a set of anything. Most analysis will be done with numbers, so we need random variables."
  },
  {
    "objectID": "probability/probability.html#distribution",
    "href": "probability/probability.html#distribution",
    "title": "Probability",
    "section": "Distribution",
    "text": "Distribution\n\n\n\nDefinition\n\n\nLet \\((\\Omega ,\\mathscr{F},P)\\) be a probability space, \\(X\\) a random variable on \\((\\Omega ,\\mathscr{F})\\). A distribution \\(P_{X}\\) induced by \\(X\\) is a probability measure on \\((\\R,\\mathscr{B}(\\R))\\) such that : \\(\\forall B\\in \\mathscr{B}(\\R)\\), \\[\nP_{X}(B)\\equiv P\\left\\{ \\omega \\in \\Omega :X(\\omega )\\in B\\right\\}\n\\]\n\n\n\n\nIn many applications, the underlying probability \\((\\Omega, \\matscr{F}, P)\\) will mostly sit in the background and most calculations will be done with \\((\\R, \\mathscr{B})(\\R), P_X)\\).\nGiven an example."
  },
  {
    "objectID": "probability/probability.html#distribution-function",
    "href": "probability/probability.html#distribution-function",
    "title": "Probability",
    "section": "Distribution Function",
    "text": "Distribution Function\n\n\n\nDefinition\n\n\nThe cumulative distribution function (CDF) of a random variable \\(X\\) with distribution \\(P_{X}\\) is defined to be a function \\(F_X:\\R\\rightarrow [0,1]\\) such that \\[\nF_X(t)=P_{X}\\left( (-\\infty ,t]\\right) .\n\\]\n\n\n\n\nBy definition, we can go from \\(P_X\\) to \\(F_X\\). By Cartheodary’s extension theorem, we can do the reverse. Either the CDF or \\(P_X\\) tells completely characterizes the distribution."
  },
  {
    "objectID": "probability/probability.html#density",
    "href": "probability/probability.html#density",
    "title": "Probability",
    "section": "Density",
    "text": "Density\n\n\n\nDefinition\n\n\n\nLet \\(X\\) be a random variable with distribution \\(P_{X}\\). When \\(P_{X}\\ll \\lambda\\), we call \\(X\\) a continuous random variable, and call the Radon-Nikodym derivative \\(f_X\\equiv dP_{X}/d\\lambda\\) the (probability) density function of \\(P_{X}\\).\nWe say that \\(X\\) is a discrete random variable, if there exists a countable set \\(A\\subset \\R\\) and such that \\(P_{X}A^{c}=0\\)\n\n\n\n\n\nDensity wrt to other measures? e.g. dirac?"
  },
  {
    "objectID": "probability/probability.html#markovs",
    "href": "probability/probability.html#markovs",
    "title": "Probability",
    "section": "Markov’s",
    "text": "Markov’s\n\n\n\nMarkov’s Inequality\n\n\n\\(P(|X|&gt;\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)"
  },
  {
    "objectID": "probability/probability.html#jensens",
    "href": "probability/probability.html#jensens",
    "title": "Probability",
    "section": "Jensen’s",
    "text": "Jensen’s\n\n\n\nJensen’s Inequality\n\n\nSuppose that \\(g\\) is convex and \\(X\\) and \\(g(X)\\) are integrable, then \\(g(\\Er X) \\leq \\Er[g(X)]\\)\n\n\n\n\n\n\nExercise\n\n\nShow \\(\\Er[|X|^p] \\leq \\left(\\Er[|X|^q] \\right)^{p/q}\\) for all \\(0 &lt; p \\leq q\\)."
  },
  {
    "objectID": "probability/probability.html#cauchy-schwarz",
    "href": "probability/probability.html#cauchy-schwarz",
    "title": "Probability",
    "section": "Cauchy-Schwarz",
    "text": "Cauchy-Schwarz\n\n\n\nCauchy-Schwarz Inequality\n\n\n\\(\\left(\\Er[XY]\\right)^2 \\leq \\Er[X^2] \\Er[Y^2]\\)\n\n\n\n\nProof:\n\n\\(\\Er[(X-Y)^2] \\geq 0\\), so\n\\(2\\Er[XY] \\leq \\Er[X^2] + \\Er[Y^2]\\)\n\\(\\Er[XY] = \\Er[\\lambda X \\lambda^{-1}Y]\\), so\n\\(2\\Er[XY] \\leq \\lambda^2 \\Er[X^2] + \\lambda^{-2} \\Er[Y^2]\\)\nset \\(\\lambda^2 = \\sqrt{\\Er[X^2]/\\Er[Y^2]}\\)"
  },
  {
    "objectID": "probability/probability.html#generated-sigma-field",
    "href": "probability/probability.html#generated-sigma-field",
    "title": "Probability",
    "section": "Generated \\(\\sigma\\)-field",
    "text": "Generated \\(\\sigma\\)-field\n\n\\(\\sigma(X)\\) is \\(\\sigma\\)-field generated by \\(X\\)\n\nsmallest \\(\\sigma\\)-field w.r.t. which \\(X\\) is measurable\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R)\\}\\)\n\n\n\nExplain some simple example(s)."
  },
  {
    "objectID": "probability/probability.html#information",
    "href": "probability/probability.html#information",
    "title": "Probability",
    "section": "Information",
    "text": "Information\n\n\\(\\forall E \\in \\sigma(X)\\), observing value \\(x\\) of \\(X\\), tells us whether \\(E\\) occurred\nif \\(\\sigma(X_1) \\subset \\sigma(X_2)\\), then \\(\\sigma(X_2)\\) has more information than \\(\\sigma(X_1)\\)\n\n\nExample 4.3 in Song (2021)."
  },
  {
    "objectID": "probability/probability.html#dependence",
    "href": "probability/probability.html#dependence",
    "title": "Probability",
    "section": "Dependence",
    "text": "Dependence\n\n\n\nTheorem 4.2\n\n\nSuppose \\(\\sigma(W) \\subset \\sigma(X)\\), then \\(\\exists\\) Borel measurable \\(g\\) s.t. \\(W=g(X)\\)\n\n\n\n\nProof in Çinlar (2011) chapter 2, section 4."
  },
  {
    "objectID": "probability/probability.html#independence-1",
    "href": "probability/probability.html#independence-1",
    "title": "Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\nDefinition\n\n\n\nEvents \\(A_1, ..., A_m\\) are independent if for any sub-collection \\(A_{i_1}, ..., A_{i_s}\\) \\[\nP\\left(\\cap_{j=1}^s A_{i_j}\\right) = \\prod_{j=1}^s P(A_{i_j})\n\\]\n\\(\\sigma\\)-fields, \\(\\mathscr{F}_1, .., \\mathscr{F}_m \\subset\n\\mathscr{F}\\) are independent if for any \\(\\mathscr{F}_{i_1}, ..,\n\\mathscr{F}_{i_s}\\) and \\(E_j \\in \\mathscr{F}_j\\), \\[\nP\\left(\\cap_{j=1}^s E_{i_j}\\right) = \\prod_{j=1}^s P(E_{i_j})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are independent if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are independent\n\n\n\n\n\nShow some simple example(s)."
  },
  {
    "objectID": "probability/probability.html#random-vectors",
    "href": "probability/probability.html#random-vectors",
    "title": "Probability",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nmeasurable \\(X: \\Omega \\to \\R^n\\)\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R^n)\\} =\\) smallest \\(\\sigma\\)-field containing \\(\\cup_{i=1}^n \\sigma(X_i)\\)\n\n\n\n\nTheorem\n\n\nSuppose that \\(X=(X_1, X_2)\\) and \\(Y=(Y_1, Y_2)\\) are independent, then \\(f(X)\\) and \\(g(Y)\\) are independent\n\n\n\n\nAs promised, an easy way to show obvious independence without any tedious calculations."
  },
  {
    "objectID": "probability/probability.html#conditional-expectation",
    "href": "probability/probability.html#conditional-expectation",
    "title": "Probability",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\nDefinition\n\n\nLet \\(\\mathscr{G} \\subset \\mathscr{F}\\) be \\(\\sigma\\)-fields, \\(Y\\) a random variable with \\(\\Er |Y| &lt; \\infty\\), then the conditional expectation of \\(Y\\) given \\(\\mathscr{G}\\) is \\(\\Er[Y|\\mathscr{G}](\\cdot): \\Omega \\to \\R\\) s.t.\n\n\\(\\Er[Y|\\mathscr{G}](\\cdot)\\) is \\(\\mathscr{G}\\) measurable\n\\(\\int_A \\Er[Y|\\mathscr{G}] dP = \\int_A Y dP\\) \\(\\forall A \\in \\mathscr{G}\\)\n\n\n\n\n\nEx: \\(\\{E_k\\}_{k=1}^m\\) partition of \\(\\Omega\\), let \\(\\mathscr{G} = \\sigma(\\{E_k\\}_{k=1}^m)\\)\n\n\\(\\Er[Y | \\mathscr{G}](\\omega) = \\sum_{k=1}^m c_k 1\\{\\omega \\in E_k\\}\\), then use 2 to solve for \\(c_k\\)\n\nExistence from Radon-Nikodym theorem\n\\(\\Er[Y|X] \\equiv \\Er[Y|\\sigma(X)]\\)"
  },
  {
    "objectID": "probability/probability.html#conditional-expectation-1",
    "href": "probability/probability.html#conditional-expectation-1",
    "title": "Probability",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\nExercise\n\n\n\nIf \\(X\\) and \\(Y\\) are discrete with support \\(\\{x_i\\}_{i=1}^I \\times \\{y_j\\}_{j=1}^J\\) and PMF \\(p\\) then \\[\n\\Er[Y|X=x_i] = \\frac{\\sum_{j=1}^J y_j p(x_i,y_j)} {\\sum_{j=1}^J p(x_i,y_j)}\n\\]\nIf \\(X\\) and \\(Y\\) are continuous with density \\(f\\), then \\[\n\\Er[Y|X=x] = \\frac{\\int y f(x,y) dy}{\\int f(x,y) dy}\n\\]"
  },
  {
    "objectID": "probability/probability.html#properties-of-conditional-expectation",
    "href": "probability/probability.html#properties-of-conditional-expectation",
    "title": "Probability",
    "section": "Properties of Conditional Expectation",
    "text": "Properties of Conditional Expectation\n\nIf \\(X\\) is \\(\\mathscr{G}\\) measurable, then \\(\\Er[XY| \\mathscr{G}] = X \\Er[Y|\\mathscr{G}]\\) a.e.\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[XY|Z] = X \\Er[Y|Z]\\)\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[\\Er[Y|Z]|X] = \\Er[Y|X]\\)\nIf \\(Y\\) and \\(X\\) are independent, then \\(\\Er[Y | X ] = \\Er[Y]\\)"
  },
  {
    "objectID": "probability/probability.html#erymathscrg-as-orthogonal-projection",
    "href": "probability/probability.html#erymathscrg-as-orthogonal-projection",
    "title": "Probability",
    "section": "\\(\\Er[Y|\\mathscr{G}]\\) as Orthogonal Projection",
    "text": "\\(\\Er[Y|\\mathscr{G}]\\) as Orthogonal Projection\n\n\n\nTheorem\n\n\nLet \\((\\Omega, \\mathscr{F}, P)\\) be a probability space, \\(\\mathscr{G}\\) a sub \\(\\sigma\\)-field, then for any \\(Y \\in \\mathcal{L}^2(\\Omega, \\mathscr{F}, P) = \\{X: \\Omega \\to \\mathbb{R} \\text{ s.t. } X \\text{ }\\mathscr{F}\\text{-measurable, } \\int X^2 dP &lt; \\infty \\}\\), \\[\n\\inf_{W \\in \\mathcal{L}^2(\\Omega, \\mathscr{G}, P)} \\Er[(Y-W)^2] = \\Er[ (Y - \\Er[Y | \\mathscr{G}])^2]\n\\]"
  },
  {
    "objectID": "probability/probability.html#conditional-measure",
    "href": "probability/probability.html#conditional-measure",
    "title": "Probability",
    "section": "Conditional Measure",
    "text": "Conditional Measure\n\n\n\nDefinition\n\n\nLet \\(\\mathscr{G}\\) be a sub \\(\\sigma\\)-field of \\(\\mathscr{F}\\). Tthe conditional probability measure given \\(\\mathscr{G}\\) is defined to be a map \\(P(\\cdot \\mid \\mathscr{G})(\\cdot ):\\mathscr{F}\\times \\Omega \\rightarrow [0,1]\\) such that\n\nFor each \\(A\\in \\mathscr{F}\\), \\(P(A \\mid \\mathscr{G})(\\cdot )=\\mathbf{E}\\left[ 1\\{\\omega \\in A\\} \\mid \\mathscr{G}\\right] (\\cdot )\\), a.e.\nfor each \\(\\omega \\in \\Omega\\), \\(P(\\cdot \\mid \\mathscr{G})(\\omega )\\) is a probability measure on \\((\\Omega ,\\mathscr{F}).\\)"
  },
  {
    "objectID": "probability/probability.html#conditional-independence",
    "href": "probability/probability.html#conditional-independence",
    "title": "Probability",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\n\n\nDefinition\n\n\n\nEvents \\(A_1, ..., A_m \\in \\mathscr{F}\\) are conditionally independent given \\(\\mathscr{G}\\) if for any sub-collection, \\[\nP\\left( \\cap_{j=1}^s A_{i_j} | \\mathscr{G} \\right) = \\prod_{j=1}^s P(A_{i_j} | \\mathscr{G})\n\\]\nSub \\(\\sigma\\)-fields \\(\\mathscr{F}_1, ..., \\mathscr{F}_m\\) are conditionally independent given \\(\\mathscr{G}\\) if for any sub-collection and events, \\(E_i \\in \\mathscr{F}_i\\), \\[\nP\\left( \\cap_{j=1}^s E_{i_j} | \\mathscr{G} \\right) = \\prod_{j=1}^s P(E_{i_j} | \\mathscr{G})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are conditionally independent given \\(\\mathscr{G}\\) if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are conditionally independent given \\(\\mathscr{G}\\)"
  },
  {
    "objectID": "problemsets/02/ps02.html",
    "href": "problemsets/02/ps02.html",
    "title": "ECON 626: Problem Set 2",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\]\n\nProblem 1\nLet \\(X_{1},...,X_{n}\\) be independent and identically distribution and \\(P(X_{1}\\leq t)=F(t)\\) for some function \\(F\\). Then, write the probability \\(P(\\max_{1\\leq i\\leq n}X_{i}\\leq t)\\) in terms of \\(F\\).\n\n\nProblem 2\n\nShow that if \\(E[Y |X] = 0\\), then \\(Y\\) and \\(g(X)\\) are uncorrelated for any (Borel) measurable function \\(g\\). Do you have the same conclusion if we weaken the condition \\(E[Y |X] = 0\\) to \\(E[Y |X] = a\\) for some fixed constant \\(a \\in \\R\\)?\nSuppose that \\(E[Y^2 |X] &lt; a^2\\) for some constant \\(a &gt; 0\\). Then, show that for any \\(b &gt; 0\\), \\[\nP\\left(|Y − EY | &gt; b\\right) \\leq \\frac{a^2}{b^2}.\n\\]\nShow that if \\(E[Y | \\exp(X)] = E[Y]\\), then the correlation between \\(E[Y | cos(X)]\\) and \\(cos(X)\\) is zero.\n\n\n\nProblem 3\nLet \\(X: \\Omega \\to \\R\\), \\(W:  \\Omega \\to \\R\\), \\(Z: \\Omega \\to \\R\\), and \\(D: \\Omega \\to \\{0, 1\\}\\) be random variables. Let \\(Y = D X + (1-D) W\\). Suppose that \\(Y\\), \\(D\\), and \\(Z\\) are observed, but \\(X\\) and \\(W\\) are not.\n\nSuppose \\(D\\) is independent of \\(X\\), \\(W\\). Then show that \\(\\Er[X - W]\\) is identified.\nSuppose \\(Z\\) is independent of \\(X\\) and \\(W\\), and \\(\\exists E_1, E_0 \\in \\sigma(Z)\\) such that \\(P(D=1 | E_1) = 1\\) and \\(P(D=0|E_0) = 1\\). Then show that \\(\\Er[X-W]\\) is identified."
  },
  {
    "objectID": "problemsets/04/ps04.html",
    "href": "problemsets/04/ps04.html",
    "title": "ECON 626: Problem Set 4",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\]\n\nProblem 1\nThe exponential distribution has density (with respect to Lesbegue measure) \\[\nf(X;\\lambda) = \\frac{1}{\\lambda} e^{-X/\\lambda} 1\\{X &gt; 0\\}\n\\] for \\(\\lambda&gt;0\\). Suppose \\(X_1, ... , X_n\\) are independently exponential\\((\\lambda)\\) distributed .\n\nShow that the maximum likelihood estimator for \\(\\lambda\\) is \\(\\hat{\\lambda}^{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i\\)\nDerive the Cramér Rao lower bound for any unbiased estimator for \\(\\lambda\\). Is \\(\\hat{\\lambda}^{MLE}\\) a minimum variance unbiased estimator?\nFind the most powerful test of size \\(\\alpha\\) for testing \\(H_0: \\lambda = \\lambda_0\\) versus \\(H_1:\\lambda = \\lambda_1\\).\n\n\n\nProblem 2\nSuppose \\(X_1, ... , X_n\\) are independently uniformly distributed on \\((0,\\theta)\\).\n\nShow that \\(2 \\bar{X} = \\frac{2}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator for \\(\\theta\\).\nShow that \\(\\hat{\\theta} = \\frac{n+1}{n} \\max_{1 \\leq i \\leq n} X_i\\) is an unbiased estimator for \\(\\theta\\) with \\(Var(\\hat{\\theta}) &lt; Var(2 \\bar{X})\\).\nIs there a Cramér Rao lower bound for the variance of \\(\\theta\\)? Why or why not?\n(Optional and challenging) Show that \\(\\hat{\\theta} = \\frac{n+1}{n} \\max_{1 \\leq i \\leq n} X_i\\) is a minimum variance unbiased estimator for \\(\\theta\\)."
  },
  {
    "objectID": "problemsets/06/ps06.html",
    "href": "problemsets/06/ps06.html",
    "title": "ECON 626: Problem Set 6",
    "section": "",
    "text": "\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\\]"
  },
  {
    "objectID": "problemsets/06/ps06.html#section",
    "href": "problemsets/06/ps06.html#section",
    "title": "ECON 626: Problem Set 6",
    "section": "1",
    "text": "1\nFind \\(\\plim \\hat{\\beta}_1\\), where \\(\\hat{\\beta}_1\\) is the OLS etimator."
  },
  {
    "objectID": "problemsets/06/ps06.html#section-1",
    "href": "problemsets/06/ps06.html#section-1",
    "title": "ECON 626: Problem Set 6",
    "section": "2",
    "text": "2\nIn ``Do Low Levels of Blood Lead Reduce Children’s Future Test Scores?’’ Aizer et al. (2018) examine the relationship between blood lead levels and children’s test scores. Table 4 shows estimates of \\(\\beta_1\\) from regressions of\n\\[\ntest_i = \\beta_0 + \\beta_1 lead_i + \\text{ other controls} + \\epsilon_i\n\\tag{1}\\]\nwhere \\(test_i\\) is a 3rd grade reading or math test and \\(lead_i\\) is a blood lead level measurement taken before the age of six. Some children had multiple measurements of their blood lead levels taken. Each blood lead level measurement has some error. In comparing columns (1) and (2), note that venous tests are known to have less measurement error than capillary tests, and in comparing columns (3) and (4) the average of all blood lead levels has less measurement error than a single one. Are the changes in the estimates across columns what you would expect from part part 1? Why or why not?\n\n\n\nAizer et al. (2018) table 4"
  },
  {
    "objectID": "problemsets/06/ps06.html#section-2",
    "href": "problemsets/06/ps06.html#section-2",
    "title": "ECON 626: Problem Set 6",
    "section": "3",
    "text": "3\nSuppose \\(z_i\\) is a second measurement of \\(x^*\\), [ z_i = x^*_i + e_i ] with \\(\\Er[e] = 0\\), \\(\\Er[x^* e] = 0\\), \\(\\Er[\\epsilon e] = 0\\) and \\(\\Er[e u] = 0\\). Show that \\[\n\\hat{\\beta}_1^{IV} = \\frac{\\sum_{i=1}^n (z_i - \\bar{z}) y_i} {\\sum_{i=1}^n\n   (z_i - \\bar{z}) x_i}\n\\] is a consistent estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "problemsets/06/ps06.html#section-3",
    "href": "problemsets/06/ps06.html#section-3",
    "title": "ECON 626: Problem Set 6",
    "section": "4",
    "text": "4\nTable 5 from Aizer et al. (2018), shows additional estimates of the model Equation 1. Column (1) shows standard multiple regression estimates. Columns (2) and (3) show estimates using the estimator \\(\\hat{\\beta}_1^{IV}\\) from part 3. Is the change in the estimates between columns (1) and (2) what you would expect based on parts 1 and 3? Why or why not?\n\n\n\nAizer et al. (2018) table 5"
  },
  {
    "objectID": "problemsets/06/ps06.html#footnotes",
    "href": "problemsets/06/ps06.html#footnotes",
    "title": "ECON 626: Problem Set 6",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis problem is partially based on Borusyak and Jaravel (2018).↩︎"
  },
  {
    "objectID": "problemsets/07/ps07_2022.html",
    "href": "problemsets/07/ps07_2022.html",
    "title": "ECON 626: Problem Set 7",
    "section": "",
    "text": "\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\n\nProblem 1\nConsider the following linear regression model such that \\[\nY_i = β_0 + X_i β_1 + u_i ,\n\\] where \\(X_i\\) and \\(Y_i\\) are observed random variables. Let us assume that \\(\\Er [u_i ] = 0\\) but \\(\\cov(X_i , u_i ) \\neq 0\\). Suppose that there exists a variable \\(Z_i\\) such that \\(\\cov(X_i , Z_i ) &gt; 0\\) and \\(\\cov(Z_i , u_i ) &gt; 0\\).\nFind the asymptotic bias of the 2SLS estimator of \\(\\hat{\\beta}_1\\). (Recall that the asymptotic bias of an estimator is its probability limit minus the true parameter.) Can you determine unambiguously whether the 2SLS estimator tends to underestimate or overestimate the parameter \\(β_1\\) ? If so, give explanations how.\n\n\nProblem 2\nIn the linear model, \\[\nY_i = \\beta_0 + X_i \\beta_1 + u_i\n\\] assume that \\(\\Er[u_i] = 0\\) and \\(X_i \\in \\R^1\\). Suppose that \\(\\Er[X_i u_i] \\neq 0\\), but, somewhat strangely, you assume \\(\\Er[u_i^2|X_i] = \\sigma^2\\).\n\nShow that a set of two elements that contains \\(\\beta_1\\) is identified. Denote this set by \\(B_1\\). Hint: use the moment condition \\(\\Er[u_i^2 (X_i-\\Er[X_i])]\\).\nDescribe an estimator for \\(B_1\\) and show that it is consistent. State any additional assumptions needed.\nFind the asymptotic distribution of your estimator for \\(B_1\\). State any additional assumptions needed."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html",
    "href": "problemsets/final2022/final2022solution.html",
    "title": "ECON 626: Final - Solutions",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\nYou have 150 minutes to complete the exam. The last two pages have some possibly useful formulas.\nThere are 100 total points. There are three questions labeled more difficult that might take longer than others, and you should not spend too much time on these questions until you have answered all the others."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#ols-5-points",
    "href": "problemsets/final2022/final2022solution.html#ols-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "OLS (5 points)",
    "text": "OLS (5 points)\nMore difficult\nTable 2 shows OLS estimates of Equation 1. The row labeled ``Effect of 1 SD change in frequency downwind’’ reports \\(\\hat{\\beta} \\times \\sqrt{ \\frac{1}{n} \\sum_i (w_i -\n  \\bar{w})^2}\\). Sadly, there are no standard errors reported for these estimates. Find the asymptotic distribution of \\(\\hat{\\beta} \\times \\sqrt{ \\frac{1}{n} \\sum_i (w_i -\n  \\bar{w})^2}\\). To simplify this part, you may pretend that \\(x_i\\) is not in the model, assume that observations are independent, and assume that \\(\\Er[\\epsilon_i | w_i] = 0\\).\n\nSolution. With the simplifications, \\(\\hat{\\beta} = (W'W)^{-1} (W'y)\\), and \\(\\hat{\\beta} - \\beta = (W'W)^{-1} W'\\epsilon\\).\nLet \\(\\hat{\\sigma}^2 = \\frac{1}{n} \\sum (w_i - \\bar{w})^2\\). Assume that the data is iid, \\(\\Er[w_i^2 \\epsilon_i^2] &lt; \\infty\\) and \\(\\Er[w_i^4]&lt;\\infty\\). Then, the CLT applies to \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\), so\n\\[\n\\begin{aligned}\n\\sqrt{n} \\begin{pmatrix} \\hat{\\beta} - \\beta \\\\ \\hat{\\sigma}^2 - \\var(w) \\end{pmatrix} \\indist & N\\left(0,\n\\begin{pmatrix} \\Er[w^2]^{-2}\\var(w\\epsilon) & \\cov((w_i - \\Er[w])^2, \\Er[w^2]^{-1}w_i\\epsilon_i) \\\\\n& \\var((w_i - \\Er[w])^2) \\end{pmatrix} \\right) \\\\\n\\indist & N\\left(0, \\begin{pmatrix} \\Er[w^2]^{-2}\\var(w\\epsilon) & 0 \\\\\n0 & \\var((w_i - \\Er[w])^2) \\end{pmatrix} \\right)\n\\end{aligned}\n\\]\nwhere the second line follows from the assumption that \\(\\Er[\\epsilon|w]=0\\).\nNow, we can use the delta method on \\(f(\\beta,\\sigma^2) = \\beta \\sqrt{\\sigma^2}\\), so \\[\n\\sqrt{n} (\\hat{\\beta} \\sqrt{\\hat{\\sigma}^2} - \\beta\\sigma) \\indist N\\left(0, \\sqrt{\\sigma^2}\\Er[w^2]^{-2} \\var(w\\epsilon)+ \\frac{\\beta}{4 \\sigma^2} \\var((w_i - \\Er[w])^4)\\right)\n\\]"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#endogeneity-5-points",
    "href": "problemsets/final2022/final2022solution.html#endogeneity-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Endogeneity (5 points)",
    "text": "Endogeneity (5 points)\nGive one reason why \\(\\Er[w_i \\epsilon_i]\\) might not be zero, and speculate on whether \\(\\Er[w_i \\epsilon_i]\\) is likely to be positive or negative.\n\nSolution. Locations near highways are likely cheaper and have lower income residents. Income could also affect mortality directly. Thus, \\(w\\) could be negatively correlated with income, and income negatively related to mortality. This suggest \\(\\Er[w_i \\epsilon_i] &gt; 0\\)."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#instrument-5-points",
    "href": "problemsets/final2022/final2022solution.html#instrument-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument (5 points)",
    "text": "Instrument (5 points)\nAs an instrument for \\(w_i\\), Anderson (2019) uses ``orientation to the nearest major highway, [encoded] as a set of seven dummy variables. Each dummy variable represents a 45-degree range (e.g., 22.5 degrees to 67.5 degrees, 67.5 degrees to 112.5 degrees, etc.).’’ Let \\(z_i\\) denote these instruments. Does this instrument address the reason for endogeneity you gave in the previous part? Do you believe that \\(\\Er[z_i\\epsilon_i] = 0\\)?\n\nSolution. No this instrument does not address the possible endogeneity of location. It seems unlikely that \\(\\Er[z_i \\epsilon_i] = 0\\).\nTo be fair to Anderson (2019), he addresses this sort of concern by controlling for location fixed effects, and census block characteristics, including income."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#first-stage-5-points",
    "href": "problemsets/final2022/final2022solution.html#first-stage-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "First-Stage (5 points)",
    "text": "First-Stage (5 points)\nTable 3 shows estimates of \\[\nw_i = z_i \\alpha + x_i \\gamma + \\nu_i\n\\tag{2}\\] What important assumption about the instruments can we check with this regression? Should we be concerned about this assumption? What should we do about it?\n\nSolution. We can check for instrument relevance in the first stage. Although, the first stage F-statistic of 30 exceeds the Stock and Yogo (2002) rule of thumb of 10, 30 is now considered too low for standard inference methods to be appropriate. We should use an identification robust inference method. Since the model is overidentified, the KLM or CLR test would be the best choices.\nAny answer that mentions weak identification and using identification robust inference is fine here. For example, mentioning just the AR test would okay."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#sls-5-points",
    "href": "problemsets/final2022/final2022solution.html#sls-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "2SLS (5 points)",
    "text": "2SLS (5 points)\nTable 4 shows 2SLS estimates of Equation 1. One reason Anderson gives for using an instrument is that not all census blocks have weather stations, so there is measurement error in \\(w_i\\). Is the change in estimates between Table 2 and Table 4 what would be expected from measurement error? (Your answer can freely refer to results shown in lecture or problem sets without proof).\n\nSolution. Yes, this is what we would expect. Measurement error biases OLS towards 0. We generally see that the estimates in Table 2 have smaller magnitude than table 4."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#dependence-5-points",
    "href": "problemsets/final2022/final2022solution.html#dependence-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Dependence (5 points)",
    "text": "Dependence (5 points)\nThe observations are from many adjacent census blocks (roughly the same as a city block) in Los Angeles. Briefly, how does this affect the consistency and asymptotic distribution of \\(\\hat{\\beta}\\)? What, if anything, in the tables above needs to be calculated differently than with independent observations?\n\nSolution. The observations are likely dependent. As long as the dependence is not too extreme, a LLN will still apply and consistency is unaffected. However, the asymptotic variance will be different than in the independent case. The standard errors should be (and are) adjusted for spatial dependence."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#estimated-alpha-10-points",
    "href": "problemsets/final2022/final2022solution.html#estimated-alpha-10-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Estimated \\(\\alpha\\) (10 points)",
    "text": "Estimated \\(\\alpha\\) (10 points)\nAssume that \\(\\theta\\) is known, \\(\\Er[\\epsilon_i g(X_i,\\theta)] = 0\\), and \\(\\Er[\\epsilon_i]=0\\). Let \\(\\tilde{X}_i = g(X_i,\\theta)\\). Assume \\(\\Er[\\tilde{X}_i\\tilde{X}_i']\\) is nonsingular, and \\(\\var(\\epsilon_i |\n\\tilde{X}_i) = \\sigma^2\\).\nSuppose you have an estimate \\(\\hat{\\alpha}\\), such that \\(\\sqrt{n}(\\hat{\\alpha}-\\alpha) \\indist N(0,\\Omega)\\) and, for simplicity, is independent of \\(X\\) and \\(Y\\). Find the asymptotic distribution of \\[\n\\hat{\\beta}^{OLS} = (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' h(Y,\\hat{\\alpha})\n\\] where, \\(h(Y,\\hat{\\alpha}) = \\left(h(Y_1,\\hat{\\alpha}), ...,\n  h(Y_n,\\hat{\\alpha}) \\right)'\\)\nHint: consider subtracting and adding \\((\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}' h(Y,\\alpha)\\) to \\(\\hat{\\beta} - \\beta\\).\n\nSolution. Proceeding as suggested by the hint: \\[\n\\begin{align*}\n\\hat{\\beta} - \\beta = & (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' \\left(h(Y, \\hat{\\alpha}) - h(Y,\\alpha) \\right) + (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}' h(Y,\\alpha) - \\beta \\\\\n= & (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' \\left(h(Y, \\hat{\\alpha}) - h(Y,\\alpha) \\right) + (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}' \\epsilon \\\\\n= & (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' \\left(D_\\alpha h(Y, \\alpha)(\\hat{\\alpha} - \\alpha) + \\epsilon + o_p(\\hat{\\alpha}-\\alpha) \\right)\n\\end{align*}\n\\] where the last line used a first order expansion, and \\(D_\\alpha(Y,\\alpha)\\) denotes the \\(n \\times dim(\\alpha)\\) matrix of derivatives of \\(h\\) with respect to \\(\\alpha\\) at each \\(Y_i\\).\nMultiplying by \\(\\sqrt{n}\\), using the convergence of \\(\\hat{\\alpha}\\), its independence from \\(\\epsilon\\), and applying a CLT to \\(\\tilde{X}'\\epsilon\\), we have \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N\\left(0, \\Er[\\tilde{X}_i\\tilde{X}_i]^{-1} \\Er[\\tilde{X}_i D_\\alpha h(Y,\\alpha)] \\Omega \\Er[\\tilde{X}_i D_\\alpha h(Y,\\alpha)]'\\Er[\\tilde{X}_i\\tilde{X}_i]^{-1} + \\Er[\\tilde{X}_i\\tilde{X}_i]^{-1} \\sigma^2 \\right)\n\\]\nYou need not have made this extra observation, but interestingly, simply ignoring the estimated \\(\\alpha\\) and using the usual heteroskedasticity robust standard error would lead to the same distribution. Doing this, the error terms becomes \\(\\epsilon + h(Y_i,\\hat{\\alpha}) - h(Y_i, \\alpha)\\), and then everything just happens to work out."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#estimated-theta-5-points",
    "href": "problemsets/final2022/final2022solution.html#estimated-theta-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Estimated \\(\\theta\\) (5 points)",
    "text": "Estimated \\(\\theta\\) (5 points)\nMore difficult\nNow, assume \\(\\alpha\\) is known, and suppose you have an estimate \\(\\hat{\\theta}\\), such that \\(\\sqrt{n}(\\hat{\\theta}-\\theta) \\indist N(0,\\Omega)\\) and, for simplicity, is independent of \\(X\\) and \\(Y\\). Find the asymptotic distribution of \\[\n\\hat{\\beta}^{OLS} = (g(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} g(X,\\hat{\\theta})' h(Y,\\alpha)\n\\]\nHint: adding and subtracting is useful here too. Note that: \\[\n\\begin{aligned}\ng(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} g(X,\\hat{\\theta})' h(Y,\\alpha) = &\n\\left[(g(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} -\n(g(X,\\theta)'g(X,\\theta))^{-1} \\right]g(X,\\hat{\\theta})' h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} \\left[g(X,\\hat{\\theta}) - g(X,\\theta)\\right]'h(Y,\\alpha) +\n(g(X,\\theta)'g(X,\\theta))^{-1} g(X,\\theta) h(Y,\\alpha)\n\\end{aligned}\n\\]\n\nSolution. The idea here is very similar to part a, but the notation gets heavy.\n\\(g(X,\\theta)\\) is already a matrix, and we haven’t defined a way to denote the derivative of a matrix. Here, I’ll just write everything in terms of partial derivatives with respect to components of \\(\\theta\\), which avoids the problem.\nUsing the hint, we have \\[\n\\begin{aligned}\n\\hat{\\beta} - \\beta = &\n\\left[(g(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} -\n(g(X,\\theta)'g(X,\\theta))^{-1} \\right]g(X,\\hat{\\theta})' h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} \\left[g(X,\\hat{\\theta}) - g(X,\\theta)\\right]'h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} g(X,\\theta) \\epsilon \\\\\n= & \\left[\\sum_{j=1}^J (\\hat{\\theta}_j - \\theta_j) \\underbrace{(-2) (g(X,\\theta)'g(X,\\theta))^{-1} \\frac{\\partial g(X,\\theta)}{\\partial \\theta_j}' g(X,\\theta) (g(X,\\theta)'g(X,\\theta))^{-1}}_{\\equiv D_{j}^{gg-}} + o_p(\\hat{\\theta} - \\theta)\\right]g(X,\\hat{\\theta})' h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} \\left[\\sum_{j=1}^J \\frac{\\partial g(X,\\theta)}{\\partial \\theta_j}(\\hat{\\theta}_j - \\theta_j) + o_p(\\hat{\\theta} - \\theta)\\right]'h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} g(X,\\theta) \\epsilon \\\\\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\indist & N\\left(0, \\sum_{j=1}^J \\sum_{\\ell=1}^J \\omega_{j,l} \\left( \\begin{array}{l} \\Er[D^{gg-}_j] \\Er[g(X,\\theta)'h(Y,\\alpha)] \\Er[g(X,\\theta)'h(Y,\\alpha)]'\\Er[D^{gg-}_\\ell]' + \\\\\n+ \\Er[g(X,\\theta)'g(X,\\theta)]^{-1} \\Er[\\frac{\\partial g(X,\\theta)}{\\partial \\theta_j}'h(Y,\\alpha)] \\Er[\\frac{\\partial g(X,\\theta)}{\\partial \\theta_\\ell}'h(Y,\\alpha)] \\Er[g(X,\\theta)'g(X,\\theta)]^{-1}  \\end{array} \\right) + \\Er[g(X,\\theta)'g(X,\\theta)]^{-1} \\sigma^2 \\right)\n\\end{aligned}\n\\] where \\(\\omega_{j,\\ell}\\) are the entries of \\(\\Omega\\)."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#instrument-9-points",
    "href": "problemsets/final2022/final2022solution.html#instrument-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument (9 points)",
    "text": "Instrument (9 points)\nYou suspect \\(x_i\\) is related to \\(u_i\\), but you have another variable, \\(z_i \\in \\R\\), such that \\(\\Er[u_i | z_i] = 0\\). Use this assumption to construct an estimator for \\(\\beta = (\\beta_0, \\cdots, \\beta_k)'\\).\n\nSolution. Mean independence, \\(\\Er[u_i | z_i] = 0\\) , implies \\(\\Er[u_i f(z_i)] = 0\\) for any function \\(f\\). Since the model has \\(k+1\\) parameters, we should choose at least \\(k+1\\) functions to use. For concreteness, let \\[\nZ_i = \\left(1, \\cos(z_i), \\cos(2z_i), \\cdots \\cos(k z_i) \\right)'\n\\] and use the usual IV estimator, \\[\n\\hat{\\beta} = (Z'X)^{-1}(Z'y)\n\\] where \\(X\\) is defined similarly as \\(Z\\).\nFor this to identify \\(\\beta\\), we must also assume relevance, \\(\\rank(\\Er[Z_i X_i']) = k+1\\)."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#consistency-9-points",
    "href": "problemsets/final2022/final2022solution.html#consistency-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Consistency (9 points)",
    "text": "Consistency (9 points)\nShow that your estimator from part a is consistent.1 State any additional assumptions.\n\nSolution. Substituting the model in for \\(y\\), we have \\[\n\\begin{align*}\n\\hat{\\beta} = & \\beta + (Z'X)^{-1}(Z'u) \\\\\n\\hat{\\beta} = & \\beta + (\\frac{1}{n}Z'X)^{-1}(\\frac{1}{n} Z'u) \\\\\n\\plim \\hat{\\beta} = & \\beta\n\\end{align*}\n\\] where, for the last line, we need an LLN to apply to \\(Z'X\\) and \\(Z'u\\). For this, it is sufficient that data be i.i.d. and \\(\\Er[\\norm{Z_i X_i'}]\\) and \\(\\Er[\\norm{Z_i u_i}]\\) are finite. We also need the relevance condition as in part a."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#asymptotic-distribution-9-points",
    "href": "problemsets/final2022/final2022solution.html#asymptotic-distribution-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Asymptotic Distribution (9 points)",
    "text": "Asymptotic Distribution (9 points)\nFind the asymptotic distribution of your estimator from part a.2 State any additional assumptions.\n\nSolution. As in the previous part, \\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\beta} - \\beta) =  & (\\frac{1}{n}Z'X)^{-1}(\\frac{1}{\\sqrt{n}} Z'u) \\\\\n\\indist & N\\left(0, \\Er[Z_i X_i']^{-1} \\Er[Z_iZ_i'u_i^2] \\Er[X_i Z_i']^{-1} \\right)\n\\end{align*}\n\\] where we again need an LLN for \\(Z'X\\) and the CLT to apply to \\(Z'u\\). A sufficient assumption, along with what was assumed for previous parts, is that \\(\\Er[\\norm{Z_i u_i}^2]\\) is finite."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#convergence-rate-3-points",
    "href": "problemsets/final2022/final2022solution.html#convergence-rate-3-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Convergence Rate (3 points)",
    "text": "Convergence Rate (3 points)\nMore difficult\nSuppose now that \\(k\\) increases with \\(n\\). To simplify, assume that \\(\\Er[u_i | x_i] = 0\\), and \\(x_i \\sim U(-\\pi,\\pi)\\), so that \\[\n\\Er[\\cos(j x_i)\\cos( \\ell x_i)] = \\begin{cases} 0 & \\text{ if } j \\neq \\ell \\\\\n1 & \\text{ if } j = \\ell\n\\end{cases}\n\\]. Let \\[\nX = \\begin{pmatrix} 1 & \\cos(x_i) & \\cdots & \\cos(k x_i) \\\\\n\\vdots & & & \\vdots \\\\\n1 & \\cos(x_n) & \\cdots & \\cos(k x_n)\n\\end{pmatrix}\n\\] Find \\(c(n,k)\\) such that \\(\\norm{\\hat{\\beta}^{OLS} - \\beta} = O_p(c(n,k))\\).\n\nSolution. A technically correct, but not useful answer, would be to give some increasing \\(c(n,k)\\), like \\(c(n,k) = n!k!\\). Then \\(\\norm{\\hat{\\beta} - \\beta}/c(n,k) \\inprob 0\\), so \\(\\norm{\\hat{\\beta} -\\beta} = o_p(c(n,k))\\) which also implies \\(O_p(c(n,k))\\). I guess such an answer is fine.\nMy intention was to ask for the smallest \\(c(n,k)\\) such that \\(\\norm{\\hat{\\beta} -\\beta} = O_p(c(n,k))\\). Let’s do that.\nNote that \\[\n\\begin{aligned}\n\\norm{\\hat{\\beta} - \\beta} = & \\norm{(\\frac{1}{n} X'X)^{-1}\\frac{1}{n}X'u} \\\\\nP\\left( \\norm{\\hat{\\beta} - \\beta} &gt; a \\right) = & P\\left( \\norm{(\\frac{1}{n} X'X)^{-1}\\frac{1}{n}X'u} &gt; a \\right) \\\\\n\\leq & \\frac{1}{a^j} \\Er\\left[\\norm{(\\frac{1}{n} X'X)^{-1}\\frac{1}{n}X'u}^j \\right]\n\\end{aligned}\n\\] where the last line follows from Markov’s inequality.\nUsing the fact that for a square matrix and any vector, \\(\\norm{A x}\n\\leq \\lambda^\\max(A) \\norm{x}\\) where \\(\\lambda^\\max(A)\\) is the maximal eigenvalue of \\(A\\), we have\n\\[\n\\begin{aligned}\nP\\left( \\norm{\\hat{\\beta} - \\beta} &gt; a \\right) \\leq & \\frac{1}{a^j} \\Er\\left[\\lambda^\\max(\\frac{1}{n} X'X)^{-j} \\norm{\\frac{1}{n}X'u}^j \\right] \\\\\n\\leq & \\frac{1}{a^j} \\left(\\Er\\left[\\lambda^\\max(\\frac{1}{n} X'X)^{-2j} \\right] \\Er\\left[\\norm{\\frac{1}{n} X'u}^{2j}\\right] \\right)^{1/2} \\\\\n\\leq & \\frac{1}{a^j} \\left(\\lambda^\\max\\left(\\Er\\left[\\frac{1}{n} X'X \\right]\\right)^{-2j} \\Er[\\norm{\\frac{1}{n} X'u}^{2j}] \\right)^{1/2} \\\\\n\\leq & \\frac{1}{a^j} \\left(\\Er[(\\frac{1}{n^2} u'X X'u)^{j}] \\right)^{1/2} \\\\\n\\leq & \\frac{1}{a} \\sqrt{\\frac{k \\sigma^2}{n}}\n\\end{aligned}\n\\]\nwhere we used the Cauchy-Schwarz inequality, then Jensen’s inequality (\\(\\lambda^\\max(A)^{-1}\\) is concave), and then the fact that \\(\\Er[X'X/n]\n= I_{k+1}\\). Finally, we set \\(j=1\\).\nThus, we get that \\(\\norm{\\hat{\\beta} - \\beta} = O_p(\\sqrt{k/n})\\)"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#ols-is-inconsistent-8-points",
    "href": "problemsets/final2022/final2022solution.html#ols-is-inconsistent-8-points",
    "title": "ECON 626: Final - Solutions",
    "section": "OLS is Inconsistent (8 points)",
    "text": "OLS is Inconsistent (8 points)\nLet \\(\\hat{\\beta}^{OLS} = \\frac{\\sum X_i Y_i}{\\sum X_i^2}\\) be the least squares estimator. Let \\(\\pi = P(X_i^*=1)\\). Compute \\(\\plim \\hat{\\beta}^{OLS}\\) in terms of \\(p\\), \\(\\pi\\), and \\(\\beta\\).\n\nSolution. \\[\n\\begin{aligned}\n\\plim \\hat{\\beta}^{OLS} = & \\plim \\frac{\\sum X_i Y_i}{\\sum X_i^2} \\\\\n= & \\plim \\frac{\\sum X_i (X_i^* \\beta + \\epsilon_i)}{\\sum X_i^2} \\\\\n= & \\frac{\\Er[X_i X_i^*]}{\\Er[X_i^2]} \\beta \\\\\n= & \\frac{\\Er[X_i | X_i^*=1]P(X_i^*=1)}{P(X_i=1)} \\beta \\\\\n= & \\frac{p \\pi}{P(X_i=1|X_i^*=1)P(X_i^*=1) +  P(X_i=1|X_i^*=0)P(X_i^*=0)} \\beta \\\\\n= & \\frac{p \\pi}{p \\pi + (1-p)(1-\\pi)} \\beta\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#instrument-8-points",
    "href": "problemsets/final2022/final2022solution.html#instrument-8-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument? (8 points)",
    "text": "Instrument? (8 points)\nSuppose you also observe \\(Z_i \\in \\{0,1\\}\\) with \\[\nP(Z_i = 1 | X_i^* = 1) = P(Z_i=0 | X_i^*=0) = q\n\\] where \\(q&gt;1/2\\), and \\(Z_i\\) and \\(X_i\\) are independent conditional on \\(X_i^*\\). Let \\(\\hat{\\beta}^{IV} = \\frac{\\sum Z_i Y_i}{\\sum Z_i X_i}\\) be the instrumental variable estimator. Compute \\(\\plim \\hat{\\beta}^{IV}\\)\n\nSolution. \\[\n\\begin{aligned}\n\\plim \\hat{\\beta}^{OLS} = & \\plim \\frac{\\sum Z_i Y_i}{\\sum Z_i X_i} \\\\\n= & \\plim \\frac{\\sum Z_i (X_i^* \\beta + \\epsilon_i)}{\\sum Z_i X_i} \\\\\n= & \\frac{\\Er[Z_i X_i^*]}{\\Er[Z_i X_i]} \\beta \\\\\n= & \\frac{q \\pi}{pq \\pi + (1-p)(1-q)(1-\\pi)} \\beta\n\\end{aligned}\n\\]\nSo with this form of classification error, neither OLS nor IV is consistent."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#or-something-else-9-points",
    "href": "problemsets/final2022/final2022solution.html#or-something-else-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Or Something Else? (9 points)",
    "text": "Or Something Else? (9 points)\nDescribe how \\(X\\), \\(Y\\), and \\(Z\\) could be used to estimate \\(\\beta\\).\nHint: think of \\(\\beta\\), \\(p\\), \\(q\\), and \\(\\pi = P(X^*_i = 1)\\) as four parameters to estimate, and come up with four moment conditions that involve these parameters.\n\nSolution. The idea here is to use GMM. There are many possible combinations of moments to use. From the previous two parts, we know that \\(\\Er[X_i\nY_i] = p\\pi\\beta\\) and \\(\\Er[Z_i Y_i] = q \\pi \\beta\\). We also know that \\(\\Er[X_i] = p \\pi + (1-p)(1-\\pi)\\) and \\(\\Er[Z_i] = q \\pi + (1-q)(1-\\pi)\\). Hence, we can estimate \\(\\beta\\) by solving \\[\n\\begin{aligned}\n\\En[X_i Y_i] & = \\hat{p}\\hat{\\pi} \\hat{\\beta} \\\\\n\\En[Z_i Y_i] & = \\hat{q}\\hat{\\pi} \\hat{\\beta} \\\\\n\\En[X_i] & = \\hat{p} \\hat{\\pi} + (1-\\hat{p})(1-\\hat{\\pi}) \\\\\n\\En[Z_i] & = \\hat{q} \\hat{\\pi} + (1-\\hat{q})(1-\\hat{\\pi})\n\\end{aligned}\n\\] It’s not clear whether this system of equations has a unique solution. In general, it might not. The restrictions that \\(p,q \\in\n[1/2,1]\\) and \\(\\pi \\in (0,1)\\) might help pin down the correct solution.\nYour answer need not have gone any further, but to ensure identification, note that \\(\\Er[X_i Z_i Y_i] = p q \\pi \\beta\\), so we can identify \\(p\\) (and \\(q\\)) by \\(\\Er[XZY] / \\Er[ZY]\\). Given \\(p\\), \\(\\pi\\) can be identified from \\(\\Er[X_i]\\). Given \\(p\\) and \\(\\pi\\), we can recover \\(\\beta\\) from \\(\\Er[XY]\\). This constructive identification argument can be turned into an estimator by replacing expecations with sample averages."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#footnotes",
    "href": "problemsets/final2022/final2022solution.html#footnotes",
    "title": "ECON 626: Final - Solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you could not solve part a, suppose your estimator was \\(\\hat{\\beta} = (W'X)^{-1}\n(W'y)\\) for some \\(n \\times k+1\\) matrix of variables of \\(W\\).↩︎\nIf you could not solve part a, suppose your estimator was \\(\\hat{\\beta} = (W'X)^{-1}\n(W'y)\\) for some \\(n \\times (k+1)\\) matrix of variables, \\(W\\).↩︎"
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html",
    "href": "problemsets/final2024/final2024_solution.html",
    "title": "ECON 626: Final",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\]\nYou have 150 minutes to complete the exam. The last two pages have some possibly useful formulas.\nThere are 100 total points. Each part of a question is worth either 6 or 7 points. Some questions are harder than others. Be careful not to get stuck on one question for too long. To help with budgeting your time, I’ve labeled two questions as More difficult and two as Most difficult."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#identifying-assumption-7-points",
    "href": "problemsets/final2024/final2024_solution.html#identifying-assumption-7-points",
    "title": "ECON 626: Final",
    "section": "Identifying Assumption (7 points)",
    "text": "Identifying Assumption (7 points)\nIn columns (1) and (2), which omit \\(\\beta Near_i \\times Auto_{ct}\\), what assumption(s) must be made for the estimated coefficient on \\(Auto_{ct}\\) to be a consistent estimate of the average treatment effect on the treated of the automated monitoring program?\n\nSolution. This difference in difference estimation is motivated by a (conditional) parallel trends assumption, i.e. if \\(y_{ict} = \\ln(PM2.5_{ict})\\) with potential outcomes \\(y_{ict}(auto)\\), then the parallel trends assumption is \\[\n\\Er[y_{ict}(0) - y_{ict-\\ell}(0) | Auto_{ct}=1, Auto_{ct-\\ell} = 0, X_{ct}, X_{ct-\\ell}] =\n\\Er[y_{ict}(0) - y_{ict-\\ell}(0) | Auto_{ct}=0, Auto_{ct-\\ell} = 0, X_{ct}, X_{ct-\\ell}]\n\\] However, the estimated linear model imposes some additional assumptions. Let \\(\\tilde{y}_{ict}(auto)\\) be the residuals from the population regression of \\(y_{ict}(auto)\\) on \\(X_{ct}\\). If all cities were treated in the same period or treatment effects do not vary with time, then \\(\\alpha\\) is an estimate of the ATT if we have parallel trends for the residualized outcomes. \\[\n\\Er[\\tilde{y}_{ict}(0) - \\tilde{y}_{ict-\\ell}(0) | Auto_{ct}=1, Auto_{ct-\\ell} = 0 ] =\n\\Er[\\tilde{y}_{ict}(0) - \\tilde{y}_{ict-\\ell}(0) | Auto_{ct}=0, Auto_{ct-\\ell} = 0]\n\\]\nA technically correct, but not the best answer, would be that \\(\\Er[Auto_{ct} \\epsilon_{ict}] = 0\\) and \\(\\Er[X_{ct}\\epsilon_{ict}] = 0\\).\nAny answer here that correctly describe parallel trends should receive 5 points. For full credit the answer should also recognize that something more is needed."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#dependence-7-points",
    "href": "problemsets/final2024/final2024_solution.html#dependence-7-points",
    "title": "ECON 626: Final",
    "section": "Dependence (7 points)",
    "text": "Dependence (7 points)\nThe sample here consists of areas / “cells” \\(i\\) within cities \\(c\\) in years \\(t\\). How does this affect the consistency and asymptotic distribution of the estimates. What in the table, if anything, needs to be calculated differently than if observations were independent?\n\nSolution. Standard errors should allow for spatial and/or time dependence. If we believe that observations from different cities are uncorrelated, then standard errors clustered on city level are sufficient. HAC standard errors could also be used."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#twfe-7-points",
    "href": "problemsets/final2024/final2024_solution.html#twfe-7-points",
    "title": "ECON 626: Final",
    "section": "TWFE (7 points)",
    "text": "TWFE (7 points)\nThe pollution monitoring program was implemented in three waves — some cities became treated earlier than others. Briefly describe a problem with Equation 1 and describe what should have been done instead.1\n\nSolution. The fixed effects estimator is a weighted average, with possible negative weights, of time specific treatment effects. This can be problematic if the treatment effect varies with time. Instead of fixed effects, an estimator that allows for different effects for different time periods and cohorts should be used."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#linear-control-12-points-6-for-each-part",
    "href": "problemsets/final2024/final2024_solution.html#linear-control-12-points-6-for-each-part",
    "title": "ECON 626: Final",
    "section": "Linear Control (12 points, 6 for each part)",
    "text": "Linear Control (12 points, 6 for each part)\nAs noted on the table, columns (3), (5), and (6) include a ``concurrent policy’’ control. This is a concurrent policy also aimed at pollution control. This question analyzes whether it is sufficient to additively control for this concurrent policy. To simplify, denote the outcome by \\(y_{ct}\\), suppose the only treatment is \\(Auto_{ct}\\) and denote it by \\(D_{ct}\\), and let \\(X_{ct} \\in \\{0,1\\}\\) be the concurrent policy. Also assume the treatment only begins at one time. Assume conditional parallel trends, \\[\n\\Er[Y_{ct}(0) - Y_{c t-\\ell}(0) | X_{ct}, X_{ct-\\ell}, D_{ct} = 1, D_{c,t-\\ell}=0] = \\Er[Y_{ct}(0) - Y_{c t-\\ell}(0) | X_{ct}, X_{c,t-\\ell}, D_{ct} = 0, D_{ct-\\ell} = 0].\n\\]\n\nDo you think the regression, \\[\ny_{ct} = \\alpha D_{ct} + \\gamma X_{ct} + \\lambda_c + \\delta_t + \\epsilon_{ct},\n\\] is likely to produce a consistent estimate of an interpretable treatment effect? Why or why not? (Your answer need not be completely rigorous; you can argue by analogy, e.g. “this is similar to XX in that YY, so it should be [not] okay.”)\nDescribe a consistent estimator for the average treatment effect on the treated.\n\n\nSolution. \n\nNo, this regression is not guaranteed to produce a consistent estimate of an interpretable treatment effect. As with TWFE, we can write the estimate as \\[\n\\hat{\\alpha} = \\sum y_{ct}(0) \\hat{\\omega}_{ct} + \\sum (y_{ct}(1) - y_{ct}(0)) \\hat{\\omega}_{ct}\n\\] where \\[\n\\hat{\\omega}_{ct} = \\frac{\\tilde{D}_{ct}}{\\sum \\tilde{D}_{ct}^2}\n\\] and \\(\\tilde{D}_{ct}\\) are the residuals from regressing \\(D\\) on \\(X\\) and fixed effects. As with just TWFE, some \\(\\tilde{D}_{ct}\\) could be negative. Moreover, even if none are negative, \\(\\hat{\\alpha}\\) is a difficult to interpret weighted average of treatment effects.\nA plug-in estimator will be a consistent estimator for the ATT. Conditional parallel trends implies that \\[\nATT = \\Er_X\\left[ \\Er[y_{ct}-y_{ct-\\ell} | X_{ct},X_{ct-\\ell},D_{ct}=1,D_{ct-\\ell}=0] - \\Er[y_{ct}-y_{ct-\\ell} | X_{ct},X_{ct-\\ell},D_{ct}=0,D_{ct-\\ell}=0] \\right].\n\\] Replacing the above population expectations with sample averages will give a consistent estimator.\n\nAlternatively, a “saturated” regression that includes an interaction between \\(D\\) and \\(X\\) will also give a consistent estimate."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#consistency-7-points",
    "href": "problemsets/final2024/final2024_solution.html#consistency-7-points",
    "title": "ECON 626: Final",
    "section": "Consistency (7 points)",
    "text": "Consistency (7 points)\n(More difficult) Assume that the probit model is correctly specified so that \\(NTV_s = \\Phi(\\gamma signal_s)\\), and that \\(|\\gamma - \\hat{\\gamma}| = O_p(n^{-1/2})\\). Show that the OLS estimate of Equation 2 using \\(\\widehat{NTV}_s\\) in place of \\(NTV_s\\) is consistent as the number of regions, \\(S\\), and number of survey respondents, \\(n\\), both approach infinity. Clearly state any extra assumptions you make.\n(Hint: to reduce notation, you may assume that \\(NTV\\) and \\(\\widehat{NTV}\\) have mean zero. It suffices to show that \\(\\frac{1}{S} \\sum_s \\widehat{NTV}_s NTV_s \\inprob \\Er[NTV_s^2]\\), \\(\\frac{1}{S} \\sum_s \\widehat{NTV}_s \\epsilon_s \\inprob \\Er[NTV_s \\epsilon_s]\\), and \\(\\frac{1}{S} \\sum_s \\widehat{NTV}_s^2 \\inprob \\Er[NTV_s^2]\\). Note that using the intermediate value theorem and the fact that the derivative of \\(\\Phi\\) is bounded by \\(1/\\sqrt{2\\pi}\\), we have \\(|NTV_s - \\widehat{NTV}_s| \\leq 1/\\sqrt{2\\pi} |signal_s(\\gamma - \\hat{\\gamma})|\\). The Cauchy-Schwarz inequality might be useful.).\n\nSolution. The estimator is \\[\n\\hat{\\beta}_1 = \\frac{\\sum \\widehat{NTV}_s  v_s}{\\sum \\widehat{NTV}_s^2}\n\\] Substituting in the model for \\(v_s\\), we have \\[\n\\hat{\\beta}_1 = \\beta_1 \\frac{\\sum \\widehat{NTV}_s NTV_s}{\\sum \\widehat{NTV}_s^2} + \\frac{\\sum \\widehat{NTV}_s \\epsilon_s}{\\sum \\widehat{NTV}_s^2}\n\\] First, we show that the last term with \\(\\epsilon_s\\) converges to 0. Note that \\[\n\\begin{align*}\n\\frac{1}{S} \\sum \\widehat{NTV}_s \\epsilon_s\n& = \\frac{1}{S} \\sum NTV_s \\epsilon_s + \\frac{1}{S}\\sum (\\hat{NTV}_s - NTV_s)\\epsilon_s \\\\\n\\left\\vert\\frac{1}{S} \\sum \\widehat{NTV}_s \\epsilon_s\\right\\vert & \\leq \\left\\vert \\frac{1}{S} \\sum NTV_s \\epsilon_s \\right\\vert + \\sqrt{ \\left(\\frac{1}{S} \\sum 1/(2\\pi) signal_s^2 (\\gamma - \\hat{\\gamma})^2  \\right)\n\\left( \\frac{1}{S} \\sum \\epsilon_s^2 \\right) }\n& \\inprob 0\n\\end{align*}\n\\] where we used the hint and the Cauchy-Schwarz inequality in the second line, and the third line follows from assuming LLNs apply to \\(\\frac{1}{S} \\sum NTV_s \\epsilon_s\\), \\(\\frac{1}{S}\\sum signal_s^2\\), and \\(\\frac{1}{S} \\sum \\epsilon_s^2\\), and the assumption that \\(\\hat{\\gamma} \\inprob \\gamma\\).\nAn identical argument would show that \\(\\frac{1}{S} \\sum \\widehat{NTV}_s NTV_s \\inprob \\Er[NTV_s^2]\\).\nFinally, \\[\n\\begin{align*}\n\\frac{1}{S} \\sum \\widehat{NTV}_s^2 = & \\frac{1}{S} \\sum NTV_s^2 + \\frac{1}{S} \\sum (\\widehat{NTV}_s^2 - NTV_s^2) \\\\\n= & \\frac{1}{S} \\sum NTV_s^2 + \\frac{1}{S} \\sum \\underbrace{2\\Phi(\\overline{\\gamma signal_s}) \\phi(\\overline{\\gamma signal_s})}_{\\leq 2/\\sqrt{2\\pi}} (\\hat{\\gamma} - \\gamma) \\\\\n\\inprob & \\Er[NTV_s^2]\n\\end{align*}\n\\] where the second line used the intermediate value theorem and the last line used the assumption that \\(\\hat{\\gamma} \\inprob \\gamma\\) and an LLN applies to \\(\\frac{1}{S} \\sum NTV_s^2\\)."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#estimation-of-gamma-7-points",
    "href": "problemsets/final2024/final2024_solution.html#estimation-of-gamma-7-points",
    "title": "ECON 626: Final",
    "section": "Estimation of \\(\\gamma\\) (7 points)",
    "text": "Estimation of \\(\\gamma\\) (7 points)\nExplain how to use GMM to estimate \\(\\gamma\\). State the moment condition(s) and the objective function. (Hint: the assumptions above can be re-written as \\(\\Er[NTV_i - \\Phi(\\gamma signal_i) | signal_i] = 0\\)).\n\nSolution. The conditional moment restriction \\(\\Er[NTV_i - \\Phi(\\gamma signal_i) | signal_i] = 0\\) implies that \\(\\Er[\\left(NTV_i - \\Phi(\\gamma signal_i)\\right)signal_i] = 0\\). This gives one moment condition to estimate one parameter. We can estimate \\(\\gamma\\) by minimizing the norm of the empirical moment, \\[\n\\hat{\\gamma} \\in \\argmin_\\gamma \\left( \\frac{1}{n} \\sum_i (NTV_i - \\Phi(\\gamma signal_i))signal_i \\right)^2.\n\\]"
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#asymptotic-normality-12-points-6-for-each-part",
    "href": "problemsets/final2024/final2024_solution.html#asymptotic-normality-12-points-6-for-each-part",
    "title": "ECON 626: Final",
    "section": "Asymptotic Normality (12 points, 6 for each part)",
    "text": "Asymptotic Normality (12 points, 6 for each part)\n\n(More difficult) Suppose \\(n &gt;&gt; S\\), so that it is sensible to consider asymptotics where \\(n \\to \\infty\\), \\(S \\to \\infty\\), and \\(S/n \\to 0\\). Assume that \\(|signal_s|\\) is bounded. Show that the usual OLS standard errors are correct when estimating Equation 2 with \\(\\widehat{NTV}_s\\) in place of \\(NTV_s\\). (Hint: Derive the asymptotic distribution \\(\\sqrt{S}(\\hat{\\beta}_1 - \\beta_1)\\). The fact that \\(\\max_{1\\leq s \\leq S} |NTV_s - \\widehat{NTV}_s| \\leq 1/\\sqrt{2\\pi} (\\max_s |signal_s| ) |\\gamma - \\hat{\\gamma}|\\) might be useful.).\n(Most difficult) Now suppose \\(n\\) is close to \\(S\\), and consider asymptotics where \\(n \\to \\infty\\), \\(S \\to \\infty\\), and \\(S/n \\to c\\) for some constant \\(c &gt; 0\\). Derive the asymptotic distribution of \\(\\hat{\\beta}_1\\) when estimating Equation 2 with \\(\\widehat{NTV}_s\\) in place of \\(NTV_s\\).\n\n\nSolution. \n\nNote that \\[\n\\sqrt{S}(\\hat{\\beta}_1 - \\beta_1) = \\frac{\\frac{1}{\\sqrt{S}} \\sum_s \\widehat{NTV}_s\\left((NTV_s - \\widehat{NTV}_s) + \\epsilon_s\\right)}\n{ \\frac{1}{S} \\sum_s \\widehat{NTV}_s^2}\n\\] as in the consistency section, \\(\\frac{1}{S} \\sum_s \\widehat{NTV}_s^2 \\inprob \\Er[NTV_s^2]\\). Also, similar to that section, \\[\n\\frac{1}{\\sqrt{S}} \\sum_s \\widehat{NTV}_s \\epsilon_s = \\frac{1}{\\sqrt{S}} \\sum_s NTV_s \\epsilon_s + \\frac{1}{\\sqrt{S}} \\sum_s (\\widehat{NTV}_s - NTV_s) \\epsilon_s\n\\] and \\[\n\\begin{align*}\n\\left\\vert \\frac{1}{\\sqrt{S}} \\sum_s (\\widehat{NTV}_s - NTV_s) \\epsilon_s \\right\\vert \\leq &\n\\left\\vert \\frac{1}{\\sqrt{S}} \\sum_s |signal_s| \\epsilon_s 1/(2\\pi) |\\hat{\\gamma} - \\gamma| \\right\\vert \\\\\n\\inprob & 0\n\\end{align*}\n\\] where we used the intermediate value theorem on the first line, and then assumed that a CLT applies to \\(\\frac{1}{\\sqrt{S}} \\sum_s |signal_s|\\epsilon_s\\), and the fact that \\(\\hat{\\gamma} \\inprob \\gamma\\) to get the second line.\n\nNext, note that\n\\[\n\\begin{align*}\n\\left\\vert \\frac{1}{\\sqrt{S}} \\sum_s \\widehat{NTV}_s\\left((NTV_s - \\widehat{NTV}_s) \\right) \\right\\vert\n& \\leq \\left\\vert \\frac{1}{\\sqrt{S}} \\sum_s 1/\\sqrt{2\\pi} |signal_s(\\gamma - \\hat{\\gamma}|\n\\right\\vert \\\\\n& \\leq \\sqrt{S} B |\\gamma - \\hat{\\gamma}| \\\\\n& \\leq O_p\\left(\\sqrt{S/n}\\right) \\inprob 0\n\\end{align*}\n\\]\nwhere we used the fact that \\(|\\widehat{NTV}| &lt; 1\\) and the assumption that \\(|signal_s|\\) is bounded.\nFinally, we can conclude that \\[\n\\sqrt{S}(\\hat{\\beta}_1 - \\beta_1) = \\frac{\\frac{1}{\\sqrt{S}} \\sum_s NTV_s \\epsilon_s}{\\frac{1}{S} \\sum_s NTV_s^2} + o_p(1) \\indist N(0,V),\n\\] the same asymptotic distribution as if \\(NTV_s\\) were observed.\n\nI revised the question to not have this part, but the idea is that under this assumption, \\[\n\\frac{1}{\\sqrt{S}} \\sum_s \\widehat{NTV}_s\\left((NTV_s - \\widehat{NTV}_s) \\right)\n\\] will no longer be \\(o_p(1)\\) and instead will contribute another term to the asymptotic distribution, which reflects estimation error in \\(\\hat{\\gamma}\\)."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#endogeneity-7-points",
    "href": "problemsets/final2024/final2024_solution.html#endogeneity-7-points",
    "title": "ECON 626: Final",
    "section": "Endogeneity (7 points)",
    "text": "Endogeneity (7 points)\nGive one reason why \\(\\Er[Telehealth_{iht} \\epsilon_{iht}]\\) might not be zero, speculate on the sign of \\(\\Er[Telehealth_{iht} \\epsilon_{iht}]\\), and say whether whether you expect \\(\\hat{\\beta}_1^{OLS}\\) to be greater or less than \\(\\beta_1\\).\n\nSolution. Patients who choose telehealth instead of an in person visit are likely to have less severe problems, so I think \\(\\Er[Telehealth_{iht} \\epsilon_{iht}] &lt; 0\\). \\(\\plim \\hat{\\beta}_1 = \\beta_1 + \\frac{\\Er[\\epsilon \\widetilde{Telehealth}_{iht}]}{\\Er[\\widetilde{Telehealth}_{iht}]}\\), so I expect \\(\\hat{\\beta}_1\\) to be less than \\(\\beta_1\\)."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#instrument-7-points",
    "href": "problemsets/final2024/final2024_solution.html#instrument-7-points",
    "title": "ECON 626: Final",
    "section": "Instrument (7 points)",
    "text": "Instrument (7 points)\nAs an instrument for \\(Telehealth_{iht}\\) the authors use the distance from patient \\(i\\) to provider \\(h\\) minus the distance from patient \\(i\\) to the closest provider. What assumptions must this instrument satisfy for the IV estimator to be consistent?\n\nSolution. The instrument must be relevant \\(\\Er[Telehealth_{iht} Z_{iht}] \\neq 0\\) and exogenous, \\(\\Er[Z_{iht} \\epsilon_{iht}] = 0\\). It and other variables must also satisfy enough regularity conditions for the LLN to apply (it is okay not to mention this)."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#instrument-strength-7-points",
    "href": "problemsets/final2024/final2024_solution.html#instrument-strength-7-points",
    "title": "ECON 626: Final",
    "section": "Instrument Strength (7 points)",
    "text": "Instrument Strength (7 points)\nThe paper does not report the first stage, but the authors do write\n\nThe Kleibergen-Paap rank Wald F statistic is 74.109, which is above the 10% maximal Stock-Yogo critical value, suggesting that the maximum bias in our IV approach can be at most 10% of the bias in an OLS approach. Hence, we conclude that our IVs are not weak and meet the necessary criteria for selection.\n\nThe authors then proceed with reporting standard IV standard errors and confidence intervals. What assumption are the authors checking? Do you agree with their conclusion? What, if anything, would you do differently?\n\nSolution. The authors are checking for instrument relevance. Recent research argues that the Stock-Yogo critical value is not stringent enough and \\(F&gt;100\\) is needed. Their F statistic is close, but to be safe, I would use an identification robust inference method to calculate confidence intervals."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#dependence-7-points-1",
    "href": "problemsets/final2024/final2024_solution.html#dependence-7-points-1",
    "title": "ECON 626: Final",
    "section": "Dependence (7 points)",
    "text": "Dependence (7 points)\nTable 2 below shows the IV estimates from Ayabakan, Bardhan, and Zheng (2024). Given the structure of the data (it has repeated observations from the same patients, hospitals, and quarters), what is an appropriate assumption to make about the dependence of \\(\\epsilon_{iht}\\)? Briefly describe how standard errors could be calculated with this assumption.\n\nSolution. The standard errors should be clustered to allow for dependence. It seems sensible here to do two way clustering on patient and hospital."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#late-7-points",
    "href": "problemsets/final2024/final2024_solution.html#late-7-points",
    "title": "ECON 626: Final",
    "section": "LATE? (7 points)",
    "text": "LATE? (7 points)\nAyabakan, Bardhan, and Zheng (2024) do not use standard 2SLS. Their reasoning is:\n\nWhen the endogenous variable is binary, a standard 2SLS approach may yield inconsistent estimates (referred to as forbidden regression) because the fitted values of a binary variable from the first stage estimation will not be binary, nor will they be probabilistic in the second stage (Wooldridge 2010)\n\nThere is a grain of truth in what they say, but also some nonsense. What is a problem with 2SLS if the effect of telemedicine is heterogeneous?\n\nSolution. If the effect is heterogenous, then a linear IV model is misspecified. If there were no controls and we believed that the instrument has a monotonic effect on telehealth, then the 2SLS would have a LATE intrepretation — it would be the average treatment effect for compliers. However, with linear controls, the LATE interpretation breaks down, and 2SLS no longer captures any interpretable treatment effect."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#nonstandard-3sls-7-points",
    "href": "problemsets/final2024/final2024_solution.html#nonstandard-3sls-7-points",
    "title": "ECON 626: Final",
    "section": "Nonstandard 3SLS (7 points)",
    "text": "Nonstandard 3SLS (7 points)\n(Most difficult) Instead of 2SLS, Ayabakan, Bardhan, and Zheng (2024) use a three step procedure that they describe as follows:\n\nTherefore, we adopt a three-step approach where we first estimated a Probit model and regressed Telehealth on our instrumental variable and other controls, including patient, hospital, and time fixed effects (Angrist and Pischke 2008). In the second step, we calculate the predicted values of Telehealth from the first stage, which we denote as Telehealth_hat. In the third step, we follow a regular 2SLS estimation approach with Telehealth_hat being the only IV for the endogenous Telehealth variable. This estimation produces consistent and unbiased estimates (Angrist and Pischke 2008).\n\nConsider a simplified setting without control variables or fixed effects. Then the three-step estimator described above becomes:\n\nEstimate a probit model for \\(Telehealth_{iht}\\) \\[\n\\hat{\\gamma} = \\argmax_\\gamma \\sum_{i,h,t} Telehealth_{iht} \\log \\Phi(\\gamma_0 + \\gamma_1 Z_{iht}) + (1-Telehealth_{iht}) \\log (1-\\Phi(\\gamma_0 + \\gamma_1 Z_{iht}))\n\\]\nFirst stage of 2SLS with instrument \\(\\widehat{Telehealth}_{iht} = \\Phi(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 Z_{iht} )\\)\nSecond stage gives \\(\\hat{\\beta}_1 = \\frac{\\sum \\widehat{Telehealth}_{iht} (Util_{iht} - \\overline{Util})}{\\sum \\widehat{Telehealth}_{iht} (Telehealth_{iht} - \\overline{Telehealth})}\\)\n\nShow that this procedure is consistent. State any additional assumptions needed. For the first step, it is okay to just make a high-level assumption like \\(\\plim \\hat{\\gamma}\\) exists.\n\nSolution. To reduce notation, let \\(x_j = Telehealth_{iht}\\), \\(y_j = Util_{iht}\\), and \\(z_j = (1,Z_{iht})'\\). Also assume both \\(x\\) and \\(y\\) have mean 0, so the model is simply \\[\ny = x\\beta + \\epsilon\n\\] The estimator can be written \\[\n\\begin{align*}\n\\hat{\\beta} = & \\frac{\\sum \\Phi(z_j'\\hat{\\gamma}) y_j}{\\sum \\Phi(z_j'\\hat{\\gamma}) x_j} \\\\\n= & \\beta + \\frac{\\sum \\Phi(z_j'\\hat{\\gamma}) \\epsilon_j}{\\sum \\Phi(z_j'\\hat{\\gamma}) x_j}\n\\end{align*}\n\\] Assume that \\(\\plim \\hat{\\gamma} = \\gamma\\) exists. Write the numerator of \\(\\hat{\\beta} - \\beta\\) as \\[\n\\frac{1}{n} \\sum \\Phi(z_j'\\hat{\\gamma}) \\epsilon_j = \\frac{1}{n} \\sum \\Phi(z_j'\\gamma) \\epsilon_j +\n\\frac{1}{n} \\sum \\left(\\Phi(z_j'\\hat{\\gamma})-\\Phi(z_j'\\gamma)\\right) \\epsilon_j\n\\] The intermediate value theorem and the fact that the derivative of \\(\\Phi\\) is bounded imply that \\[\n\\left\\vert \\frac{1}{n} \\sum \\left(\\Phi(z_j'\\hat{\\gamma})-\\Phi(z_j'\\gamma)\\right) \\epsilon_j \\right\\vert \\leq \\Vert \\hat{\\gamma} - \\gamma \\Vert \\frac{B}{n} \\sum \\Vert z_j \\epsilon_j \\Vert\n\\] If we assume that an LLN applies to \\(\\frac{1}{n} \\sum \\Vert z_j \\epsilon_j \\Vert  \\inprob \\Er[\\Vert z\\epsilon \\Vert]\\), then \\(\\frac{1}{n} \\sum \\left(\\Phi(z_j'\\hat{\\gamma})-\\Phi(z_j'\\gamma)\\right) \\epsilon_j\\inprob 0\\).\nAn identical argument shows that \\[\n\\frac{1}{n} \\sum \\Phi(z_j'\\hat{\\gamma}) x_j = \\frac{1}{n} \\sum \\Phi(z_j'\\gamma) x_j + o_p(1)\n\\]\nAssume that the LLN applies so \\(\\frac{1}{n} \\sum \\Phi(z_j'\\gamma) \\epsilon_j \\inprob 0\\) and \\(\\frac{1}{n} \\sum \\Phi(z_j'\\gamma) x_j \\inprob \\Er[\\Phi(z_j'\\gamma)x_j] \\neq 0\\). Conclude that \\(\\hat{\\beta} \\inprob \\beta\\)."
  },
  {
    "objectID": "problemsets/final2024/final2024_solution.html#footnotes",
    "href": "problemsets/final2024/final2024_solution.html#footnotes",
    "title": "ECON 626: Final",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe appendix of Yang et al. (2024) does include an alternative appropriate estimator.↩︎"
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html",
    "href": "problemsets/midterm2023/midterm2023_solution.html",
    "title": "Midterm Solutions 2023",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\nSuppose \\(u_{i,k} \\in \\{0,1\\}\\) for \\(k=1,...,K\\), and \\(Y_i = \\sum_{k=1}^K u_{i,k}\\). Observations for different \\(i\\) are independent and identically distributed. \\(Y_1,\n..., Y_n\\) are observed, but \\(u_{i,k}\\) are not. For the same \\(i\\) and \\(j\n\\neq k\\), \\(u_{ij}\\) and \\(u_{ik}\\) are independent (but may not be identically distributed).\n\n\nFor \\(K=2\\), what is \\(\\sigma(Y)\\)?\n\nSolution. The range of \\(Y\\) is \\(\\{0,1,2\\}\\). \\(\\sigma(Y)\\) will consist of the preimages of each of these values and all their intersections and unions. That is, \\[\n\\sigma(Y) = \\begin{Bmatrix} \\emptyset, \\, \\{(0,0)\\}, \\, \\{(1,1)\\}, \\, \\{(0,1), (1,0)\\}, \\\\\n\\{(0,0), (0,1), (1,0) \\}, \\, \\{(0,1),(1,0),(1,1)\\} \\\\\n\\{(0,0), (1,1) \\}, \\, \\{(0,0), (0,1), (1,0 ), (1,1)\\} \\end{Bmatrix}\n\\]\n\n\n\n\n\nLet \\(\\theta_k = P(u_{i,k}=1)\\) show that \\(\\theta = (\\theta_1, ..., \\theta_K)\\) is not identified by finding an observationally equivalent \\(\\tilde{\\theta}\\).\n\n\nSolution. Since addition is commutative, \\(\\theta\\) is observationally equivalent to any permutation of its values. For example, with \\(K=2\\), \\(\\theta=(\\theta_1,\\theta_2)\\) is observationally equivalent to \\(\\tilde{\\theta}=(\\theta_2, \\theta_1)\\) because \\[\n\\begin{align*}\nP_\\theta(Y=0) = & (1-\\theta_1)(1-\\theta_2) = P_{\\tilde{\\theta}}(Y=0) \\\\\nP_\\theta(Y=1) = & theta_1(1-\\theta_2) + \\theta_2(1-\\theta_1) = P_{\\tilde{\\theta}}(Y=1) \\\\\nP_\\theta(Y=2) = & \\theta_1\\theta_2 = P_{\\tilde{\\theta}}(Y=2).\n\\end{align*}\n\\]\n\n\nShow that if \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_K = \\vartheta\\), then \\(\\vartheta\\) is identified.\n\n\nSolution. In this case, the expectation of \\(Y\\) is \\(\\Er[Y] =  K \\vartheta\\), so \\(\\vartheta\\) is identified by \\(\\Er[Y]/K\\).\n\n\n\n\n\nAssuming \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_K = \\vartheta\\), find the maximum likelihood estimator1 for \\(\\vartheta\\), and show whether it is unbiased.\n\n\nSolution. The density of \\(Y\\) with respect to a uniform discrete measure on \\(\\{0,\n..., K\\}\\) is \\[\nf(Y_i;\\theta) = \\binom{K}{Y_i} \\theta^{Y_i} (1-\\theta)^{K-Y_i}\n\\] The loglikelihood is then \\[\n\\log \\ell(\\theta;Y) = \\sum_{i=1}^n \\left(\\log \\binom{K}{Y_i} + Y_i \\log \\theta + (K-Y_i) \\log(1-\\theta)\\right)\n\\] Solving the first order condition for \\(\\hat{\\theta}\\) gives: \\[\n\\begin{align*}\n0 = & \\sum \\frac{Y_i}{\\hat{\\theta}} - \\frac{K-Y_i}{1-\\hat{\\theta}} \\\\\n\\hat{\\theta} = \\frac{1}{nK} \\sum_{i=1}^n Y_i\n\\end{align*}\n\\]\nThis estimator is unbiased. \\[\n\\Er[\\hat{\\theta}] = \\frac{1}{nK} \\sum_{i=1}^n \\Er[Y_i] = \\vartheta.\n\\]\n\n\nShow whether or not the maximum likelihood estimator is consistent.\n\n\nSolution. Note that \\(\\Er[|Y_i|] = \\Er[Y_i] = K\\vartheta\\) is finite, and \\(Y_i\\) is iid. Therefore, by the law of large numbers, \\[\n\\frac{1}{n} \\sum_{i=1}^n Y_i \\inprob \\Er[Y_i] = K \\vartheta\n\\] so \\[\n\\frac{1}{nK} \\sum_{i=1}^n Y_i \\inprob \\vartheta.\n\\]\n\n\n\n\nFor this part, still assume \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_K = \\vartheta\\).\n\nFor find the most powerful test for testing \\(H_0: \\vartheta =\n\\vartheta_0\\) against \\(H_1: \\vartheta = \\vartheta_a\\) where \\(\\vartheta_a &gt; \\vartheta_0\\).\n\n\nSolution. By the Neyman-Pearson Lemma, the likelihood ratio is most powerful. Let’s describe the critical region for this test. The likelihood ratio is \\[\n\\begin{align*}\nlr(\\vartheta_a,\\vartheta_0;Y)  = & \\frac{\\prod_i \\vartheta_a^{Y_i}(1-\\vartheta_a)^{K-Y_i}}{ \\prod_i \\vartheta_0^{Y_i}(1-\\vartheta_0)^{K-Y_i}} \\\\\n= & \\frac{\\vartheta_a^{\\sum Y_i}(1-\\vartheta_a)^{nK - \\sum Y_i}} {\\vartheta_0^{\\sum Y_i}(1-\\vartheta_0)^{nK - \\sum Y_i}} \\\\\n= & \\left(\\frac{\\vartheta_a}{\\vartheta_0} \\right)^{\\sum Y_i} \\left( \\frac{1-\\vartheta_a}{1-\\vartheta_0} \\right)^{nK - \\sum Y_i}\n\\end{align*}\n\\] When \\(\\vartheta_a &gt; \\vartheta_0\\), so that \\(\\frac{\\vartheta_a}{\\vartheta_0}&gt;0\\) and \\(\\frac{1-\\vartheta_a}{1-\\vartheta_0} &lt; 1\\), the likelihood ratio is an increasing function of \\(\\sum Y_i\\) and does not depend on the data in any other way. The critical region for a test of size \\(\\alpha\\) will be \\[\nC = \\{Y_i : \\sum Y_i &gt; c^*(\\alpha,\\theta_0) \\}\n\\] where \\(P_{\\theta_0}(\\sum Y_i &gt; c^*(\\theta_0)) = \\alpha\\).\n\n\nIs this test also most powerful against the alternative \\(H_1: \\vartheta &gt;\\ vartheta_0\\)? (Hint: does the critical region depend on \\(\\vartheta_a\\)?\n\n\nSolution. In the previous part, we saw that the critical region is the same for any \\(\\vartheta_a &gt; \\vartheta_0\\). Hence, the test is uniformly most powerful again \\(H_1: \\vartheta &gt; \\vartheta_0\\)."
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#sigma-fields",
    "href": "problemsets/midterm2023/midterm2023_solution.html#sigma-fields",
    "title": "Midterm Solutions 2023",
    "section": "",
    "text": "For \\(K=2\\), what is \\(\\sigma(Y)\\)?\n\nSolution. The range of \\(Y\\) is \\(\\{0,1,2\\}\\). \\(\\sigma(Y)\\) will consist of the preimages of each of these values and all their intersections and unions. That is, \\[\n\\sigma(Y) = \\begin{Bmatrix} \\emptyset, \\, \\{(0,0)\\}, \\, \\{(1,1)\\}, \\, \\{(0,1), (1,0)\\}, \\\\\n\\{(0,0), (0,1), (1,0) \\}, \\, \\{(0,1),(1,0),(1,1)\\} \\\\\n\\{(0,0), (1,1) \\}, \\, \\{(0,0), (0,1), (1,0 ), (1,1)\\} \\end{Bmatrix}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#identification",
    "href": "problemsets/midterm2023/midterm2023_solution.html#identification",
    "title": "Midterm Solutions 2023",
    "section": "",
    "text": "Let \\(\\theta_k = P(u_{i,k}=1)\\) show that \\(\\theta = (\\theta_1, ..., \\theta_K)\\) is not identified by finding an observationally equivalent \\(\\tilde{\\theta}\\).\n\n\nSolution. Since addition is commutative, \\(\\theta\\) is observationally equivalent to any permutation of its values. For example, with \\(K=2\\), \\(\\theta=(\\theta_1,\\theta_2)\\) is observationally equivalent to \\(\\tilde{\\theta}=(\\theta_2, \\theta_1)\\) because \\[\n\\begin{align*}\nP_\\theta(Y=0) = & (1-\\theta_1)(1-\\theta_2) = P_{\\tilde{\\theta}}(Y=0) \\\\\nP_\\theta(Y=1) = & theta_1(1-\\theta_2) + \\theta_2(1-\\theta_1) = P_{\\tilde{\\theta}}(Y=1) \\\\\nP_\\theta(Y=2) = & \\theta_1\\theta_2 = P_{\\tilde{\\theta}}(Y=2).\n\\end{align*}\n\\]\n\n\nShow that if \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_K = \\vartheta\\), then \\(\\vartheta\\) is identified.\n\n\nSolution. In this case, the expectation of \\(Y\\) is \\(\\Er[Y] =  K \\vartheta\\), so \\(\\vartheta\\) is identified by \\(\\Er[Y]/K\\)."
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#estimation",
    "href": "problemsets/midterm2023/midterm2023_solution.html#estimation",
    "title": "Midterm Solutions 2023",
    "section": "",
    "text": "Assuming \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_K = \\vartheta\\), find the maximum likelihood estimator1 for \\(\\vartheta\\), and show whether it is unbiased.\n\n\nSolution. The density of \\(Y\\) with respect to a uniform discrete measure on \\(\\{0,\n..., K\\}\\) is \\[\nf(Y_i;\\theta) = \\binom{K}{Y_i} \\theta^{Y_i} (1-\\theta)^{K-Y_i}\n\\] The loglikelihood is then \\[\n\\log \\ell(\\theta;Y) = \\sum_{i=1}^n \\left(\\log \\binom{K}{Y_i} + Y_i \\log \\theta + (K-Y_i) \\log(1-\\theta)\\right)\n\\] Solving the first order condition for \\(\\hat{\\theta}\\) gives: \\[\n\\begin{align*}\n0 = & \\sum \\frac{Y_i}{\\hat{\\theta}} - \\frac{K-Y_i}{1-\\hat{\\theta}} \\\\\n\\hat{\\theta} = \\frac{1}{nK} \\sum_{i=1}^n Y_i\n\\end{align*}\n\\]\nThis estimator is unbiased. \\[\n\\Er[\\hat{\\theta}] = \\frac{1}{nK} \\sum_{i=1}^n \\Er[Y_i] = \\vartheta.\n\\]\n\n\nShow whether or not the maximum likelihood estimator is consistent.\n\n\nSolution. Note that \\(\\Er[|Y_i|] = \\Er[Y_i] = K\\vartheta\\) is finite, and \\(Y_i\\) is iid. Therefore, by the law of large numbers, \\[\n\\frac{1}{n} \\sum_{i=1}^n Y_i \\inprob \\Er[Y_i] = K \\vartheta\n\\] so \\[\n\\frac{1}{nK} \\sum_{i=1}^n Y_i \\inprob \\vartheta.\n\\]"
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#testing",
    "href": "problemsets/midterm2023/midterm2023_solution.html#testing",
    "title": "Midterm Solutions 2023",
    "section": "",
    "text": "For this part, still assume \\(\\theta_1 = \\theta_2 = \\cdots = \\theta_K = \\vartheta\\).\n\nFor find the most powerful test for testing \\(H_0: \\vartheta =\n\\vartheta_0\\) against \\(H_1: \\vartheta = \\vartheta_a\\) where \\(\\vartheta_a &gt; \\vartheta_0\\).\n\n\nSolution. By the Neyman-Pearson Lemma, the likelihood ratio is most powerful. Let’s describe the critical region for this test. The likelihood ratio is \\[\n\\begin{align*}\nlr(\\vartheta_a,\\vartheta_0;Y)  = & \\frac{\\prod_i \\vartheta_a^{Y_i}(1-\\vartheta_a)^{K-Y_i}}{ \\prod_i \\vartheta_0^{Y_i}(1-\\vartheta_0)^{K-Y_i}} \\\\\n= & \\frac{\\vartheta_a^{\\sum Y_i}(1-\\vartheta_a)^{nK - \\sum Y_i}} {\\vartheta_0^{\\sum Y_i}(1-\\vartheta_0)^{nK - \\sum Y_i}} \\\\\n= & \\left(\\frac{\\vartheta_a}{\\vartheta_0} \\right)^{\\sum Y_i} \\left( \\frac{1-\\vartheta_a}{1-\\vartheta_0} \\right)^{nK - \\sum Y_i}\n\\end{align*}\n\\] When \\(\\vartheta_a &gt; \\vartheta_0\\), so that \\(\\frac{\\vartheta_a}{\\vartheta_0}&gt;0\\) and \\(\\frac{1-\\vartheta_a}{1-\\vartheta_0} &lt; 1\\), the likelihood ratio is an increasing function of \\(\\sum Y_i\\) and does not depend on the data in any other way. The critical region for a test of size \\(\\alpha\\) will be \\[\nC = \\{Y_i : \\sum Y_i &gt; c^*(\\alpha,\\theta_0) \\}\n\\] where \\(P_{\\theta_0}(\\sum Y_i &gt; c^*(\\theta_0)) = \\alpha\\).\n\n\nIs this test also most powerful against the alternative \\(H_1: \\vartheta &gt;\\ vartheta_0\\)? (Hint: does the critical region depend on \\(\\vartheta_a\\)?\n\n\nSolution. In the previous part, we saw that the critical region is the same for any \\(\\vartheta_a &gt; \\vartheta_0\\). Hence, the test is uniformly most powerful again \\(H_1: \\vartheta &gt; \\vartheta_0\\)."
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#identification-1",
    "href": "problemsets/midterm2023/midterm2023_solution.html#identification-1",
    "title": "Midterm Solutions 2023",
    "section": "Identification",
    "text": "Identification\n\nShow that \\(\\beta\\) is identified by explicitly writing \\(\\beta\\) as a function of the distribution of \\(x_{ij}\\) and \\(y_{ij}\\).\n\n\nSolution. We can identify \\(\\beta\\) by regression, \\(\\beta = \\Er[x_{ij} x_{ij}']^{-1} \\Er[x_ij y_ij]\\).\n\n\nSuppose that instead of observing \\((y_{ij}, x_{ij})\\) for each \\(i\\) and \\(j\\), you only observe group averages, \\(\\bar{y}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij}\\) and \\(\\bar{x}_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} x_{ij}\\). Can \\(\\beta\\) still be identified?\n\n\nSolution. Yes, \\(\\beta\\) can still be identified by \\[\n\\beta = \\Er[\\bar{x}_{j} \\bar{x}_{j}']^{-1} \\Er[\\bar{x}_j \\bar{y}_j]\n\\] as long as \\(\\Er[\\bar{x}_{j} \\bar{x}_{j}']\\) is nonsingular."
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#estimation-1",
    "href": "problemsets/midterm2023/midterm2023_solution.html#estimation-1",
    "title": "Midterm Solutions 2023",
    "section": "Estimation",
    "text": "Estimation\nContinue to assume that you only observe group averages. Construct a sample analogue estimator for \\(\\beta\\) based on your answer to 2.b.2.2\n\nShow whether your estimator is unbiased.\n\n\nSolution. The estimator is \\[\n\\hat{\\beta} = \\left(\\sum_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\sum_j \\bar{x}_j \\bar{y}_j\n\\] Substituting the model in for \\(\\bar{y}_j\\), we have \\[\n\\hat{\\beta} = \\left(\\sum_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\sum_j \\bar{x}_j (\\bar{x}_j' \\beta + \\bar{u}_j)\n\\] so \\[\n\\Er[\\hat{\\beta}] = \\beta + \\Er\\left[\\left(\\sum_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\sum_j \\bar{x}_j (\\bar{x}_j'\\bar{u}_j) \\right].\n\\] We are assuming that \\(\\Er[x_{ij}u_{ij}] = 0\\), so \\(\\Er[\\bar{x}_j\\bar{u_j}] = 0\\) as well. However, this does not imply that \\[\n\\Er\\left[\\left(\\sum_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\sum_j \\bar{x}_j (\\bar{x}_j'\\bar{u}_j)\\right] = 0,\n\\] so \\(\\hat{\\beta}\\) is biased.\nFor \\(\\hat{\\beta}\\) to be unbiased, we would need the stronger assumption that \\(\\Er[\\bar{u}_j | \\bar{x}_j] = 0\\).\n\n\nAssume that \\(\\Er[\\Vert \\bar{x}_j \\bar{x}_j' \\Vert_2^2] \\leq M\\) and \\(\\Er[\\Vert \\bar{x}_j u_j\\Vert_2^2] \\leq M\\) for all \\(j\\). Show whether or not your estimator is consistent as \\(J \\to \\infty\\).\n\n\nSolution. In this question and in the distribution question, there is some complication because \\(\\Er[\\bar{x}_j \\bar{x}_j']\\) potentially varies with \\(j\\). A really great answer recognizes this and addresses it in some way. An answer that correctly uses the law of large number and Slutsky’s lemma is also okay.\nThese assumptions imply that the law of large numbers applies to \\(\\frac{1}{J} \\sum \\bar{x}_j \\bar{x}_j'\\) and \\(\\frac{1}{J} \\sum\n\\bar{x}_j \\bar{u}_j\\). We can show this using Markov’s inequality, but doing so is not necessary for full credit. Using Markov’s inequality we have \\[\n\\begin{align*}\nP\\left( \\Vert \\frac{1}{J} \\sum (\\bar{x}_j \\bar{x}_j' - \\Er[\\bar{x}_j \\bar{x}_j']) \\Vert &gt; \\epsilon \\right) & \\leq \\frac{\\Er[ \\Vert \\frac{1}{J} \\sum (\\bar{x}_j \\bar{x}_j' - \\Er[\\bar{x}_j \\bar{x}_j]) \\Vert^2]}{\\epsilon^2} \\\\\n& \\leq \\frac{ \\frac{1}{J} \\sum \\Er[\\Vert \\bar{x}_j \\bar{x}_j'\\Vert^2]} {\\epsilon^2} \\\\\n& \\leq \\frac{M}{J \\epsilon^2}\n\\end{align*}\n\\] so \\(\\frac{1}{J} \\sum \\bar{x}_j \\bar{x}_j' \\inprob \\frac{1}{J} \\sum\n\\Er[\\bar{x}_j \\bar{x}_j']\\). An identical argument shows that \\(\\frac{1}{J} \\sum \\bar{x}_j \\bar{u}_j \\inprob 0\\).\nTherefore, \\[\n\\begin{align*}\n\\hat{\\beta} = &\\beta + \\left(\\frac{1}{J} \\sum_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\frac{1}{J} \\sum_j \\bar{x}_j \\bar{u}_j \\\\\n\\inprob & \\beta\n\\end{align*}\n\\] provided that \\(\\frac{1}{J} \\sum \\Er[\\bar{x}_j \\bar{x}_j']\\) is invertible for all \\(J\\) large enough. It would be sufficient to assume \\(\\lim_{J \\to \\infty} \\frac{1}{J} \\sum \\Er[\\bar{x}_j \\bar{x}_j'] = C\\) exists and \\(C\\) is invertible, but weaker conditions are possible as well."
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#efficiency",
    "href": "problemsets/midterm2023/midterm2023_solution.html#efficiency",
    "title": "Midterm Solutions 2023",
    "section": "Efficiency",
    "text": "Efficiency\nAssume that \\(\\Er[u_{ij}u_{\\ell k}] = \\begin{cases}\n                          \\sigma^2 & \\text{ if } i=\\ell \\text{ and } j=k \\\\\n                          0 & \\text{ otherwise}\n                        \\end{cases}\\).\n\nSuppose you observe \\(y_{ij}\\) and \\(x_{ij}\\) for each \\(i\\) and \\(j\\). What is the minimal variance unbiased estimator for \\(c'\\beta\\) that is a linear function of \\(y\\)?\n\n\nSolution. In this case, all the assumptions of the Gauss Markov thereom are met, so ordinary least squares in the best linear unbiased estimator.\n\n\nSuppose you only observe group averages \\(\\bar{y}_j\\) and \\(\\bar{x}_j\\). What is the minimal variance unbiased estimator for \\(c'\\beta\\) that is a linear function of \\(\\bar{y}\\)?\n\n\nSolution. Now, the Gauss Markov theorem does not directly apply because \\(var(\\bar{u}_j) = \\frac{\\sigma^n}{n_j}\\) is not the same for all \\(j\\). However, we can transform the model to make the variance constant, \\[\n\\sqrt{n_j} \\bar{y}_j = \\sqrt{n_j} \\bar{x}_j \\beta + \\underbrace{\\sqrt{n_j} \\bar{u}_j}_{\\tilde{u}_j}\n\\] Now \\(\\Er[\\tilde{u}\\tilde{u}'] = \\sigma^2 I_J\\), so the Gauss Markov theorem applies and the best linear unbiased estimator is \\[\n\\hat{\\beta}^{WLS} = \\left(\\sum_j n_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\sum_j n_j \\bar{x}_j \\bar{y}_j\n\\]"
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#distribution",
    "href": "problemsets/midterm2023/midterm2023_solution.html#distribution",
    "title": "Midterm Solutions 2023",
    "section": "Distribution",
    "text": "Distribution\nContinue to assume that you only observe group averages. Let \\(\\hat{\\beta}^{WLS} = \\left( \\sum_{j=1}^J n_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\left( \\sum_{j=1}^J n_j \\bar{x}_j \\bar{y}_j \\right)\\). Show that \\[\n\\sqrt{J}(\\hat{\\beta}^{WLS} - \\beta) = \\sqrt{J}\\left( \\sum_{j=1}^J n_j \\bar{x}_j \\bar{x}_j' \\right)^{-1} \\left( \\sum_{j=1}^J n_j \\bar{x}_j \\bar{u}_j \\right)\n\\] converges in distributions and compute the limiting distribution. State any additional assumptions that you need to show convergence.\n\nSolution. As above, assuming \\(\\Er[\\Vert n_j \\bar{x}_j \\bar{x}_j' \\Vert_2^2]\\) is finite is sufficient for a law of large numbers to apply to \\(\\frac{1}{J} \\sum_j n_j \\bar{x}_j \\bar{x}_j'\\).\nWe already have assumptions that imply \\(\\Er[n_j \\bar{x}_j \\bar{u}_j] = 0\\). If we also assume that this has a variance, then a central limit theorem would apply. It would be an acceptable (and the expected) answer to just assume \\(\\Er[n^2_j \\bar{x}_j \\bar{x}_j' \\bar{u}_j^2] = \\Omega\\) for all \\(j\\) and proceed.\nAlternatively, we could assume \\(\\Er[u_{ij}^2 | x_{ij}] = \\sigma^2\\) for all \\(i\\) and \\(j\\) and \\(u_{ij}\\) are independent across \\(i\\) and \\(j\\). Then, \\[\n\\begin{align*}\n\\Er[n_j \\bar{x}_j \\bar{x}_j' \\bar{u}_j^2] = & \\Er[ n_j \\bar{x}_j \\bar{x}_j' \\Er[n_j \\bar{u}_j^2 | \\bar{x}_j]] \\\\\n= & \\Er[n_j \\bar{x}_j \\bar{x}_j'] \\sigma^2\n\\end{align*}\n\\] If we also assume \\(\\Er[x_{ij} x_{ij}']=C\\) is the same for all \\(i\\) and \\(j\\) and observations are independent, then \\(\\Er[n_j \\bar{x}_j\n\\bar{x}_j'] = C\\) for all \\(j\\).\nIn that case, by the central limit theorem, \\[\n\\frac{1}{\\sqrt{J}} \\sum_{j=1}^J \\bar{x}_j \\bar{u}_j \\indist N(0,\\Er[x_{ij}x_{ij}'] \\sigma^2 )\n\\] and therefore, \\[\n\\sqrt{J}(\\hat{\\beta} - \\beta) \\indist N\\left(0, \\Er[x_{ij} x_{ij}']^{-1} \\sigma^2 \\right).\n\\]"
  },
  {
    "objectID": "problemsets/midterm2023/midterm2023_solution.html#footnotes",
    "href": "problemsets/midterm2023/midterm2023_solution.html#footnotes",
    "title": "Midterm Solutions 2023",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you cannot find the maximum likelihood estimator, show whether \\(\\bar{Y}/K = \\frac{1}{nK}\n\\sum_{i=1}^n Y_i\\) is unbiased and consistent for partial credit.↩︎\nIf you could not answer that part, suppose you had shown that \\(\\beta = \\Er[\\bar{x}_j \\bar{x}_j']^{-1} \\Er[\\bar{x}_j \\bar{y}_j]\\).↩︎"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html",
    "href": "problemsets/midterm_review/midterm_review.html",
    "title": "ECON 626: Midterm Review",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\]"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#measure-theory",
    "href": "problemsets/midterm_review/midterm_review.html#measure-theory",
    "title": "ECON 626: Midterm Review",
    "section": "Measure Theory",
    "text": "Measure Theory\n\nMeasure Space\n\nA set \\(\\Omega\\)\nA collection of subsets, \\(\\mathscr{F}\\), of \\(\\Omega\\) that is a \\(\\sigma\\)-field (aka \\(\\sigma\\)-algebra) , that is\n\n\\(\\Omega \\in \\mathscr{F}\\)\nIf \\(A \\in \\mathscr{F}\\), then \\(A^c \\in \\mathscr{F}\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\), then \\(\\cup_{j=1}^\\infty A_j \\in \\mathscr{F}\\)\n\nA measure, \\(\\mu: \\mathcal{F} \\to [0, \\infty]\\) s.t.\n\n\\(\\mu(\\emptyset) = 0\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\) are pairwise disjoint, then \\(\\mu\\left(\\cup_{j=1}^\\infty A_j \\right) = \\sum_{j=1}^\\infty \\mu(A_j)\\)\n\n\nGiven a topology on \\(\\Omega\\), the Borel \\(\\sigma\\)-field, \\(\\mathscr{B}(\\Omega)\\), is the smallest \\(\\sigma\\)-field containing all open subsets of \\(\\Omega\\).\n\n\\(f: \\Omega \\to \\mathbf{R}\\) is (\\(\\mathscr{F}\\)-)measurable if \\(\\forall\\) \\(B \\in \\mathscr{B}(\\mathbf{R})\\), \\(f^{-1}(B) \\in \\mathscr{F}\\)\n\n\n\n\n\n\n\nTip\n\n\n\nChapter 1, exercises 1.1, 1.2,\n\n\n\n\nLesbegue Integral\nThe Lesbegue integral satisfies:\n\nIf \\(f \\geq 0\\) a.e., then \\(\\int f d\\mu \\geq 0\\)\nLinearity: \\(\\int (af + bg) d\\mu = a\\int f d\\mu + b \\int g d \\mu\\)\n\n\nMeasure \\(\\nu\\) is absolutely continuous with respect to \\(\\mu\\) if for \\(A \\in \\mathscr{F}\\), \\(\\mu(A) = 0\\) implies \\(\\nu(A) = 0\\)\n\ndenotate as \\(\\nu \\ll \\mu\\)\n\\(\\mu\\) is called a dominating measure\n\n\n\nRadon-Nikodym Derivative\nLet \\((\\Omega,\\mathscr{F},\\mu)\\) be a measure space, and let \\(\\nu\\) and \\(\\mu\\) be \\(\\sigma\\)-finite measures defined on \\(\\mathscr{F}\\) and \\(\\nu \\ll \\mu\\). Then there is a nonnegative measurable function \\(f\\) such that for each set \\(A\\in \\mathscr{F}\\), \\[\n\\nu (A)=\\int_{A}fd\\mu\n\\] For any such \\(f\\) and \\(g\\), \\(\\mu (\\{\\omega \\in \\Omega:f(\\omega )\\neq g(\\omega )\\})=0\\)\n\n\n\n\n\n\nTip\n\n\n\nChapter 1, exercises 3.1, 3.2\n\n\n\n\n\nConvergence Theorems\n\nContinuity of Measure\nSuppose that \\(\\{E_{n}\\}\\) is a monotone sequence of events. Then \\[\n\\mu \\left( \\lim_{n\\rightarrow \\infty}E_{n}\\right) =\\lim_{n\\rightarrow \\infty }\\mu (E_{n}).\n\\]\n\n\nMonotone Convergence Theorem\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\uparrow f(\\omega )\\), then \\(\\int f_{n}d\\mu \\uparrow \\int fd\\mu\\) as \\(n\\rightarrow \\infty\\)\n\n\nFatou’s Lemma\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), then \\[\n\\int \\left( \\text{liminf}_{n\\rightarrow \\infty }f_{n}d\\mu \\right) \\leq \\text{liminf}_{n\\rightarrow \\infty }\\int f_{n}d\\mu\n\\]\n\n\nDominated Convergence Theorem\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\rightarrow f(\\omega ).\\) Furthermore, for some \\(g\\geq 0\\) such that \\(\\int gd\\mu &lt;\\infty\\), \\(|f_{n}|\\leq g\\) for each \\(n\\geq 1\\). Then, \\(\\int f_{n}d\\mu \\rightarrow \\int fd\\mu\\)"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#random-variables",
    "href": "problemsets/midterm_review/midterm_review.html#random-variables",
    "title": "ECON 626: Midterm Review",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable \\(X\\) is a measurable function from \\(\\Omega\\) to \\(\\mathbf{R}\\)\n\nDistribution\nLet \\((\\Omega ,\\mathscr{F},P)\\) be a probability space, \\(X\\) a random variable on \\((\\Omega ,\\mathscr{F})\\). A distribution \\(P_{X}\\) induced by \\(X\\) is a probability measure on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that : \\(\\forall B\\in \\mathscr{B}(\\mathbf{R})\\), \\[\nP_{X}(B)\\equiv P\\left\\{ \\omega \\in \\Omega :X(\\omega )\\in B\\right\\}\n\\]\n\n\nCDF\nThe CDF of a random variable \\(X\\) with distribution \\(P_{X}\\) is defined to be a function \\(F:\\mathbf{R}\\rightarrow [0,1]\\) such that \\[\nF(t)=P_{X}\\left( (-\\infty ,t]\\right) .\n\\]\n\n\nPDF\nLet \\(X\\) be a random variable with distribution \\(P_{X}\\). When \\(P_{X}\\ll \\lambda\\), we call \\(X\\) a continuous random variable, and call the Radon-Nikodym derivative \\(f\\equiv dP_{X}/d\\lambda\\) the (probability) density function of \\(P_{X}\\).\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercise 2.1, 2.2, 2.3"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#inequalities",
    "href": "problemsets/midterm_review/midterm_review.html#inequalities",
    "title": "ECON 626: Midterm Review",
    "section": "Inequalities",
    "text": "Inequalities\n\nMarkov’s Inequality\n\\(P(|X|&gt;\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)\n\n\nJensen’s Inequality\nSuppose that \\(g\\) is convex and \\(X\\) and \\(g(X)\\) are integrable, then \\(g(\\Er X) \\leq \\Er[g(X)]\\)\n\n\nCauchy-Schwarz Inequality\n\\(\\left(\\Er[XY]\\right)^2 \\leq \\Er[X^2] \\Er[Y^2]\\)\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercise 3.1"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#dependence-and-information",
    "href": "problemsets/midterm_review/midterm_review.html#dependence-and-information",
    "title": "ECON 626: Midterm Review",
    "section": "Dependence and Information",
    "text": "Dependence and Information\n\nGenerated \\(\\sigma\\)-field\n\n\\(\\sigma(X)\\) is \\(\\sigma\\)-field generated by \\(X\\)\n\nsmallest \\(\\sigma\\)-field w.r.t. which \\(X\\) is measurable\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R)\\}\\)\n\n\n\n\nInformation\n\n\\(\\forall E \\in \\sigma(X)\\), observing value \\(x\\) of \\(X\\), tells us whether \\(E\\) occurred\nif \\(\\sigma(X_1) \\subset \\sigma(X_2)\\), then \\(\\sigma(X_2)\\) has more information than \\(\\sigma(X_1)\\)\n\n\n\nDependence\nSuppose \\(g:\\R \\to \\R\\) is Borel measurable, then \\(\\sigma(g(X)) \\subset \\sigma(X)\\)\nSuppose \\(\\sigma(W) \\subset \\sigma(X)\\), then \\(\\exists\\) Borel measurable \\(g\\) s.t. \\(W=g(X)\\)\n\n\nIndependence\n\nEvents \\(A_1, ..., A_m\\) are independent if for any sub-collection \\(A_{i_1}, ..., A_{i_s}\\) \\[\nP\\left(\\cap_{j=1}^s A_{i_j}\\right) = \\prod_{j=1}^s P(A_{i_j})\n\\]\n\\(\\sigma\\)-fields, \\(\\mathscr{F}_1, .., \\mathscr{F}_m \\subset\n\\mathscr{F}\\) are independent if for any \\(\\mathscr{F}_{i_1}, ..,\n\\mathscr{F}_{i_s}\\) and \\(E_j \\in \\mathscr{F}_j\\), \\[\nP\\left(\\cap_{j=1}^s E_{i_j}\\right) = \\prod_{j=1}^s P(E_{i_j})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are independent if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are independent\n\nSuppose that \\(X=(X_1, X_2)\\) and \\(Y=(Y_1, Y_2)\\) are independent, then \\(f(X)\\) and \\(g(Y)\\) are independent\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercises 4.1-4.8"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#conditional-expectation",
    "href": "problemsets/midterm_review/midterm_review.html#conditional-expectation",
    "title": "ECON 626: Midterm Review",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nLet \\(\\mathscr{G} \\subset \\mathscr{F}\\) be \\(\\sigma\\)-fields, \\(Y\\) a random variable with \\(\\Er |Y| &lt; \\infty\\), then the conditional expectation of \\(Y\\) given \\(\\mathscr{G}\\) is \\(\\Er[Y|\\mathscr{G}](\\cdot): \\Omega \\to \\R\\) s.t.\n\n\\(\\Er[Y|\\mathscr{G}](\\cdot)\\) is \\(\\mathscr{G}\\) measurable\n\\(\\int_A \\Er[Y|\\mathscr{G}] dP = \\int_A Y dP\\) \\(\\forall A \\in \\mathscr{G}\\)\n\n\nProperties\n\nIf \\(X\\) is \\(\\mathscr{G}\\) measurable, then \\(\\Er[XY| \\mathscr{G}] = X \\Er[Y|\\mathscr{G}]\\) a.e.\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[XY|Z] = X \\Er[Y|Z]\\)\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[\\Er[Y|Z]|X] = \\Er[Y|X]\\)\nIf \\(Y\\) and \\(X\\) are independent, then \\(\\Er[Y | X ] = \\Er[Y]\\)\n\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercise 5.1, 5.2, 5.3"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#observationally-equivalent",
    "href": "problemsets/midterm_review/midterm_review.html#observationally-equivalent",
    "title": "ECON 626: Midterm Review",
    "section": "Observationally Equivalent",
    "text": "Observationally Equivalent\n\nLet \\(\\mathcal{P} = \\{ P(\\cdot; s) : s \\in S \\}\\), two structures \\(s\\) and \\(\\tilde{s}\\) in \\(S\\) are observationally equivalent if they imply the same distribution for the observed data, i.e. \\[ P(B;s) = P(B; \\tilde{s}) \\] for all \\(B \\in \\sigma(X)\\).\nLet \\(\\lambda: S \\to \\R^k\\), \\(\\theta\\) is observationally equivalent to \\(\\tilde{\\theta}\\) if \\(\\exists s, \\tilde{s} \\in S\\) that are observationally equivalent and \\(\\theta = \\lambda(s)\\) and \\(\\tilde{\\theta} = \\lambda(\\tilde{s})\\)\n\nLet \\(\\Gamma(\\theta, S) = \\{P(\\dot; s) | s \\in S, \\theta = \\lambda(s) \\}\\), then \\(\\theta\\) and \\(\\tilde{\\theta}\\) are observationally equivalent iff \\(\\Gamma(\\theta,S) \\cap \\Gamma(\\tilde{\\theta}, S) \\neq \\emptyset\\)"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#non-constructive-identification",
    "href": "problemsets/midterm_review/midterm_review.html#non-constructive-identification",
    "title": "ECON 626: Midterm Review",
    "section": "(Non-Constructive) Identification",
    "text": "(Non-Constructive) Identification\n\n\\(s_0 \\in S\\) is identified if there is no \\(s\\) that is observationally equivalent to \\(s_0\\)\n\\(\\theta_0\\) is identified (in \\(S\\)) if there is no observationally equivalent \\(\\theta \\neq \\theta_0\\)\n\ni.e. \\(\\Gamma(\\theta_0, S) \\cap \\Gamma(\\theta, S) = \\emptyset\\) \\(\\forall \\theta \\neq \\theta_0\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nChapter 3, exercise 1.1, 1.2"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#sample-analogue-estimation",
    "href": "problemsets/midterm_review/midterm_review.html#sample-analogue-estimation",
    "title": "ECON 626: Midterm Review",
    "section": "Sample Analogue Estimation",
    "text": "Sample Analogue Estimation\n\n\n\n\n\n\nTip\n\n\n\nChapter 3, exercise 1.3, 1.4"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#maximum-likelihood-estimation",
    "href": "problemsets/midterm_review/midterm_review.html#maximum-likelihood-estimation",
    "title": "ECON 626: Midterm Review",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#cramer-rao-lower-bound",
    "href": "problemsets/midterm_review/midterm_review.html#cramer-rao-lower-bound",
    "title": "ECON 626: Midterm Review",
    "section": "Cramer-Rao Lower Bound",
    "text": "Cramer-Rao Lower Bound\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta\n\\in \\Theta \\subset \\R^d \\}\\), likelihood \\(\\ell(\\theta;x) = f_X(x;\\theta)\\)"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#score-equality",
    "href": "problemsets/midterm_review/midterm_review.html#score-equality",
    "title": "ECON 626: Midterm Review",
    "section": "Score Equality",
    "text": "Score Equality\n\nIf \\(\\frac{\\partial}{\\partial \\theta} \\int f_X(x;\\theta) d\\mu(x) =\n\\int \\frac{\\partial}{\\partial \\theta} f_X(x;\\theta) d\\mu(x)\\), then \\[\n\\int \\underbrace{\\frac{\\partial \\log \\ell(\\theta;x)}{\\partial \\theta}}_{\\text{\"score\"}=s(x,\\theta)} dP_\\theta(x) = 0\n\\]\n\n\nInformation Equality\n\nFischer Information \\(I(\\theta) = \\int s(x,\\theta) s(x,\\theta)' dP_\\theta(x)\\)\nIf \\(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} \\int f_X(x;\\theta) d\\mu(x) =\n\\int \\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} f_X(x;\\theta)\nd\\mu(x)\\), then \\[\nI(\\theta) = -\\int \\underbrace{\\frac{\\partial^2 \\ell(\\theta;x)}{\\partial \\theta \\partial \\theta'}}_{\\text{\"Hessian\"}=h(x,\\theta)} dP_\\theta(x)\n\\]\n\n\n\n2\n\nIf \\(T = \\tau(X)\\) is an unbiased estimator for \\(\\theta\\) and \\[\n\\frac{\\partial}{\\partial \\theta} \\int \\tau(x) f_X(x;\\theta) d\\mu(x) =\n\\int \\tau(x) \\frac{\\partial f_X(x,\\theta)}{\\partial \\theta\\partial \\theta'} d\\mu(x)\n\\] then \\[\n\\int \\tau(x) s(x,\\theta)'dP_\\theta(x) = I\n\\]\n\n\n\nCramér-Rao Bound\nLet \\(T = \\tau(X)\\) be an unbiased estimator, and suppose the condition of the previous slide and of the score equality hold. Then, \\[\n\\var_\\theta(\\tau(X)) \\equiv \\int \\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)\\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)' dP\\theta \\geq I(\\theta)^{-1}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nChapter 3, exercise 1.5"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#hypothesis-testing",
    "href": "problemsets/midterm_review/midterm_review.html#hypothesis-testing",
    "title": "ECON 626: Midterm Review",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_0)\\)=Type I error \\(=P_x(C)\\)\n\\(P(\\text{fail to reject } H_0 | P_x \\in \\mathcal{P}_1)\\)=Type II error\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_1)\\) = power\n\\(\\sup_{P_x \\in \\mathcal{P}_0} P_x(C)\\) = size of test\n\n\nNeyman-Pearson Lemma\nLet \\(\\Theta = \\{0, 1\\}\\), \\(f_0\\) and \\(f_1\\) be densities of \\(P_0\\) and \\(P_1\\), $(x) =f_1(x)/f_0(x) $ and \\(C^* =\\{x \\in X: \\tau(x) &gt; c\\}\\). Then among all tests \\(C\\) s.t. \\(P_0(C) = P_0(C^*)\\), \\(C^*\\) is most powerful."
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#gauss-markov-theorem",
    "href": "problemsets/midterm_review/midterm_review.html#gauss-markov-theorem",
    "title": "ECON 626: Midterm Review",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\\[\nY = \\theta + u\n\\] with \\(\\theta \\in L \\subset \\R^n\\), \\(L\\) a known subspace. If \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the best linear unbiased estimator (BLUE) of \\(a'\\theta = a'\\hat{\\theta}\\) where \\(\\hat{\\theta} = P_L y\\)\n\n\n\n\n\n\nTip\n\n\n\nChapter 4, exercise 2.1"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Asymptotic Theory of Least Squares\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence in Distribution\n\n\n\n\n\n\n\n\nDec 14, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nConvergence in Probability\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nDifference in Diffferences\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Final\n\n\n\n\n\n\n\n\nDec 14, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Final - Solutions\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Final - Solutions\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Midterm\n\n\n\n\n\n\n\n\nOct 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Midterm Review\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Midterm Solutions\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 1\n\n\n\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 2\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 3\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 4\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 5\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 6\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 7\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 7\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nECON 626: Problem Set 8\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEndogeneity\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\n\n\n\n\n\n\n\n\nSep 3, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Method of Moments\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nIdentification\n\n\n\n\n\n\n\n\nDec 14, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nInstrumental Variables Estimation\n\n\n\n\n\n\n\n\nDec 14, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares as a Projection\n\n\n\n\n\n\n\n\nOct 21, 2024\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nMidterm Solutions 2023\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\nProbability\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\nPaul Schrimpf\n\n\n\n\n\nNo matching items"
  }
]