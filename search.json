[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site was created using Quarto.\nSource code is available on github with a CC-by-SA license."
  },
  {
    "objectID": "estimation/estimation.html#reading",
    "href": "estimation/estimation.html#reading",
    "title": "Estimation",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 4, sections 1.2 and 2 (which is the basis for these slides)\nSupplemental: Erich Leo Lehmann and Romano (n.d.) , Erich L. Lehmann and Casella (2006)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#estimator",
    "href": "estimation/estimation.html#estimator",
    "title": "Estimation",
    "section": "Estimator",
    "text": "Estimator\n\n\n\nDef\n\n\n\nGiven a parameter of interest \\(\\theta_0\\), an estimator is a measurable function of an observed random vector X, i.e. \\(\\hat{\\theta} = \\tau(X)\\) for some known map \\(\\tau\\)\nAn estimate given \\(X=x\\) is \\(\\tau(x)\\)"
  },
  {
    "objectID": "estimation/estimation.html#sample-analogue-estimation",
    "href": "estimation/estimation.html#sample-analogue-estimation",
    "title": "Estimation",
    "section": "Sample Analogue Estimation",
    "text": "Sample Analogue Estimation\n\ni.i.d. observations from \\(P\\), \\(X = (X_1, ...., X_n)\\)\nconstructively identified parameter \\(\\theta_0 = \\psi(P)\\)\nempirical measure: \\[\n\\hat{P}(B) = \\frac{1}{n} \\sum_{i=1}^n 1\\{X_i \\in B \\}.\n\\]\nSample analogue estimator \\[\n\\hat{\\theta} = \\psi(\\hat{P})\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#sample-analogue-estimation---examples",
    "href": "estimation/estimation.html#sample-analogue-estimation---examples",
    "title": "Estimation",
    "section": "Sample Analogue Estimation - Examples",
    "text": "Sample Analogue Estimation - Examples\n\n\nMean\nOLS\n\n\n\nGo over some examples. Introduce empirical expectation.\nMention idea of conditions on \\(\\psi\\) and \\(\\hat{P}\\) such that estimator has good properties."
  },
  {
    "objectID": "estimation/estimation.html#maximum-likelihood-estimation",
    "href": "estimation/estimation.html#maximum-likelihood-estimation",
    "title": "Estimation",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta \\subset \\R^d \\}\\)\n\\(P_\\theta\\) dominated by \\(\\sigma\\)-finite \\(\\mu\\) with density \\(f_X(\\cdot;\\theta)\\)\nLikelihood \\(\\ell(\\cdot, X): \\Theta \\to [0,\\infty)\\) \\[\n\\ell(\\theta; X)= f(X; \\theta)\n\\]\nMaximum likelihood estimator \\[\n\\hat{\\theta}_{MLE} = \\textrm{arg}\\max_{\\theta \\in \\Theta} \\ell(\\theta;X)\n\\]\n\n\nNormal mean example.\nLog-likelihood."
  },
  {
    "objectID": "estimation/estimation.html#mle-examples",
    "href": "estimation/estimation.html#mle-examples",
    "title": "Estimation",
    "section": "MLE: Examples",
    "text": "MLE: Examples\n\n\n\\(X_i \\sim N(\\mu, 1)\\)\n\\(Y_i = \\alpha_0 + \\beta_0 X_i + \\epsilon_i\\), \\(\\epsilon_i \\sim N(0, \\sigma_0^2)\\)"
  },
  {
    "objectID": "estimation/estimation.html#mle-invariance",
    "href": "estimation/estimation.html#mle-invariance",
    "title": "Estimation",
    "section": "MLE: Invariance",
    "text": "MLE: Invariance\n\n\n\nTheorem 1.1\n\n\nIf \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\), then for any function \\(g:\\Theta \\to G\\), the MLE of \\(g(\\theta)\\) is \\(g(\\hat{\\theta})\\)."
  },
  {
    "objectID": "estimation/estimation.html#mean-squared-error",
    "href": "estimation/estimation.html#mean-squared-error",
    "title": "Estimation",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nLoss function \\(L: \\R^d \\times \\Theta \\to [0,\\infty)\\) with \\(L(\\theta,\\theta)=0\\)\nRisk of at \\(\\theta_0\\) \\(\\Er[L(\\hat{\\theta}, \\theta_0)]\\)\nSquared error loss \\(L_2(\\theta, \\theta_0) = (\\theta-\\theta_0)'(\\theta-\\theta_0)\\)\nMean squared error \\[\nMSE(\\hat{\\theta}) = \\Er[ (\\theta-\\theta_0)'(\\theta-\\theta_0) ]\n\\]\nBias-variance decomposition \\[\nMSE(\\hat{\\theta}) = \\textrm{Bias}(\\hat{\\theta})'\\textrm{Bias}(\\hat{\\theta}) + \\textrm{tr}(\\var(\\hat{\\theta}))\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#setup",
    "href": "estimation/estimation.html#setup",
    "title": "Estimation",
    "section": "Setup",
    "text": "Setup\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta \\subset \\R^d \\}\\), likelihood \\(\\ell(\\theta;x) = f_X(x;\\theta)\\)\nQuestion: if an estimator is unbiased, what is the smallest possible variance?"
  },
  {
    "objectID": "estimation/estimation.html#score-equality",
    "href": "estimation/estimation.html#score-equality",
    "title": "Estimation",
    "section": "Score Equality",
    "text": "Score Equality\n\nIf \\(\\frac{\\partial}{\\partial \\theta} \\int f_X(x;\\theta) d\\mu(x) =  \\int \\frac{\\partial}{\\partial \\theta} f_X(x;\\theta) d\\mu(x)\\), then \\[\n\\int \\underbrace{\\frac{\\partial \\log \\ell(\\theta;x)}{\\partial \\theta}}_{\\text{\"score\"}=s(x,\\theta)} dP_\\theta(x) = 0\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#information-equality",
    "href": "estimation/estimation.html#information-equality",
    "title": "Estimation",
    "section": "Information Equality",
    "text": "Information Equality\n\nFischer Information \\(I(\\theta) = \\int s(x,\\theta) s(x,\\theta)' dP_\\theta(x)\\)\nIf \\(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} \\int f_X(x;\\theta) d\\mu(x) =  \\int \\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} f_X(x;\\theta)  d\\mu(x)\\), then \\[\nI(\\theta) = -\\int \\underbrace{\\frac{\\partial^2 \\ell(\\theta;x)}{\\partial \\theta \\partial \\theta'}}_{\\text{\"Hessian\"}=h(x,\\theta)} dP_\\theta(x)\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#section",
    "href": "estimation/estimation.html#section",
    "title": "Estimation",
    "section": "",
    "text": "If \\(T = \\tau(X)\\) is an unbiased estimator for \\(\\theta\\) and \\[\n\\frac{\\partial}{\\partial \\theta} \\int \\tau(x) f_X(x;\\theta) d\\mu(x) =\n\\int \\tau(x) \\frac{\\partial f_X(x,\\theta)}{\\partial \\theta\\partial \\theta'} d\\mu(x)\n\\] then \\[\n\\int \\tau(x) s(x,\\theta)'dP_\\theta(x) = I\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#cramér-rao-bound",
    "href": "estimation/estimation.html#cramér-rao-bound",
    "title": "Estimation",
    "section": "Cramér-Rao Bound",
    "text": "Cramér-Rao Bound\n\n\n\nCramér-Rao Bound\n\n\nLet \\(T = \\tau(X)\\) be an unbiased estimator, and suppose the condition of the previous slide and of the score equality hold. Then, \\[\n\\var_\\theta(\\tau(X)) \\equiv \\int \\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)\\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)' dP\\theta \\geq I(\\theta)^{-1}\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-1",
    "href": "estimation/estimation.html#hypothesis-testing-1",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(X \\in \\mathcal{X} \\subset \\R^n\\), distribution \\(P_x \\in \\mathcal{P}\\)\nPartition \\(\\mathcal{P} = \\mathcal{P}_0 \\cup \\mathcal{P}_1\\)\nNull and alternative hypotheses:\n\n\\(H_0: \\; P_x \\in \\mathcal{P}_0\\)\n\\(H_1: \\; P_x \\in \\mathcal{P}_1\\)"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-2",
    "href": "estimation/estimation.html#hypothesis-testing-2",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nTest partitions \\(\\mathcal{X} = \\underbrace{C}_{\\text{critical region}} \\cup A\\)\n\nReject null if \\(X \\in C\\)\nOften \\(C = \\{x \\in \\mathcal{X}:  \\underbrace{\\tau(x)}_{\\text{test statistic}} >  \\underbrace{c}_{\\text{critical value}} \\}\\)"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-3",
    "href": "estimation/estimation.html#hypothesis-testing-3",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_0)\\)=Type I error \\(=P_x(C)\\)\n\\(P(\\text{fail to reject } H_0 | P_x \\in \\mathcal{P}_1)\\)=Type II error\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_1)\\) = power\n\\(\\sup_{P_x \\in \\mathcal{P}_0} P_x(C)\\) = size of test\n\n\nIllustrate schematically how these vary with \\(C\\)"
  },
  {
    "objectID": "estimation/estimation.html#p-value",
    "href": "estimation/estimation.html#p-value",
    "title": "Estimation",
    "section": "p-value",
    "text": "p-value\n\ntest statistic \\(\\tau(X)\\) , define \\[ G_P(t)  =  P(\\tau(X) > t) \\]\np-value is \\[\np= \\sup_{P \\in \\mathcal{P}_0} G_P(\\tau(X))\n\\]\n\n\n\nif \\(\\mathcal{P}_0 = \\{P_0\\}\\), critical value \\(c\\), let \\(\\alpha = G_{P_0}(c)\\), then \\(\\tau(X) > c\\) iff \\(p < \\alpha\\)"
  },
  {
    "objectID": "estimation/estimation.html#testing-in-parametric-family",
    "href": "estimation/estimation.html#testing-in-parametric-family",
    "title": "Estimation",
    "section": "Testing in Parametric Family",
    "text": "Testing in Parametric Family\n\nParametric family \\(\\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta\\}\\)\n\n\\(\\Theta_0 = \\{\\theta \\in \\Theta: P_\\theta \\in \\mathcal{P}_0\\}\\)\n\\(\\Theta_1 = \\{\\theta \\in \\Theta: P_\\theta \\in \\mathcal{P}_1\\}\\)\n\nHypotheses\n\n\\(H_0 : \\theta \\in \\Theta_0\\)\n\\(H_1: \\theta \\in \\Theta_1\\)\n\nPower function of test \\(C\\) \\[\\pi:\\Theta \\to [0,1] , \\;\\;  \\pi(\\theta) = P_\\theta(C)\\]\nSize \\(= \\sup_{\\theta \\in \\Theta_0} \\pi(\\theta)\\)"
  },
  {
    "objectID": "estimation/estimation.html#more-powerful",
    "href": "estimation/estimation.html#more-powerful",
    "title": "Estimation",
    "section": "More Powerful",
    "text": "More Powerful\n\n\n\nDefinition\n\n\n\nFor test \\(C_1\\) and \\(C_2\\) with same size, \\(C_1\\) is more powerful at \\(\\theta \\in \\Theta_1\\) than \\(C_2\\) if \\(P_\\theta(C_1) \\geq P_\\theta(C_2)\\)\n\\(C\\) is most powerful at \\(\\theta \\in \\Theta_1\\) if is more powerfull than any test of the same size\n\\(C\\) is uniformly most powerful if it is most powerful at any \\(\\theta \\in \\Theta_1\\)"
  },
  {
    "objectID": "estimation/estimation.html#neyman-pearson",
    "href": "estimation/estimation.html#neyman-pearson",
    "title": "Estimation",
    "section": "Neyman-Pearson",
    "text": "Neyman-Pearson\n\n\n\nLemma (Neyman-Pearson)\n\n\nLet \\(\\Theta = \\{0, 1\\}\\), \\(f_0\\) and \\(f_1\\) be densities of \\(P_0\\) and \\(P_1\\), $(x) =f_1(x)/f_0(x) $ and \\(C^* =\\{x \\in X: \\tau(x) > c\\}\\). Then among all tests \\(C\\) s.t. \\(P_0(C) = P_0(C^*)\\), \\(C^*\\) is most powerful."
  },
  {
    "objectID": "estimation/estimation.html#example",
    "href": "estimation/estimation.html#example",
    "title": "Estimation",
    "section": "Example",
    "text": "Example\n\n\\(X_i \\sim N(\\mu, 1)\\)\n\\(H_0: \\mu = 0\\) against \\(H_1: \\mu = 1\\)\nFind a most powerful test\n\n\n\nWhat is the most powerful test if \\(H_1: \\mu = a\\) for \\(a>0\\) instead?\n\n\n\n\nWhat is the uniformly most powerful test if \\(H_1: \\mu > 0\\) ?\n\n\n\n\nWhat is the uniformly most powerful test if \\(H_1: \\mu \\neq 0\\) ?"
  },
  {
    "objectID": "identification/identification.html#reading",
    "href": "identification/identification.html#reading",
    "title": "Identification",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 4 (which is the basis for these slides)\nRecommended: Lewbel (2019)\nSupplementary: Matzkin (2013), Molinari (2020) , Imbens (2020)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\]\n\nMotivate by wanting to go from data to parameters …"
  },
  {
    "objectID": "identification/identification.html#example-descriptive-statistics",
    "href": "identification/identification.html#example-descriptive-statistics",
    "title": "Identification",
    "section": "Example: Descriptive Statistics",
    "text": "Example: Descriptive Statistics\n\n\n\\(\\theta_0 =\\) mean of \\(X\\), then \\(\\theta_0\\) is identified by \\[\n\\psi_\\mu(P) = \\int x dP(x)\n\\] in \\(\\mathcal{P} = \\{P : \\int x dP(x) < \\infty \\}\\)\nGenerally, descriptive statistics identified in a broad probability model with just regularity restrictions to ensure the statistics exist"
  },
  {
    "objectID": "identification/identification.html#example-linear-model",
    "href": "identification/identification.html#example-linear-model",
    "title": "Identification",
    "section": "Example: Linear Model",
    "text": "Example: Linear Model\n\\[\nY = \\alpha + \\beta X + \\epsilon\n\\]\n\n\\(\\mathcal{P} = \\{P_{X,Y}:\\) \\(Y=\\alpha + \\beta X + \\epsilon\\),\n\n\\(| \\mathrm{Cov}(X,Y) | < \\infty\\), \\(0 < \\mathrm{Var}(X) < \\infty\\)\n\\(\\mathrm{Cov}(X, \\epsilon) = 0\\) \\(\\}\\)\n\n\\(\\beta\\) identified as\n\n\\[\n\\beta = \\frac{\\int (x - \\Er X) (y - \\Er Y ) dP_{X,Y}(x,y)}\n{\\int (x - \\Er X)^2 dP_{X}(x)}  = \\frac{ \\cov(X,Y) }{ \\var(X) }\n\\]\n\n\nIdentification requires:\n\nUsually innocuous regularity conditions\nSubstantive exogeneity restriction\n\nEvaluating plausibility of exogeneity restrictions requires a priori knowledge of data context and related economic theory\n\n\n\nExamples of production function and returns to schooling here?\n2 is more difficult to get right and more context dependent, so we will first tackle the easier problem of 1."
  },
  {
    "objectID": "identification/identification.html#example-multiple-regression",
    "href": "identification/identification.html#example-multiple-regression",
    "title": "Identification",
    "section": "Example: Multiple Regression",
    "text": "Example: Multiple Regression\n\\[\nY = X'\\beta + \\epsilon\n\\]\n\n\\(\\mathcal{P} = \\{P: \\Er X \\epsilon = 0, \\Er X X' \\text{ invertible} \\}\\)"
  },
  {
    "objectID": "identification/identification.html#example-binary-choice",
    "href": "identification/identification.html#example-binary-choice",
    "title": "Identification",
    "section": "Example: Binary Choice",
    "text": "Example: Binary Choice\n\\[\nY = 1\\{ \\beta_0 + \\beta_1 X > u \\}\n\\]\n\n\\(\\mathcal{P} = \\{P: u \\sim N(0,1), 0< \\var(X) < \\infty \\}\\)\n\n\n\nIs \\(u \\sim N(0,1)\\) innocuous?\n\n\n\nMore generally, think about importance of statistical assumptions in terms of how they affect identifiable quantities and counterfactuals of interest."
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes",
    "href": "identification/identification.html#example-potential-outcomes",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\nData:\n\nTreatment \\(D_i\\)\nPotential outcomes \\((Y_{i,0}, Y_{i,1})\\), observed outcome \\(Y_i = D_i Y_{i,1} + (1-D_i) Y_{i,0}\\)\nCovarites \\(X_i\\)\n\nParameter: \\(\\theta_0 = \\Er[Y_{i,1} - Y_{i,0}] =\\) average treatment effect\nAssume:\n\nUnconfoundedness: \\((Y_{i,0}, Y_{i,1})\\) conditionally independent of \\(D_i\\) given \\(X_i\\)\nOverlap: \\(\\epsilon < P(D=1|X=x) < 1-\\epsilon\\) for some \\(\\epsilon > 0\\) and all \\(x\\)"
  },
  {
    "objectID": "identification/identification.html#causal-diagrams-1",
    "href": "identification/identification.html#causal-diagrams-1",
    "title": "Identification",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\n\nOriginate with Wright around 1920, e.g. Wright (1934)\nRecently advocated by Pearl, e.g. Pearl (2015), Pearl and Mackenzie (2018)\nRecommended introduction Imbens (2020)\nSometimes useful expository tool for explaining identifying restriction, but should not be your only or primary approach\n\ne.g. Chernozhukov, Kasahara, and Schrimpf (2021)"
  },
  {
    "objectID": "identification/identification.html#example-regression",
    "href": "identification/identification.html#example-regression",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-1",
    "href": "identification/identification.html#example-potential-outcomes-1",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes"
  },
  {
    "objectID": "identification/identification.html#example-regression-1",
    "href": "identification/identification.html#example-regression-1",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\nIn linear model \\(Y_i = X_i'\\beta + \\epsilon_i\\), if just assume \\(\\Er X X'\\) invertible,\nPopulation regression \\[\n\\begin{align*}\n\\theta = & \\Er[ X X']^{-1} \\Er[ X Y]  \\\\\n= & \\Er[X X']^{-1} \\Er[X (X' \\beta + \\epsilon)] \\\\\n= & \\beta + \\Er[X X']^{-1} \\Er[X\\epsilon]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "identification/identification.html#example-regression-2",
    "href": "identification/identification.html#example-regression-2",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\nIf relevant moments exist (no linear model required) population regression solves \\[\n\\Er[ X X']^{-1} \\Er[ X Y]  \\in \\mathrm{arg}\\min_b \\Er[ (X'b - \\Er[Y|X])^2 ]\n\\]"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-2",
    "href": "identification/identification.html#example-potential-outcomes-2",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\nMatching initially studied with a linear regression model, e.g. Cochran (1953) \\[\nY_i = \\alpha D_i + X_i' \\beta + \\epsilon_i\n\\]\nImplies constant treatment effect \\(Y_{i,1} - Y_{i,0} = \\alpha\\)\n\n\nThe LATE interpretation of IV developed along similar lines – replace a constant treatment effect linear model with a heterogenous effect potential outcomes framework, and see what IV does.\nSimilarly, the recent literature on difference in differences with staggered treatment timing arises from moving from a model with a constant treatment effect over time to one where treatment effects vary with time."
  },
  {
    "objectID": "identification/identification.html#non-constructive-identification-1",
    "href": "identification/identification.html#non-constructive-identification-1",
    "title": "Identification",
    "section": "Non-constructive Identification",
    "text": "Non-constructive Identification\n\nIdentification sometimes defined without explicit mapping from data to parameters, e.g. Hsiao (1983), Matzkin (2007)\n\n\n\n\nDefinition: Observationally Equivalent\n\n\n\nLet \\(\\mathcal{P} = \\{ P(\\cdot; s) : s \\in S \\}\\), two structures \\(s\\) and \\(\\tilde{s}\\) in \\(S\\) are observationally equivalent if they imply the same distribution for the observed data, i.e. \\[ P(B;s) = P(B; \\tilde{s}) \\] for all \\(B \\in \\sigma(X)\\).\nLet \\(\\lambda: S \\to \\R^k\\), \\(\\theta\\) is observationally equivalent to \\(\\tilde{\\theta}\\) if \\(\\exists s, \\tilde{s} \\in S\\) that are observationally equivalent and \\(\\theta = \\lambda(s)\\) and \\(\\tilde{\\theta} = \\lambda(\\tilde{s})\\)\n\nLet \\(\\Gamma(\\theta, S) = \\{P(\\dot; s) | s \\in S, \\theta = \\lambda(s) \\}\\), then \\(\\theta\\) and \\(\\tilde{\\theta}\\) are observationally equivalent iff \\(\\Gamma(\\theta,S) \\cap \\Gamma(\\tilde{\\theta}, S) \\neq \\emptyset\\)"
  },
  {
    "objectID": "identification/identification.html#non-constructive-identification-2",
    "href": "identification/identification.html#non-constructive-identification-2",
    "title": "Identification",
    "section": "Non-constructive Identification",
    "text": "Non-constructive Identification\n\n\n\nDefinition: (Non-Constructive) Identification\n\n\n\n\\(s_0 \\in S\\) is identified if there is no \\(s\\) that is observationally equivalent to \\(s_0\\)\n\\(\\theta_0\\) is identified (in \\(S\\)) if there is no observationally equivalent \\(\\theta \\neq \\theta_0\\)\n\ni.e. \\(\\Gamma(\\theta_0, S) \\cap \\Gamma(\\theta, S) = \\emptyset\\) \\(\\forall \\theta \\neq \\theta_0\\)\n\n\n\n\n\n\nCompared to constructive definition with \\(\\theta_0 = \\psi(P)\\):\n\nLess clear how to use identification to estimate\nEasier to show non-identification"
  },
  {
    "objectID": "identification/identification.html#example-multiple-regression-1",
    "href": "identification/identification.html#example-multiple-regression-1",
    "title": "Identification",
    "section": "Example: Multiple Regression",
    "text": "Example: Multiple Regression\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]\n\n\n\\(X = [X_1\\, X_2]'\\), if rank \\(\\Er X X' = 1\\), then \\(\\beta_1, \\beta_2\\) is observationally equivalent to any \\(\\tilde{\\beta}_1, \\tilde{\\beta}_2\\) s.t. \\[\n\\tilde{\\beta}_1 + \\tilde{\\beta}_2 = \\beta_1 + \\beta_2 \\frac{\\cov(X_1, X_2)}{\\var(X_2)}\n\\]\n\\(\\theta_0 = \\lambda( \\beta ) = \\beta_1 + \\beta_2\\) is identified if rank \\(\\Er X X' \\geq 1\\)"
  },
  {
    "objectID": "identification/identification.html#example-random-coefficients-logit",
    "href": "identification/identification.html#example-random-coefficients-logit",
    "title": "Identification",
    "section": "Example: Random Coefficients Logit",
    "text": "Example: Random Coefficients Logit\n\n\\(Y_i = 1\\{\\beta_0 + \\beta_i X_i \\geq U_i \\}\\)\n\n\\(U\\) independent \\(X_i,\\beta_i\\),\n\\(\\beta_i\\) indepedent \\(X_i\\),\n\\(F_u(z) = \\frac{e^z}{1+e^z}\\)\n\n\\(\\Er[Y|X] = \\int \\frac{e^{\\beta_0 + \\beta X_i}} {1+e^{\\beta_0 + \\beta X_i}} dF_\\beta(\\beta)\\)\nNon-constructive and constructive identification of \\(F_\\beta\\) in Fox et al. (2012)\n\n\nHard to give a brief example where non-constructive argument is required. Non constructive identification proofs typically leverage some high level mathematical result.\nChristensen (2015) is another example of non-constructive identification."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 626: Econometric Theory I",
    "section": "",
    "text": "Part II: Generalized Linear Model\n\nPreliminaries of Projection Geometry\nGeneralized Linear Models and Gauss-Markov Theorem\nTests of Linear Hypothesis\n\n\n\nPart III: Tools of Asymptotic Theory\n\nModes of Convergence\nSlutsky’s Lemma/Continuous Mapping Theorem\nDelta Methods\n\n\n\nPart V: Linear Models with Endogeneity\n\nIdentification and Endogeneity\nIdentification through IV and Inference\n\n\n\n\n\n\nReferences\n\nÇinlar, Erhan. 2011. Probability and Stochastics. Vol. 261. Springer.\n\n\nPollard, David. 2002. A User’s Guide to Measure Theoretic Probability. 8. Cambridge University Press.\n\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”\n\n\nTao, Terence. 2011. An Introduction to Measure Theory. Vol. 126. American Mathematical Society Providence."
  },
  {
    "objectID": "measure/measure.html#references",
    "href": "measure/measure.html#references",
    "title": "Measure",
    "section": "References",
    "text": "References\n\nSong (2021) chapter 1 (which is the basis for these slides)\nPollard (2002)\nTao (2011)"
  },
  {
    "objectID": "measure/measure.html#why-measure-theory",
    "href": "measure/measure.html#why-measure-theory",
    "title": "Measure",
    "section": "Why Measure Theory?",
    "text": "Why Measure Theory?\n\n\nSimplifies some arguments\n\nExample from Pollard (2002), define independence as factorization of distribution functions \\[ P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) P(Y \\leq y) \\]\nIf \\(X_1,X_2,X_3, X_4\\) are independent, show that \\[ Y = X_1 X_2 \\log\\left(\\frac{X_1^2 + X_2^3}{|X_1| + |X_2|}\\right) \\] is independent of \\[ Z =  sin\\left(X_3 + X_3^2 + X_3X_4 + X_4^2 \\right) \\]"
  },
  {
    "objectID": "measure/measure.html#why-measure-theory-1",
    "href": "measure/measure.html#why-measure-theory-1",
    "title": "Measure",
    "section": "Why Measure Theory?",
    "text": "Why Measure Theory?\n\nSimplifies some arguments\n\n\n\nUnifies treatment\n\ndiscrete vs continuous\nuni- vs multi-variate\n\nResolves some difficulties with infinity"
  },
  {
    "objectID": "measure/measure.html#measure-space",
    "href": "measure/measure.html#measure-space",
    "title": "Measure",
    "section": "Measure Space",
    "text": "Measure Space\n\n\nA set \\(\\Omega\\)\nA collection of subsets, \\(\\mathscr{F}\\), of \\(\\Omega\\) that is a \\(\\sigma\\)-field (aka \\(\\sigma\\)-algebra) , that is\n\n\\(\\Omega \\in \\mathscr{F}\\)\nIf \\(A \\in \\mathscr{F}\\), then \\(A^c \\in \\mathscr{F}\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\), then \\(\\cup_{j=1}^\\infty A_j \\in \\mathscr{F}\\)\n\nA measure, \\(\\mu: \\mathcal{F} \\to [0, \\infty]\\) s.t.\n\n\\(\\mu(\\emptyset) = 0\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\) are pairwise disjoint, then \\(\\mu\\left(\\cup_{j=1}^\\infty A_j \\right) = \\sum_{j=1}^\\infty \\mu(A_j)\\)\n\n\n\n\nExample of sigma fields"
  },
  {
    "objectID": "measure/measure.html#measurable-function",
    "href": "measure/measure.html#measurable-function",
    "title": "Measure",
    "section": "Measurable Function",
    "text": "Measurable Function\n\nGiven a topology on \\(\\Omega\\), the Borel \\(\\sigma\\)-field, \\(\\mathscr{B}(\\Omega)\\), is the smallest \\(\\sigma\\)-field containing all open subsets of \\(\\Omega\\)\n\n\nWhen working with \\(R^d\\), we will generally use \\(\\mathscr{B}(\\mathbf{R}^d)\\) as our \\(\\sigma\\)-field.\n\n\n\\(f: \\Omega \\to \\mathbf{R}\\) is (\\(\\mathscr{F}\\)-)measurable if \\(\\forall\\) \\(B \\in \\mathscr{B}(\\mathbf{R})\\), \\(f^{-1}(B) \\in \\mathscr{F}\\)\na statement holds almost everywhere (a.e.) if the measure of the set where the statement is false is 0"
  },
  {
    "objectID": "measure/measure.html#simple-functions",
    "href": "measure/measure.html#simple-functions",
    "title": "Measure",
    "section": "Simple Functions",
    "text": "Simple Functions\n\nAssume \\(\\mu(\\Omega) < \\infty\\)\n\\(f\\) is a simple function if \\(f = \\sum_{j=1}^n a_j 1\\{\\omega \\in E_j \\}\\) for \\(a_j \\in \\mathbf{R}\\) and \\(E_j \\in \\mathscr{F}\\)\nIntegral of simple functions: \\[ \\int f d \\mu = \\sum_{j=1}^n a_j \\mu(E_j) \\]"
  },
  {
    "objectID": "measure/measure.html#bounded-functions",
    "href": "measure/measure.html#bounded-functions",
    "title": "Measure",
    "section": "Bounded Functions",
    "text": "Bounded Functions\n\nLet \\(E\\) be such that \\(\\mu(E)<\\infty\\)\nLet \\(f\\) be bounded function and \\(f(x) = 0 \\forall x \\in E^c\\)\nDefine: \\[\n\\int f d\\mu \\equiv \\sup_{\\varphi \\leq f: \\varphi \\text{ simple}} \\int \\varphi d\\mu =\\inf_{\\varphi \\geq f: \\varphi \\text{ simple}} \\int \\varphi d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#section",
    "href": "measure/measure.html#section",
    "title": "Measure",
    "section": "",
    "text": "Exercise\n\n\nShow that for all bounded \\(f\\) and \\(g\\) that vanish outside a finite measure set,\n\nIf \\(f \\geq 0\\) a.e., then \\(\\int f d\\mu \\geq 0\\)\n\\(\\forall a \\in \\mathbf{R}\\) , \\(\\int a f d\\mu = a \\int f d\\mu\\)\n\\(\\int (f + g) d\\mu = \\int f d\\mu + \\int g d \\mu\\)"
  },
  {
    "objectID": "measure/measure.html#nonnegative-functions",
    "href": "measure/measure.html#nonnegative-functions",
    "title": "Measure",
    "section": "Nonnegative Functions",
    "text": "Nonnegative Functions\n\nIf \\(f \\geq 0\\), define \\[\n\\int fd\\mu =\\sup_{f_n \\leq f \\text{ simple, bounded+}} \\int f_{n}d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#measurable-functions",
    "href": "measure/measure.html#measurable-functions",
    "title": "Measure",
    "section": "Measurable Functions",
    "text": "Measurable Functions\n\nIf \\(f\\) is measurable, let \\(f^{+} = \\max\\{f, 0\\}\\) and \\(f^{-} = \\max\\{-f, 0\\}\\) and define the Lesbegue integral\n\n\\[ \\int f d\\mu = \\int f^{+} d\\mu - \\int f^{-} d\\mu \\]"
  },
  {
    "objectID": "measure/measure.html#finite-measure",
    "href": "measure/measure.html#finite-measure",
    "title": "Measure",
    "section": "Finite Measure",
    "text": "Finite Measure\n\nMeasure \\(\\mu\\) is finite if \\(\\mu(\\Omega)\\) is finite\n\\(\\mu\\) is \\(\\sigma\\)-finite if \\(\\exists\\) \\(\\{A_n\\}_{n=1}^\\infty \\in \\mathscr{F}\\) s.t. \\(\\mu(A_n)\\) is finite \\(\\forall n\\) and \\(\\cup_{n=1}^\\infty A_n = \\Omega\\)\n\n\n\n\nExercise\n\n\nLet \\(\\Omega\\) be countable with any \\(\\mathscr{F}\\), define \\(\\mu(A)\\) as the number of elements of \\(A\\). Show \\(\\mu\\) is \\(\\sigma\\) finite."
  },
  {
    "objectID": "measure/measure.html#lebesgue-measure",
    "href": "measure/measure.html#lebesgue-measure",
    "title": "Measure",
    "section": "Lebesgue Measure",
    "text": "Lebesgue Measure\n\n\n\nTheorem\n\n\nThere exists a unique \\(\\sigma\\)-finite measure \\(\\mu\\) on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that for any \\(a\\leq b\\) with \\(a,b\\in \\mathbf{R}\\), \\[\n\\mu ((a,b])=b-a\n\\]"
  },
  {
    "objectID": "measure/measure.html#absolute-continuity",
    "href": "measure/measure.html#absolute-continuity",
    "title": "Measure",
    "section": "Absolute Continuity",
    "text": "Absolute Continuity\n\nMeasure \\(\\nu\\) is absolutely continuous with respect to \\(\\mu\\) if for \\(A \\in \\mathscr{F}\\), \\(\\mu(A) = 0\\) implies \\(\\nu(A) = 0\\)\n\ndenotate as \\(\\nu \\ll \\mu\\)\n\\(\\mu\\) is called a dominating measure\n\n\n\nRelation with continuity?"
  },
  {
    "objectID": "measure/measure.html#radon-nikodym-derivative",
    "href": "measure/measure.html#radon-nikodym-derivative",
    "title": "Measure",
    "section": "Radon-Nikodym Derivative",
    "text": "Radon-Nikodym Derivative\n\n\n\nTheorem\n\n\nLet \\((\\Omega,\\mathscr{F},\\mu)\\) be a measure space, and let \\(\\nu\\) and \\(\\mu\\) be \\(\\sigma\\)-finite measures defined on \\(\\mathscr{F}\\) and \\(\\nu \\ll \\mu\\). Then there is a nonnegative measurable function \\(f\\) such that for each set \\(A\\in \\mathscr{F}\\), \\[\n\\nu (A)=\\int_{A}fd\\mu\n\\] For any such \\(f\\) and \\(g\\), \\(\\mu (\\{\\omega \\in \\Omega:f(\\omega )\\neq g(\\omega )\\})=0\\)\n\n\n\n\nDenote $f = \nExercise: show coincides with usual derivative?"
  },
  {
    "objectID": "measure/measure.html#sequences-of-sets",
    "href": "measure/measure.html#sequences-of-sets",
    "title": "Measure",
    "section": "Sequences of Sets",
    "text": "Sequences of Sets\n\n\\(\\{E_n\\}_{n \\geq 1} \\in \\mathscr{F}\\)\n\nincreasing if \\(E_1 \\subset E_2 \\subset ...\\)\ndecreasing if \\(E_1 \\supset E_2 \\supset ...\\)\nmonotone if either increasing or decreasing\n\nFor increasing \\(E_n\\), define \\(\\lim_{n \\to \\infty} E_n =\\cup_{n=1}^\\infty E_n\\)\nFor decreasing \\(E_n\\), define \\(\\lim_{n \\to \\infty} E_n =\\cap_{n=1}^\\infty E_n\\)"
  },
  {
    "objectID": "measure/measure.html#continuity-of-measure",
    "href": "measure/measure.html#continuity-of-measure",
    "title": "Measure",
    "section": "Continuity of Measure",
    "text": "Continuity of Measure\n\n\n\nLemma\n\n\nSuppose that \\(\\{E_{n}\\}\\) is a monotone sequence of events. Then \\[\n\\mu \\left( \\lim_{n\\rightarrow \\infty}E_{n}\\right) =\\lim_{n\\rightarrow \\infty }\\mu (E_{n}).\n\\]"
  },
  {
    "objectID": "measure/measure.html#monotone-convergence-theorem",
    "href": "measure/measure.html#monotone-convergence-theorem",
    "title": "Measure",
    "section": "Monotone Convergence Theorem",
    "text": "Monotone Convergence Theorem\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\uparrow f(\\omega )\\), then \\(\\int f_{n}d\\mu \\uparrow \\int fd\\mu\\) as \\(n\\rightarrow \\infty\\)"
  },
  {
    "objectID": "measure/measure.html#fatous-lemma",
    "href": "measure/measure.html#fatous-lemma",
    "title": "Measure",
    "section": "Fatou’s Lemma",
    "text": "Fatou’s Lemma\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), then \\[\n\\int \\left( \\text{liminf}_{n\\rightarrow \\infty }f_{n}d\\mu \\right) \\leq \\text{liminf}_{n\\rightarrow \\infty }\\int f_{n}d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#dominated-convergence-theorem",
    "href": "measure/measure.html#dominated-convergence-theorem",
    "title": "Measure",
    "section": "Dominated Convergence Theorem",
    "text": "Dominated Convergence Theorem\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\rightarrow f(\\omega ).\\) Furthermore, for some \\(g\\geq 0\\) such that \\(\\int gd\\mu <\\infty\\), \\(|f_{n}|\\leq g\\) for each \\(n\\geq 1\\). Then, \\(\\int f_{n}d\\mu \\rightarrow \\int fd\\mu\\)\n\n\n\n\nThe measurable condition here can be dropped and outermeasure used instead. This simplifies some proofs in asymptotic theory."
  },
  {
    "objectID": "probability/probability.html#reading",
    "href": "probability/probability.html#reading",
    "title": "Probability",
    "section": "Reading",
    "text": "Reading\n\nSong (2021) chapter 2 (which is the basis for these slides)\nPollard (2002)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\R{{\\mathbb{R}}}\n\\]"
  },
  {
    "objectID": "probability/probability.html#probability-space",
    "href": "probability/probability.html#probability-space",
    "title": "Probability",
    "section": "Probability Space",
    "text": "Probability Space\n\n\n\nDefinitions\n\n\n\nGiven a measure space \\((\\Omega ,\\mathscr{F})\\), a probability (or probability measure)\\(\\ P\\) is a measure s.t. \\(P(\\Omega )=1\\)\n\\((\\Omega ,\\mathscr{F}, P)\\) is a probability space\n\\(\\Omega\\) is a sample space\n\\(\\omega \\in \\Omega\\) is an outcome\n\\(A \\in \\mathscr{F}\\) is an event"
  },
  {
    "objectID": "probability/probability.html#random-variable",
    "href": "probability/probability.html#random-variable",
    "title": "Probability",
    "section": "Random Variable",
    "text": "Random Variable\n\n\n\nDefinition\n\n\nA random variable \\(X\\) is a measurable function from \\(\\Omega\\) to \\(\\mathbf{R}\\)"
  },
  {
    "objectID": "probability/probability.html#distribution",
    "href": "probability/probability.html#distribution",
    "title": "Probability",
    "section": "Distribution",
    "text": "Distribution\n\n\n\nDefinition\n\n\nLet \\((\\Omega ,\\mathscr{F},P)\\) be a probability space, \\(X\\) a random variable on \\((\\Omega ,\\mathscr{F})\\). A distribution \\(P_{X}\\) induced by \\(X\\) is a probability measure on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that : \\(\\forall B\\in \\mathscr{B}(\\mathbf{R})\\), \\[\nP_{X}(B)\\equiv P\\left\\{ \\omega \\in \\Omega :X(\\omega )\\in B\\right\\}\n\\]"
  },
  {
    "objectID": "probability/probability.html#distribution-function",
    "href": "probability/probability.html#distribution-function",
    "title": "Probability",
    "section": "Distribution Function",
    "text": "Distribution Function\n\n\n\nDefinition\n\n\nThe CDF of a random variable \\(X\\) with distribution \\(P_{X}\\) is defined to be a function \\(F:\\mathbf{R}\\rightarrow [0,1]\\) such that \\[\nF(t)=P_{X}\\left( (-\\infty ,t]\\right) .\n\\]"
  },
  {
    "objectID": "probability/probability.html#stochastic-dominance",
    "href": "probability/probability.html#stochastic-dominance",
    "title": "Probability",
    "section": "Stochastic Dominance",
    "text": "Stochastic Dominance\n\n\n\nDefinition\n\n\nSuppose that two random variables \\(X_1\\) and \\(X_2\\) have CDFs \\(F_1\\) and \\(F_2\\).\n\nSuppose that \\(F_1(x) \\le F_2(x)\\) for all \\(x \\in \\mathbf{R}\\). Then \\(X_1\\) first order stochastically dominates (FOSD) \\(X_2\\).\nSuppose that for all \\(y \\in \\mathbf{R}\\), \\[\n     \\int_{-\\infty}^y F_1(x) dx \\le \\int_{-\\infty}^y F_2(x) dx.\n\\] Then \\(X_1\\) second order stochastically dominates (SOSD) \\(X_2\\)"
  },
  {
    "objectID": "probability/probability.html#density",
    "href": "probability/probability.html#density",
    "title": "Probability",
    "section": "Density",
    "text": "Density\n\n\n\nDefinition\n\n\n\nLet \\(X\\) be a random variable with distribution \\(P_{X}\\). When \\(P_{X}\\ll \\lambda\\), we call \\(X\\) a continuous random variable, and call the Radon-Nikodym derivative \\(f\\equiv dP_{X}/d\\lambda\\) the (probability) density function of \\(P_{X}\\).\nWe say that \\(X\\) is a discrete random variable, if there exists a countable set \\(A\\subset \\mathbf{R}\\) and such that \\(P_{X}A^{c}=0\\)\n\n\n\n\n\nDensity wrt to other measures? e.g. dirac?"
  },
  {
    "objectID": "probability/probability.html#markovs",
    "href": "probability/probability.html#markovs",
    "title": "Probability",
    "section": "Markov’s",
    "text": "Markov’s\n\n\n\nMarkov’s Inequality\n\n\n\\(P(|X|>\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon > 0, k > 0\\)"
  },
  {
    "objectID": "probability/probability.html#jensens",
    "href": "probability/probability.html#jensens",
    "title": "Probability",
    "section": "Jensen’s",
    "text": "Jensen’s\n\n\n\nJensen’s Inequality\n\n\nSuppose that \\(g\\) is convex and \\(X\\) and \\(g(X)\\) are integrable, then \\(g(\\Er X) \\leq \\Er[g(X)]\\)\n\n\n\n\n\n\nExercise\n\n\nShow \\(\\Er[|X|^p] \\leq \\left(\\Er[|X|^q] \\right)^{p/q}\\) for all \\(0 < p \\leq q\\)."
  },
  {
    "objectID": "probability/probability.html#cauchy-schwarz",
    "href": "probability/probability.html#cauchy-schwarz",
    "title": "Probability",
    "section": "Cauchy-Schwarz",
    "text": "Cauchy-Schwarz\n\n\n\nCauchy-Schwarz Inequality\n\n\n\\(\\left(\\Er[XY]\\right)^2 \\leq \\Er[X^2] \\Er[Y^2]\\)"
  },
  {
    "objectID": "probability/probability.html#generated-sigma-field",
    "href": "probability/probability.html#generated-sigma-field",
    "title": "Probability",
    "section": "Generated \\(\\sigma\\)-field",
    "text": "Generated \\(\\sigma\\)-field\n\n\\(\\sigma(X)\\) is \\(\\sigma\\)-field generated by \\(X\\)\n\nsmallest \\(\\sigma\\)-field w.r.t. which \\(X\\) is measurable\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R)\\}\\)"
  },
  {
    "objectID": "probability/probability.html#information",
    "href": "probability/probability.html#information",
    "title": "Probability",
    "section": "Information",
    "text": "Information\n\n\\(\\forall E \\in \\sigma(X)\\), observing value \\(x\\) of \\(X\\), tells us whether \\(E\\) occurred\nif \\(\\sigma(X_1) \\subset \\sigma(X_2)\\), then \\(\\sigma(X_2)\\) has more information than \\(\\sigma(X_1)\\)\n\n\nExample 4.3 in Song (2021)."
  },
  {
    "objectID": "probability/probability.html#dependence",
    "href": "probability/probability.html#dependence",
    "title": "Probability",
    "section": "Dependence",
    "text": "Dependence\n\n\n\nTheorem 4.2\n\n\nSuppose \\(\\sigma(W) \\subset \\sigma(X)\\), then \\(\\exists\\) Borel measurable \\(g\\) s.t. \\(W=g(X)\\)\n\n\n\n\nProof in Çinlar (2011) chapter 2, section 4."
  },
  {
    "objectID": "probability/probability.html#independence-1",
    "href": "probability/probability.html#independence-1",
    "title": "Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\nDefinition\n\n\n\nEvents \\(A_1, ..., A_m\\) are independent if for any sub-collection \\(A_{i_1}, ..., A_{i_s}\\) \\[\nP\\left(\\cap_{j=1}^s A_{i_j}\\right) = \\prod_{j=1}^s P(A_{i_j})\n\\]\n\\(\\sigma\\)-fields, \\(\\mathscr{F}_1, .., \\mathscr{F}_m \\subset \\mathscr{F}\\) are independent if for any \\(\\mathscr{F}_{i_1}, .., \\mathscr{F}_{i_s}\\) and \\(E_j \\in \\mathscr{F}_j\\), \\[\nP\\left(\\cap_{j=1}^s E_{i_j}\\right) = \\prod_{j=1}^s P(E_{i_j})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are independent if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are independent"
  },
  {
    "objectID": "probability/probability.html#random-vectors",
    "href": "probability/probability.html#random-vectors",
    "title": "Probability",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nmeasurable \\(X: \\Omega \\to \\R^n\\)\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R^n)\\} =\\) smallest \\(\\sigma\\)-field containing \\(\\cup_{i=1}^n \\sigma(X_i)\\)\n\n\n\n\nTheorem\n\n\nSuppose that \\(X=(X_1, X_2)\\) and \\(Y=(Y_1, Y_2)\\) are independent, then \\(f(X)\\) and \\(g(Y)\\) are independent\n\n\n\n\nAs promised, an easy way to show obvious independence without any tedious calculations."
  },
  {
    "objectID": "probability/probability.html#conditional-expectation",
    "href": "probability/probability.html#conditional-expectation",
    "title": "Probability",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\nDefinition\n\n\nLet \\(\\mathscr{G} \\subset \\mathscr{F}\\) be \\(\\sigma\\)-fields, \\(Y\\) a random variable with \\(\\Er |Y| < \\infty\\), then the conditional expectation of \\(Y\\) given \\(\\mathscr{G}\\) is \\(\\Er[Y|\\mathscr{G}](\\cdot): \\Omega \\to \\R\\) s.t.\n\n\\(\\Er[Y|\\mathscr{G}](\\cdot)\\) is \\(\\mathscr{G}\\) measurable\n\\(\\int_A \\Er[Y|\\mathscr{G}] dP = \\int_A Y dP\\) \\(\\forall A \\in \\mathscr{G}\\)\n\n\n\n\n\n\nEx: \\(\\{E_k\\}_{k=1}^m\\) partition of \\(\\Omega\\), let \\(\\mathscr{G} = \\sigma(\\{E_k\\}_{k=1}^m)\\)\n\n\\(\\Er[Y | \\mathscr{G}](\\omega) = \\sum_{k=1}^m c_k 1\\{\\omega \\in E_k\\}\\)\n\nExistence from Radon-Nikodym theorem\n\\(\\Er[Y|X] \\equiv \\Er[Y|\\sigma(X)]\\)"
  },
  {
    "objectID": "probability/probability.html#properties-of-conditional-expectation",
    "href": "probability/probability.html#properties-of-conditional-expectation",
    "title": "Probability",
    "section": "Properties of Conditional Expectation",
    "text": "Properties of Conditional Expectation\n\n\nIf \\(X\\) is \\(\\mathscr{G}\\) measurable, then \\(\\Er[XY| \\mathscr{G}] = X \\Er[Y|\\mathscr{G}]\\) a.e.\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[XY|Z] = X \\Er[Y|Z]\\)\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[\\Er[Y|Z]|X] = \\Er[Y|X]\\)\nIf \\(Y\\) and \\(X\\) are independent, then \\(\\Er[Y | X ] = \\Er[Y]\\)"
  },
  {
    "objectID": "probability/probability.html#erymathscrg-as-orthogonal-projection",
    "href": "probability/probability.html#erymathscrg-as-orthogonal-projection",
    "title": "Probability",
    "section": "\\(\\Er[Y|\\mathscr{G}]\\) as Orthogonal Projection",
    "text": "\\(\\Er[Y|\\mathscr{G}]\\) as Orthogonal Projection\n\n\n\nWarning\n\n\nLet \\((\\Omega, \\mathscr{F}, P)\\) be a probability space, \\(\\mathscr{G}\\) a sub \\(\\sigma\\)-field, then for any \\(Y \\in \\mathcal{L}^2(\\Omega, \\mathscr{F}, P) = \\{X: \\Omega \\to \\mathbb{R} \\text{ s.t. } X \\text{ }\\mathscr{F}\\text{-measurable, } \\int X^2 dP < \\infty \\}\\), \\[\n\\inf_{W \\in \\mathcal{L}^2(\\Omega, \\mathscr{G}, P)} \\Er[(Y-W)^2] = \\Er[ (Y - \\Er[Y | \\mathscr{G}])^2]\n\\]"
  },
  {
    "objectID": "probability/probability.html#conditional-measure",
    "href": "probability/probability.html#conditional-measure",
    "title": "Probability",
    "section": "Conditional Measure",
    "text": "Conditional Measure\n\n\n\nDefinition\n\n\nLet \\(\\mathscr{G}\\) be a sub \\(\\sigma\\)-field of \\(\\mathscr{F}\\). Tthe conditional probability measure given \\(\\mathscr{G}\\) is defined to be a map \\(P(\\cdot \\mid \\mathscr{G})(\\cdot ):\\mathscr{F}\\times \\Omega \\rightarrow [0,1]\\) such that\n\nFor each \\(A\\in \\mathscr{F}\\), \\(P(A \\mid \\mathscr{G})(\\cdot )=\\mathbf{E}\\left[ 1\\{\\omega \\in A\\} \\mid \\mathscr{G}\\right] (\\cdot )\\), a.e.\nfor each \\(\\omega \\in \\Omega\\), \\(P(\\cdot \\mid \\mathscr{G})(\\omega )\\) is a probability measure on \\((\\Omega ,\\mathscr{F}).\\)"
  },
  {
    "objectID": "probability/probability.html#conditional-independence",
    "href": "probability/probability.html#conditional-independence",
    "title": "Probability",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\n\n\nDefinition\n\n\n\nEvents \\(A_1, ..., A_m \\in \\mathscr{F}\\) are conditionally independent given \\(\\mathscr{G}\\) if for any sub-collection, \\[\nP\\left( \\cap_{j=1}^s A_{i_j} | \\mathscr{G} \\right) = \\prod_{j=1}^s P(A_{i_j} | \\mathscr{G})\n\\]\nSub \\(\\sigma\\)-fields \\(\\mathscr{F}_1, ..., \\mathscr{F}_m\\) are conditionally independent given \\(\\mathscr{G}\\) if for any sub-collection and events, \\(E_i \\in \\mathscr{F}_i\\), \\[\nP\\left( \\cap_{j=1}^s E_{i_j} | \\mathscr{G} \\right) = \\prod_{j=1}^s P(E_{i_j} | \\mathscr{G})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are conditionally independent given \\(\\mathscr{G}\\) if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are conditionally independent given \\(\\mathscr{G}\\)"
  },
  {
    "objectID": "problemsets/01/ps01.html",
    "href": "problemsets/01/ps01.html",
    "title": "ECON 626: Problem Set 1",
    "section": "",
    "text": "Problem 2\nSong (2021) exercise 4.1.\nFor a collection \\(\\mathscr{C}\\) of sets, we write \\(\\sigma (\\mathscr{C})\\) to denote the smallest \\(\\sigma\\)-field that contains \\(\\mathscr{C}\\), and say that \\(\\sigma (\\mathscr{C})\\) is the \\(\\sigma\\)-field generated by \\(\\mathscr{C}\\).\n\nLet \\(X\\) be a random variable on \\((\\Omega ,\\mathscr{F})\\) and let \\(\\mathscr{G}\\) be the collection of the sets of the form \\(\\{\\omega \\in \\Omega :X(\\omega )\\in B\\}\\) with \\(B\\in \\mathscr{B}(\\mathbf{R})\\). Then show that \\(\\mathscr{G}\\) is a \\(\\sigma\\)-field.\nShow that \\(\\{X^{-1}(A):A\\in \\sigma (\\mathscr{C})\\}=\\sigma(\\{X^{-1}(A):A\\in \\mathscr{C}\\})\\) for any subset \\(\\mathscr{C}\\) of \\(\\mathscr{B}(\\mathbf{R})\\).\n\n\n\n\n\n\nReferences\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”\n\n\nVaart, Aad W, and Jon A Wellner. 1996. Weak Convergence and Empirical Processes. Springer."
  },
  {
    "objectID": "problemsets/02/ps02.html",
    "href": "problemsets/02/ps02.html",
    "title": "ECON 626: Problem Set 2",
    "section": "",
    "text": "Problem 1\nLet \\(\\Omega = \\{1, 2, 3, 4\\}\\), \\(\\mathscr{F} = 2^\\Omega\\), and \\(X = 1\\{\\omega > 2 \\}\\). What is \\(\\sigma(X)\\)?\n\n\nProblem 2\nLet \\(f: \\R^2 \\to \\R\\). Assume \\(f(\\cdot, y): \\R \\to \\R\\) is measurable for all \\(y : |y - y_0| < \\epsilon\\) for some \\(\\epsilon>0\\). Also assume that \\(f(x, \\dot): \\R \\to \\R\\) is differentiable at \\(y_0\\) for all \\(x \\in A \\subset \\R\\) and \\(\\left\\vert \\frac{\\partial f}{\\partial y}(x, y_0) \\right \\vert \\leq M(x)\\) for all \\(x \\in A\\) and \\(M(x)\\) is integrable. Show that \\[\n\\frac{d}{dy} \\int_A f(\\cdot, y) d\\mu \\vert_{y=y_0} = \\int_A \\frac{\\partial f}{\\partial y} f(\\cdot, y_0) d\\mu.\n\\]\n\n\nProblem 3\nLet \\(X: \\Omega \\to \\R\\), \\(W: \\Omega \\to \\R\\), \\(Z: \\Omega \\to \\R\\), and \\(D: \\Omega \\to \\{0, 1\\}\\) be random variables. Let \\(Y = D X + (1-D) W\\). Suppose that \\(Y\\), \\(D\\), and \\(Z\\) are observed, but \\(X\\) and \\(W\\) are not.\n\nSuppose \\(D\\) is independent of \\(X\\), \\(W\\). Then show that \\(\\Er[X - W]\\) is identified.\nSuppose \\(Z\\) is independent of \\(X\\) and \\(W\\), and \\(\\exists E_1, E_0 \\in \\sigma(Z)\\) such that \\(P(D=1 | E_1) = 1\\) and \\(P(D=0|E_0) = 1\\). Then show that \\(\\Er[X-W]\\) is identified."
  },
  {
    "objectID": "problemsets/03/ps03.html",
    "href": "problemsets/03/ps03.html",
    "title": "ECON 626: Problem Set 3",
    "section": "",
    "text": "Problem 1\n(This was problem 3 last week)\nLet \\(X: \\Omega \\to \\R\\), \\(W: \\Omega \\to \\R\\), \\(Z: \\Omega \\to \\R\\), and \\(D: \\Omega \\to \\{0, 1\\}\\) be random variables. Let \\(Y = D X + (1-D) W\\). Suppose that \\(Y\\), \\(D\\), and \\(Z\\) are observed, but \\(X\\) and \\(W\\) are not.\n\nSuppose \\(D\\) is independent of \\(X\\), \\(W\\). Then show that \\(\\Er[X - W]\\) is identified.\nSuppose \\(Z\\) is independent of \\(X\\) and \\(W\\), and \\(\\exists E_1, E_0 \\in \\sigma(Z)\\) such that \\(P(D=1 | E_1) = 1\\) and \\(P(D=0|E_0) = 1\\). Then show that \\(\\Er[X-W]\\) is identified.1\n\n\n\nProblem 2\nIn the linear model, \\(Y = \\alpha + \\beta X + \\epsilon\\), assume \\(\\mathrm{Var}(X)>0\\) and \\(\\Er[\\epsilon] = 0\\). Show that without any more assumptions, \\(\\beta\\) is not identified by finding values of \\(\\beta\\) that are observationally equivalent.\n\n\nProblem 3\nSong (2021) Chatper 4, exercise 1.3.\nConsider the binary choice model in Example 1.3 and assume that \\(\\tilde{\\beta}_0\\) and \\(\\tilde{\\beta}_1\\) have appropriate estimators \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). Provide a sample analogue estimator of the average derivative, replacing \\(\\tilde{\\beta}_0\\) and \\(\\tilde{\\beta}_1\\) with \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\)\n\n\n\n\n\n\nReferences\n\nHeckman, James, and Salvador Navarro-Lozano. 2004. “Using Matching, Instrumental Variables, and Control Functions to Estimate Economic Choice Models.” The Review of Economics and Statistics 86 (1): 30–57. https://doi.org/10.1162/003465304323023660.\n\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”\n\nFootnotes\n\n\nAn assumption like this is used in nonparametric selection estimators. One way to have such \\(E_1\\) and \\(E_0\\) is if \\(D = 1\\{Z \\gamma \\geq U\\}\\) and \\(Z\\) has unbounded supported. Then \\(E_0\\) and \\(E_1\\) are sets where \\(Z \\to \\pm \\infty\\). Accordingly, this is sometimes called an identification at infinity assumption. See Heckman and Navarro-Lozano (2004) for more information, and a comparison between this case, and the (conditional) independence assumption of part 1. (This footnote and that paper are extra information, not hints to help answer the question).↩︎"
  },
  {
    "objectID": "problemsets/04/ps04.html",
    "href": "problemsets/04/ps04.html",
    "title": "ECON 626: Problem Set 4",
    "section": "",
    "text": "Problem 1\nThe exponential distribution has density (with respect to Lesbegue measure) \\[\nf(X;\\lambda) = \\frac{1}{\\lambda} e^{-X/\\lambda} 1\\{X > 0\\}\n\\] for \\(\\lambda>0\\). Suppose \\(X_1, ... , X_n\\) are independently exponential\\((\\lambda)\\) distributed .\n\nShow that the maximum likelihood estimator for \\(\\lambda\\) is \\(\\hat{\\lambda}^{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i\\)\nDerive the Cramér Rao lower bound for any unbiased estimator for \\(\\lambda\\). Is \\(\\hat{\\lambda}^{MLE}\\) a minimum variance unbiased estimator?\nFind the most powerful test of size \\(\\alpha\\) for testing \\(H_0: \\lambda = \\lambda_0\\) versus \\(H_1:\\lambda = \\lambda_1\\).\n\n\n\nProblem 2\nSuppose \\(X_1, ... , X_n\\) are independently uniformly distributed on \\((0,\\theta)\\).\n\nShow that \\(2 \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator for \\(\\theta\\).\nDerive the Cramér Rao lower bound for any unbiased estimator for \\(\\theta\\). Is \\(2 \\bar{X}\\) a minimum variance unbiased estimator?\nShow that \\(\\hat{\\theta} = \\frac{n+1}{n} \\max_{1 \\leq i \\leq n} X_i\\) is a minimum variance unbiased estimator for \\(\\theta\\).\n\n\n\nProblem 3"
  },
  {
    "objectID": "projection/projection.html#reading",
    "href": "projection/projection.html#reading",
    "title": "Least Squares as a Projection",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 5 (which is the basis for these slides)\nSupplemental: Schrimpf (2018), Schrimpf (2013)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\]"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces",
    "href": "projection/projection.html#orthogonal-subspaces",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\\(V \\subseteq \\R^n\\), inner product space\n\\(L \\subset V\\) a subspace\n\n::: {.callout-tip icon=false\nDefinition\nAn \\(L \\subset V\\) is a subspace if \\(\\forall x, y \\in L\\), \\(\\alpha, \\beta \\in \\R\\), \\(\\alpha x + \\beta y \\in L\\)\n:::"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces-1",
    "href": "projection/projection.html#orthogonal-subspaces-1",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\\(V \\subseteq \\R^n\\), inner product space\n\\(L \\subset V\\) a subspace\n\n::: {.callout-tip icon=false\nDefinition\nGiven a subspace \\(L \\subset V\\) the orthogonal complement of \\(L\\) is \\[\nL^\\perp = \\{x \\in V: x' l = 0 \\,\\forall l \\in L\\}\n\\]\n:::\n\nFor any \\(y \\in V\\), \\(\\exists y_1 \\in L\\), \\(y_2 \\in L^\\perp\\) s.t. \\(y = y_1 + y_2\\)"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces-2",
    "href": "projection/projection.html#orthogonal-subspaces-2",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\n\nLemma 1.1\n\n\nLet \\(L_1\\) and \\(L_2\\) be subspaces of \\(V\\), then \\[\n(\\underbrace{L_1 + L_2}_{\\{x \\in V: x = l_1 + l_2 \\text{ for some } l_1 \\in L_2, l_2 \\in L_2\\}})^\\perp = L_1^\\perp \\cap L_2^\\perp\n\\] and \\[\n(L_1 \\cap L_2)^\\perp = L_1^\\perp + L_2^\\perp\n\\]"
  },
  {
    "objectID": "projection/projection.html#projection",
    "href": "projection/projection.html#projection",
    "title": "Least Squares as a Projection",
    "section": "Projection",
    "text": "Projection"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Sep 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus626.html",
    "href": "syllabus626.html",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "",
    "text": "Paul Schrimpf\n107 Iona\npaul.schrimpf@ubc.ca\nOffice Hours: Wednesday 11:30am-12:30pm\n\n\n\nYige Duan\nyige.duan@gmail.com\nOffice Hours: TBA\n\n\n\nMondays and Wednesdays 10:00am - 11:20am, Iona 633\n\n\n\nOur primary textbook will be notes written by Kyunchul Song. You can find them on Canvas. Other recommended (but not required) textbooks include:\n\nBruce Hansen (2022) Probability and Statistics for Economists and Econometrics\nFumio Hayashi (2000) Econometrics\nGeorge Casella and Roger L. Berger (2002) Statistical Inference\nTakeshi Amemiya (1985) Advanced Econometrics"
  },
  {
    "objectID": "syllabus626.html#grading",
    "href": "syllabus626.html#grading",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Grading",
    "text": "Grading\n\nProblem Sets (30%)\nThere will be 6-9 problem sets. A problem set with the lowest grade will be dropped from final grading.\n\n\nMidterm Exam (30%)\nGiven in class on Wednesday, October 26. Closed books and notes. There will be no make-up exam for midterm. When a student is excused from midterm exam, his or her grade will be based on reweighting of the problem sets (40%) and the final exam (60%).\n\n\nFinal Exam (40%)\nClosed books and notes."
  },
  {
    "objectID": "syllabus626.html#part-i-basics",
    "href": "syllabus626.html#part-i-basics",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part I: Basics",
    "text": "Part I: Basics\n\nProbability\n\nRandom Variables, and Distributions\nConditional Expectations and Conditional Distributions\nFamily of Distributions\n\nBasics of Inference - Estimation - Hypothesis Testing"
  },
  {
    "objectID": "syllabus626.html#part-ii-generalized-linear-model",
    "href": "syllabus626.html#part-ii-generalized-linear-model",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part II: Generalized Linear Model",
    "text": "Part II: Generalized Linear Model\n\nPreliminaries of Projection Geometry - Generalized Linear Models and Gauss-Markov Theorem - Tests of Linear Hypothesis"
  },
  {
    "objectID": "syllabus626.html#part-iii-tools-of-asymptotic-theory",
    "href": "syllabus626.html#part-iii-tools-of-asymptotic-theory",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part III: Tools of Asymptotic Theory",
    "text": "Part III: Tools of Asymptotic Theory\n\nModes of Convergence\nSlutsky’s Lemma/Continuous Mapping Theorem\nDelta Methods"
  },
  {
    "objectID": "syllabus626.html#part-v-linear-models-with-endogeneity",
    "href": "syllabus626.html#part-v-linear-models-with-endogeneity",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part V: Linear Models with Endogeneity",
    "text": "Part V: Linear Models with Endogeneity\n\nIdentification and Endogeneity\nIdentification through IV and Inference\n\n\n\n\n\n\n\nUBC values and policies\n\n\n\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here (https://senate.ubc.ca/policiesresources-support-student-success )"
  }
]