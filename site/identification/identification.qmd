---
title: "Identification"
author: "Paul Schrimpf"
date: last-modified
format:
  revealjs:
    theme: moon
    width: 1600
    height: 900
    min-scale: 0.1
    chalkboard:
      chalk-width: 6
      chalk-effect: 0.5
    title-slide-attributes:
      data-background-image: "tricouni.jpg"
      data-background-size: contain
    mermaid:
      theme: dark
bibliography: ../626.bib

# jupyter: julia-1.9
# execute:
#   cache: false
---


## Reading

- Required: **@song2021 chapter 4** (which is the basis for these slides)
- Recommended: @lewbel2019
- Supplementary: @matzkin2013, @molinari2020 , @imbens2020

$$
\def\Er{{\mathrm{E}}}
\def\cov{{\mathrm{Cov}}}
\def\var{{\mathrm{Var}}}
\def\R{{\mathbb{R}}}
$$


::: {.notes}

Motivate by wanting to go from data to parameters ...
:::

# Identification

::: {.callout-tip icon=false}

### Definition

Let $X$ be an observed random vector with distribution $P_X$. Let
$\mathcal{P}$ be a**probability model** --- a collection of
probabilities such that $P_X \in \mathcal{P}$. Then $\theta_0 \in
\R^k$ is **identified** in $\mathcal{P}$ if there exists a known
$\psi: \mathcal{P} \to \R^k$ s.t.

$$
\theta_0 = \psi(P_X)
$$

:::

# Examples

## Example: Descriptive Statistics

::: {.incremental}

- $\theta_0 =$ mean of $X$, then $\theta_0$ is identified by
$$
\psi_\mu(P) = \int x dP(x)
$$
in $\mathcal{P} = \{P : \int x dP(x) < \infty \}$

- Generally, descriptive statistics identified in a broad probability model with just regularity restrictions to ensure the statistics exist

:::

## Example: Linear Model

$$
Y = \alpha + \beta X + \epsilon
$$

- $\mathcal{P} = \{P_{X,Y}:$ $Y=\alpha + \beta X + \epsilon$,

    1. $| \mathrm{Cov}(X,Y) | < \infty$, $0 < \mathrm{Var}(X) < \infty$

    2.  $\mathrm{Cov}(X, \epsilon)  = 0$ $\}$

- $\beta$ identified as

$$
\beta = \frac{\int (x - \Er X) (y - \Er Y ) dP_{X,Y}(x,y)}
{\int (x - \Er X)^2 dP_{X}(x)}  = \frac{ \cov(X,Y) }{ \var(X) }
$$

::: {.fragment}

- Identification requires:
    1. Usually innocuous regularity conditions
    2. **Substantive exogeneity restriction**

- Evaluating plausibility of exogeneity restrictions requires a priori knowledge of data context and related economic theory

:::

::: {.notes}

Examples of production function and returns to schooling here?

2 is more difficult to get right and more context dependent, so we will first tackle the easier problem of 1.

:::

## Example: Multiple Regression

$$
Y = X'\beta + \epsilon
$$

- $\mathcal{P} = \{P: \Er X \epsilon = 0, \Er X X' \text{ invertible} \}$



## Example: Binary Choice

$$
Y = 1\{ \beta_0 + \beta_1 X > u \}
$$

- $\mathcal{P} = \{P: u \sim N(0,1), 0< \var(X) < \infty \}$


::: {.fragment}

- Is $u \sim N(0,1)$ innocuous?

:::

::: {.notes}

More generally, think about importance of statistical assumptions in terms of how they affect identifiable quantities and counterfactuals of interest.

:::

## Example: Potential Outcomes

- Data:
    - Treatment $D_i$
    - Potential outcomes $(Y_{i,0}, Y_{i,1})$, observed outcome $Y_i = D_i Y_{i,1} + (1-D_i) Y_{i,0}$
    - Covarites $X_i$

- Parameter: $\theta_0 = \Er[Y_{i,1} - Y_{i,0}] =$ average treatment effect

- Assume:
    1. Unconfoundedness: $(Y_{i,0}, Y_{i,1})$ conditionally independent of $D_i$ given $X_i$
    2. Overlap: $\epsilon < P(D=1|X=x) < 1-\epsilon$ for  some $\epsilon > 0$ and all $x$

# Causal Diagrams

## Causal Diagrams

- Originate with Wright around 1920, e.g. @wright1934
- Recently advocated by Pearl, e.g. @pearl2015, @pearl2018
- Recommended introduction @imbens2020
- Sometimes useful expository tool for explaining identifying restriction, but should not be your only or primary approach
     - e.g. @chernozhukov2021

## Example: Regression

<!-- ```{r, engine = 'tikz'} -->
<!--  \usetikzlibrary{positioning} -->
<!--  \begin{tikzpicture}[-latex, auto, node distance=1.5cm and 2cm, on grid, thick, scale=10, -->
<!--                      obs/.style={circle, draw,black, text=black}] -->
<!--  \node[obs] (X){$X_i$}; -->
<!--  \node[obs] (Y) [right=of X] {$Y_i$}; -->
<!--  \node(E) [below=of Y]{$\epsilon_i$}; -->
<!--  \path[->] (X) edge (Y); -->
<!--  \path[->] (E) edge[dashed] (Y); -->
<!--  \end{tikzpicture} -->
<!-- ``` -->

```{mermaid}
%%| fig-width: 6.5
%%| fig-height: 2
flowchart LR
    X --> Y
    系 -.-> Y
```
- Arrow means $X$ causes $Y$
- Dashed arrow means $\epsilon$ causes $Y$ and is unobserved
- Lack of connection between $X$ and $\epsilon$ means they are independent

## Example: Regression

If you believe:
```{mermaid}
%%| fig-width: 6.5
flowchart LR
    subgraph " Y"
        Y[Wage]
    end
    subgraph X
       E[Education] --> Y
       T[SAT] --> E
       F[Family SES] --> E
       F --> Y
    end
    subgraph 系
        I[Intelligence] -.-> Y
        L[Luck] -.-> Y
        I -.-> T
    end
```
then regression $Wage = \beta_1 Education + \beta_2 SAT + \beta_3 FamilySES + \epsilon$ identifies causal effect of education on wages

## Example: Regression

But reality is likely more complex ...

```{mermaid}
%%| fig-width: 6.5
flowchart LR
   subgraph " Y"
        Y[Wage]
   end
   subgraph X
       E[Education] --> Y
       T[SAT] --> E
       F[Family SES] --> E
       F --> Y
   end
   subgraph eps
        I[Intelligence] -.-> Y
        L[Luck] -.-> Y
        G[Grit] -.-> Y
        G -.-> E
        L -.-> E
        I -.-> T
        I -.-> E
        I -.-> G
        G -.-> I
   end
```

## Example: Potential Outcomes

```{mermaid}
%%| fig-width: 6.5
flowchart LR
    u -.-> D
    X --> Y
    X --> D
    系 -.-> Y
```

## Example: Potential Outcomes

```{mermaid}
%%| fig-width: 6.5
flowchart LR
    subgraph Treatment
        D[Naloxone distribution site opens]
    end
    subgraph Outcome
        Y[ER visits for overdose]
    end
    u -.-> D
    系 -.-> Y
    subgraph X
        Income --> Y
        Unemployment --> Y
        Income --> D
        Unemployment --> D
        OD[OD rate prior to opening] --> Y
        OD --> D
        Crime --> Y
        Crime --> D
    end
```

# Generalized and/or Descriptive Intrepretation of Population Estimator

- Analyze familiar estimator under more general assumptions
    - Understand bias when exogeneity assumptions fail
    - Sometimes give more general interpretation of existing estimator

## Example: Regression

- In linear model $Y_i = X_i'\beta + \epsilon_i$, if just assume $\Er X X'$ invertible,
- Population regression
$$
   \begin{align*}
   \theta = & \Er[ X X']^{-1} \Er[ X Y]  \\
   = & \Er[X X']^{-1} \Er[X (X' \beta + \epsilon)] \\
   = & \beta + \Er[X X']^{-1} \Er[X\epsilon]
   \end{align*}
$$

## Example: Regression

- If relevant moments exist (no linear model required) population regression solves
$$
 \Er[ X X']^{-1} \Er[ X Y]  \in \mathrm{arg}\min_b \Er[ (X'b - \Er[Y|X])^2 ]
$$

## Example: Potential Outcomes

-  Matching initially studied with a linear regression model, e.g. @cochran1953
$$
Y_i = \alpha D_i + X_i' \beta + \epsilon_i
$$
- Implies constant treatment effect $Y_{i,1} - Y_{i,0} = \alpha$

::: {.notes}

The LATE interpretation of IV developed along similar lines -- replace
a constant treatment effect linear model with a heterogenous effect
potential outcomes framework, and see what IV does.

Similarly, the recent literature on difference in differences with
staggered treatment timing arises from moving from a model with a
constant treatment effect over time to one where treatment effects
vary with time.

:::

# Non-constructive Identification

## Non-constructive Identification

- Identification sometimes defined without explicit mapping from data to parameters, e.g. @hsiao1983, @matzkin2007

::: {.callout-tip icon=false}

### Definition: Observationally Equivalent

- Let $\mathcal{P} = \{ P(\cdot; s) : s \in S \}$,  two structures $s$ and $\tilde{s}$ in $S$ are **observationally equivalent**  if they imply the same distribution for the observed data, i.e.
$$ P(B;s) = P(B; \tilde{s}) $$
for all $B \in \sigma(X)$.

- Let $\lambda: S \to \R^k$, $\theta$ is **observationally equivalent** to
$\tilde{\theta}$ if $\exists s, \tilde{s} \in S$  that are
observationally equivalent and $\theta = \lambda(s)$ and
$\tilde{\theta} = \lambda(\tilde{s})$
    - Let $\Gamma(\theta, S) = \{P(\dot; s) | s \in S, \theta = \lambda(s) \}$, then
    $\theta$ and $\tilde{\theta}$ are observationally equivalent iff
    $\Gamma(\theta,S) \cap \Gamma(\tilde{\theta}, S) \neq \emptyset$

:::

## Non-constructive Identification

::: {.callout-tip icon=false}

### Definition: (Non-Constructive) Identification

- $s_0 \in S$ is **identified** if there is no $s$ that is observationally equivalent to $s_0$

- $\theta_0$ is **identified** (in $S$) if there is no observationally equivalent $\theta \neq \theta_0$
    - i.e. $\Gamma(\theta_0, S) \cap \Gamma(\theta, S) = \emptyset$ $\forall \theta \neq \theta_0$

:::

- Compared to constructive definition with $\theta_0 = \psi(P)$:
    - Less clear how to use identification to estimate
    - Easier to show non-identification


## Example: Multiple Regression

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
$$

::: {.incremental}

- $X = [X_1\, X_2]'$, if rank $\Er X X' = 1$, then $\beta_1, \beta_2$
is observationally equivalent to any $\tilde{\beta}_1, \tilde{\beta}_2$ s.t.
$$
\tilde{\beta}_1 + \tilde{\beta}_2 = \beta_1 + \beta_2 \frac{\cov(X_1, X_2)}{\var(X_2)}
$$

- $\theta_0 = \lambda( \beta ) = \beta_1 + \beta_2$ is identified if
rank $\Er X X' \geq 1$

:::

## Example: Random Coefficients Logit

- $Y_i = 1\{\beta_0 + \beta_i X_i  \geq U_i \}$
    - $U$ independent $X_i,\beta_i$,
    - $\beta_i$ indepedent $X_i$,
    - $F_u(z) = \frac{e^z}{1+e^z}$

- $\Er[Y|X] = \int \frac{e^{\beta_0 + \beta X_i}} {1+e^{\beta_0 + \beta X_i}} dF_\beta(\beta)$

- Non-constructive and constructive identification of $F_\beta$ in @fox2012

::: {.notes}

Hard to give a brief example where non-constructive argument is
required. Non constructive identification proofs typically leverage
some high level mathematical result.

@christensen2015 is another example of non-constructive identification.

:::

# References
