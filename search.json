[
  {
    "objectID": "syllabus626.html",
    "href": "syllabus626.html",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "",
    "text": "Paul Schrimpf\n112 Iona\npaul.schrimpf@ubc.ca\nOffice Hours: Wednesday 12:30pm-1:30pm\n\n\n\nYige Duan\nyige.duan@gmail.com\nOffice Hours: Monxday 2-3pm\n\n\n\nLecture: Mondays and Wednesdays 11:00am - 12:20pm, Iona 633\nDiscussion: Friday 1:30pm - 2:50pm, Buchanan D209\n\n\n\nOur primary textbook will be notes written by Kyunchul Song. You can find them on Canvas. Other recommended (but not required) textbooks include:\n\nBruce Hansen (2022) Probability and Statistics for Economists and Econometrics\nFumio Hayashi (2000) Econometrics\nGeorge Casella and Roger L. Berger (2002) Statistical Inference\nTakeshi Amemiya (1985) Advanced Econometrics"
  },
  {
    "objectID": "syllabus626.html#basic-information",
    "href": "syllabus626.html#basic-information",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "",
    "text": "Paul Schrimpf\n112 Iona\npaul.schrimpf@ubc.ca\nOffice Hours: Wednesday 12:30pm-1:30pm\n\n\n\nYige Duan\nyige.duan@gmail.com\nOffice Hours: Monxday 2-3pm\n\n\n\nLecture: Mondays and Wednesdays 11:00am - 12:20pm, Iona 633\nDiscussion: Friday 1:30pm - 2:50pm, Buchanan D209\n\n\n\nOur primary textbook will be notes written by Kyunchul Song. You can find them on Canvas. Other recommended (but not required) textbooks include:\n\nBruce Hansen (2022) Probability and Statistics for Economists and Econometrics\nFumio Hayashi (2000) Econometrics\nGeorge Casella and Roger L. Berger (2002) Statistical Inference\nTakeshi Amemiya (1985) Advanced Econometrics"
  },
  {
    "objectID": "syllabus626.html#grading",
    "href": "syllabus626.html#grading",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Grading",
    "text": "Grading\n\nProblem Sets (30%)\nThere will be 6-9 problem sets. A problem set with the lowest grade will be dropped from final grading.\n\n\nMidterm Exam (30%)\nGiven in class on Wednesday, October 25. Closed books and notes. There will be no make-up exam for midterm. When a student is excused from midterm exam, his or her grade will be based on reweighting of the problem sets (30%) and the final exam (70%).\n\n\nFinal Exam (40%)\nClosed books and notes."
  },
  {
    "objectID": "syllabus626.html#part-i-basics",
    "href": "syllabus626.html#part-i-basics",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part I: Basics",
    "text": "Part I: Basics\n\nProbability\n\nRandom Variables, and Distributions\nConditional Expectations and Conditional Distributions\nFamily of Distributions\n\nBasics of Inference - Estimation - Hypothesis Testing"
  },
  {
    "objectID": "syllabus626.html#part-ii-generalized-linear-model",
    "href": "syllabus626.html#part-ii-generalized-linear-model",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part II: Generalized Linear Model",
    "text": "Part II: Generalized Linear Model\n\nPreliminaries of Projection Geometry\nGeneralized Linear Models and Gauss-Markov Theorem\nTests of Linear Hypothesis"
  },
  {
    "objectID": "syllabus626.html#part-iii-tools-of-asymptotic-theory",
    "href": "syllabus626.html#part-iii-tools-of-asymptotic-theory",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part III: Tools of Asymptotic Theory",
    "text": "Part III: Tools of Asymptotic Theory\n\nModes of Convergence\nSlutsky’s Lemma/Continuous Mapping Theorem\nDelta Methods"
  },
  {
    "objectID": "syllabus626.html#part-v-linear-models-with-endogeneity",
    "href": "syllabus626.html#part-v-linear-models-with-endogeneity",
    "title": "Syllabus for ECON 626: Econometric Theory I",
    "section": "Part V: Linear Models with Endogeneity",
    "text": "Part V: Linear Models with Endogeneity\n\nIdentification and Endogeneity\nIdentification through IV and Inference\n\n\n\n\n\n\n\nStudent Success\n\n\n\n\n\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here: https://senate.ubc.ca/policies-resources-support-student-success/.\n\n\n\n\n\n\n\n\n\nPolicy on Academic Honesty\n\n\n\n\n\nIt is the policy of the Vancouver School of Economics to report all violations of UBC’s standards for academic integrity to the office of the Dean of Arts. All violations of academic integrity standards will result in a grade of zero on the relevant assessment (exam, paper, assignment etc.). Students who do not have a previous offence may have the option to enter into a diversionary process with the Dean of Arts to resolve their misconduct (https://academicintegrity.ubc.ca/diversionary-process/). Any student who has a previous academic offence will be referred to the President’s Advisory Committee on Student Discipline (PACSD) (https://universitycounsel.ubc.ca/homepage/guides-and-resources/discipline/). PACSD may impose additional penalties including: a transcript notation indicating that the student has committed an academic offence, zero in the course, and/or suspension or expulsion from the University. You are personally responsible for understanding and following the UBC’s policies for academic integrity: https://vancouver.calendar.ubc.ca/campus-wide-policies-and-regulations/academic-honesty-and-standards.\n\n\n\n\n\n\n\n\n\nPolicy on Academic Concessions\n\n\n\n\n\nThere are only three acceptable grounds for academic concessions at UBC: unexpected changes in personal responsibilities that create a schedule conflict; medical circumstances; and compassionate grounds when the student experiences a traumatic event, sexual assault, or death in the family or of a close friend. Academic concessions for graded work and exams are granted for work that will be missed due to unexpected situations or circumstances. Situations that are expected (such as time constraints due to workload in other courses) or are predictable (such as being scheduled for paid work) are not grounds for academic concession.\nRequests for academic concessions should be made before the due date for that graded work and/or the writing of the exam. UBC policy does not allow for concessions to students who have missed work because they have registered for a course after the due date for that work. You can read more about the rules for academic concessions here: https://students.ubc.ca/enrolment/academic-learning-resources/academic-concessions. Students in the Faculty of Arts who require a concession can apply for concessions using this form here: https://students.air.arts.ubc.ca/academic-concession-form/. Students in other Faculties should consult their faculty website on academic concessions. Please note that the role of the faculty advising office is to review the evidence and to either support or not support concession requests. The final decision to grant the request always rests with your instructor.\n\n\n\n\n\n\n\n\n\nPolicy on the Use of AI Learning Tools\n\n\n\n\n\nThe use of AI tools is permitted except where explicitly forbidden. However, that usage must be documented and attributed within your assessment(s). Students are responsible for all factual inaccuracies that are created by the use of AI tools."
  },
  {
    "objectID": "projection/projection.html#reading",
    "href": "projection/projection.html#reading",
    "title": "Least Squares as a Projection",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 5 (which is the basis for these slides)\nSupplemental: Schrimpf (2018), Schrimpf (2013b), Schrimpf (2013a), Schrimpf (2013d), Schrimpf (2013c)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\]"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces",
    "href": "projection/projection.html#orthogonal-subspaces",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\\(V \\subseteq \\R^n\\), inner product space\n\\(L \\subset V\\) a subspace\n\n\n\n\nDefinition\n\n\nAn \\(L \\subset V\\) is a subspace if \\(\\forall x, y \\in L\\), \\(\\alpha, \\beta \\in \\R\\), \\(\\alpha x + \\beta y \\in L\\)"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces-1",
    "href": "projection/projection.html#orthogonal-subspaces-1",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\\(V \\subseteq \\R^n\\), inner product space\n\\(L \\subset V\\) a subspace\n\n\n\n\nDefinition\n\n\nGiven a subspace \\(L \\subset V\\) the orthogonal complement of \\(L\\) is \\[\nL^\\perp = \\{x \\in V: x' l = 0 \\,\\forall l \\in L\\}\n\\]\n\n\n\n\nFor any \\(y \\in V\\), \\(\\exists y_1 \\in L\\), \\(y_2 \\in L^\\perp\\) s.t. \\(y = y_1 + y_2\\)"
  },
  {
    "objectID": "projection/projection.html#orthogonal-subspaces-2",
    "href": "projection/projection.html#orthogonal-subspaces-2",
    "title": "Least Squares as a Projection",
    "section": "Orthogonal Subspaces",
    "text": "Orthogonal Subspaces\n\n\n\nLemma 1.1\n\n\nLet \\(L_1\\) and \\(L_2\\) be subspaces of \\(V\\), then \\[\n(\\underbrace{L_1 + L_2}_{\\{l_1 + l_2 \\in V:  l_1 \\in L_2, l_2 \\in L_2\\}})^\\perp = L_1^\\perp \\cap L_2^\\perp\n\\] and \\[\n(L_1 \\cap L_2)^\\perp = L_1^\\perp + L_2^\\perp\n\\]"
  },
  {
    "objectID": "projection/projection.html#projection",
    "href": "projection/projection.html#projection",
    "title": "Least Squares as a Projection",
    "section": "Projection",
    "text": "Projection\n\n\n\nDefinition\n\n\n\\(P_L y \\in L\\) is the projection of \\(y\\) on \\(L\\) if \\[\n\\norm{y - P_L y } = \\inf_{w \\in L} \\norm{y - w}\n\\]\n\n\n\n\n\n\n\nProjection Theorem\n\n\n\n\\(P_L y\\) exists, is unique, and is a linear function of \\(y\\)\nFor any \\(y_1^* \\in L\\), \\(y_1^* = P_L y\\) iff \\(y- y_1^* \\perp L\\)\n\n\n\n\n\n2 implies if \\(y = y_1 + y_2\\) with \\(y_1 \\in L\\) and \\(y_2 \\in L^\\perp\\), then \\(y_1 = P_L y\\)"
  },
  {
    "objectID": "projection/projection.html#projection-map",
    "href": "projection/projection.html#projection-map",
    "title": "Least Squares as a Projection",
    "section": "Projection Map",
    "text": "Projection Map\n\n\n\nTheorem 1.2\n\n\nA linear map \\(G: V \\to L\\) is the projection map onto \\(L\\) iff \\(Gy = y\\) \\(\\forall y \\in L\\) and \\(Gy = 0\\) \\(\\forall y \\in L^\\perp\\)"
  },
  {
    "objectID": "projection/projection.html#projection-map-1",
    "href": "projection/projection.html#projection-map-1",
    "title": "Least Squares as a Projection",
    "section": "Projection Map",
    "text": "Projection Map\n\n\n\nDefinition\n\n\nLinear \\(G: V \\to V\\) is\n\nidempotent if \\(G (G y) = G y\\) \\(\\forall y \\in V\\)\nsymmetric if \\(G'y = G y\\) \\(\\forall y \\in V\\)\n\n\n\n\n\n\n\nTheorem 1.3\n\n\nA linear map \\(G: V \\to V\\) is a projection map onto its range, \\(\\mathcal{R}(G)\\), iff \\(G\\) is idempotent and symmetric."
  },
  {
    "objectID": "projection/projection.html#projection-differences",
    "href": "projection/projection.html#projection-differences",
    "title": "Least Squares as a Projection",
    "section": "Projection Differences",
    "text": "Projection Differences\n\n\n\nTheorem 1.4\n\n\nLet \\(L \\subset V\\) and \\(L_0 \\subset L\\) be subspaces. Then \\(P_L - P_{L_0} = P_{L \\cap L_0^\\perp}\\)"
  },
  {
    "objectID": "projection/projection.html#projection-onto-x",
    "href": "projection/projection.html#projection-onto-x",
    "title": "Least Squares as a Projection",
    "section": "Projection onto \\(X\\)",
    "text": "Projection onto \\(X\\)\n\n\n\nDefinition\n\n\nFor linear \\(H: \\R^s \\to \\R^r\\), the g-inverse of \\(H\\) is any \\(H^{-}\\) s.t. \\(H H^{-} H = H\\)\n\n\n\n\n\n\n\nTheorem 1.5\n\n\nLet \\(X: \\R^k \\to \\R^n\\) be linear. The projection onto \\(\\mathcal{R}(X)\\) is \\(P_X = X(X'X)^- X'\\) where \\((X'X)^{-}\\) is any g-inverse of \\(X'X\\)"
  },
  {
    "objectID": "projection/projection.html#projection-spectrum",
    "href": "projection/projection.html#projection-spectrum",
    "title": "Least Squares as a Projection",
    "section": "Projection Spectrum",
    "text": "Projection Spectrum\n\n\n\nDefinition\n\n\nLet \\(A: V \\to V\\) be linear. Then \\(\\lambda\\) is an eigenvalue of \\(A\\) and \\(v \\neq 0\\) is an associated eigenvector if \\(A v = \\lambda v\\)\n\n\n\n\nExistence of (possibly complex) eigenvalues as roots of characteristic polynomial\n\n\n\n\nLemma 1.2\n\n\nThe eigenvalues of a symmetric and idempotent matrix, \\(P\\) are either \\(0\\) or \\(1\\). Furthmore rank of \\(P\\) is the sum of its eigenvalues."
  },
  {
    "objectID": "projection/projection.html#projection-rank",
    "href": "projection/projection.html#projection-rank",
    "title": "Least Squares as a Projection",
    "section": "Projection Rank",
    "text": "Projection Rank\n\n\n\nTheorem 1.6\n\n\n\n\\(\\mathrm{rank}(P_X) = \\mathrm{rank}(X)\\)\n\\(\\rank(I-P_X) = n - \\rank(X)\\)"
  },
  {
    "objectID": "projection/projection.html#generalized-linear-model-1",
    "href": "projection/projection.html#generalized-linear-model-1",
    "title": "Least Squares as a Projection",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\n\\[\nY = \\theta + u\n\\]\n\n\\(\\theta \\in L \\subset \\R^n\\), \\(L\\) a known subspace\n\\(u \\in \\R^n\\) unobserved\n\n\n\nExample: \\[\nY_i = x_{i,1} \\beta_1 + \\cdots + x_{i,k} \\beta_k + u_i\n\\]\n\n\\(X_k \\equiv (x_{1,k}, ... , x{n,k})'\\), \\(X \\equiv(X_1, ..., X_k)\\), \\(\\beta \\equiv (\\beta_1, ..., \\beta_k)'\\), \\(y \\equiv (Y_1, ..., Y_n)'\\), \\(u \\equiv (u_1, ..., u_n)'\\) \\[y= X\\beta + u\\] fits setup with \\(L = \\mathcal{R}(X)\\)"
  },
  {
    "objectID": "projection/projection.html#least-squares",
    "href": "projection/projection.html#least-squares",
    "title": "Least Squares as a Projection",
    "section": "Least-Squares",
    "text": "Least-Squares\n\n\\(\\hat{\\theta} = P_L y\\), i.e. \\(\\norm{y - \\hat{\\theta} y } = \\inf_{w \\in L} \\norm{y - w}\\)"
  },
  {
    "objectID": "projection/projection.html#gauss-markov-theorem-1",
    "href": "projection/projection.html#gauss-markov-theorem-1",
    "title": "Least Squares as a Projection",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\n\n\nTheorem: Gauss-Markov\n\n\nIf \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the best linear unbiased estimator (BLUE) of \\(a'\\theta = a'\\hat{\\theta}\\) where \\(\\hat{\\theta} = P_L y\\)\n\n\n\n\n\n\n\nCorollary\n\n\nIf \\[\ny = X'\\beta + u\n\\] and \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the BLUE of \\(c'\\beta\\) is \\(c'\\hat{\\beta}\\) with \\(\\hat{\\beta} = (X'X)^{-1} X' y\\)"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html",
    "href": "problemsets/midterm2022/midterm2022_solution.html",
    "title": "ECON 626: Midterm Solutions",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#identification",
    "href": "problemsets/midterm2022/midterm2022_solution.html#identification",
    "title": "ECON 626: Midterm Solutions",
    "section": "Identification",
    "text": "Identification\n\nIf \\(m\\) is strictly increasing show that \\(\\beta\\) is identified by explicitly writing \\(\\beta\\) as a function of the distribution of \\(X\\) and \\(Y\\).\n\n\nSolution. Since \\(m\\) is increasing, \\(m^{-1}\\) exists. We can then identify \\(\\beta\\) as \\[\n\\begin{aligned}\n\\beta = & \\Er[X_iX_i']^{-1} \\Er[X_i m^{-1}(Y_i)] \\\\\n= & \\Er[X_iX_i']^{-1} \\Er[X_i (X_i' \\beta + u_i)]  = \\beta\n\\end{aligned}\n\\]\n\n\nSuppose \\(m(z) = 1\\{z \\geq 0\\}\\). For simplicitly, let \\(k=1\\) and \\(X_i \\in \\{-1, 1\\}\\). Show that \\(\\beta\\) is not identified by finding an observationally equivalent \\(\\tilde{\\beta}\\).\n\n\nSolution. Since \\(Y \\in \\{0,1\\}\\) and \\(P(Y=0|X) = 1-P(Y=1|X)\\), just looking at \\(P(Y=1|X)\\) completely describes \\(P(Y|X)\\). For any \\(\\beta\\), we have \\[\n\\begin{aligned}\nP_{Y,X}(\\dot|\\beta, F_u) = & P(Y=1|X;\\beta, F_u) P(X) \\\\\n= & P(X\\beta + u \\geq 0 | X; F_u) P(X) \\\\\n= & \\left[ 1 - F_u(-X\\beta) \\right] P(X) \\\\\n= & \\begin{cases} \\left[ 1 - F_u(\\beta) \\right] P(X=-1) & \\text{ if} X=-1 \\\\\n\\left[ 1 - F_u(-\\beta) \\right] P(X=1) & \\text{ if} X=1\n\\end{cases}\n\\end{aligned}\n\\] If \\(\\beta \\neq 0\\), \\(\\beta\\) is observationally equivalent to \\(\\tilde{\\beta} = s \\beta\\) for any \\(s \\neq 0\\) with \\(\\tilde{u} = s u\\). The even \\(X\\beta + u \\geq 0\\) is not affected by multiplying by \\(s\\), so \\(P(X\\beta + u \\geq 0 | X; F_u) P(X) = P(X\\tilde{\\beta} + \\tilde{u} \\geq 0 | X; F_\\tilde{u}) P(X)\\).\nIf \\(\\beta = 0\\), then \\(\\beta\\) is observationally equivalent to any \\(\\tilde{\\beta}&gt;0\\) (also negative ones with an appropriate modified \\(\\tilde{u}\\)) and \\(\\tilde{u}\\) with \\[\nF_{\\tilde{u}}(x) = \\begin{cases} F_u(x + \\tilde{\\beta}) & \\text{ if } x &lt; -\\tilde{\\beta} \\\\\nF_u(0) & \\text{ if } -\\tilde{\\beta} &lt; x &lt; \\tilde{\\beta} \\\\\nF_u(x-\\tilde{\\beta}) & \\text{ if } \\tilde{\\beta} &lt; x \\\\\n\\end{cases}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#estimation",
    "href": "problemsets/midterm2022/midterm2022_solution.html#estimation",
    "title": "ECON 626: Midterm Solutions",
    "section": "Estimation",
    "text": "Estimation\nConstruct a sample analogue estimator for \\(\\beta\\) based on your answer to 1.i.1.1 Show whether your estimator is unbiased.\n\nSolution. The sample analogue estimator is \\[\n\\hat{\\beta} = \\left( \\sum_{i=1}^n X_i X_i' \\right)^{-1} \\left(\\sum_{i=1}^n X_i m^{-1}(Y_i) \\right)\n\\] It is unbiased because \\[\n\\begin{aligned}\n\\Er[\\hat{\\beta}] & = \\Er\\left[\\left( \\sum_{i=1}^n X_i X_i'\\right)^{-1} (\\sum_{i=1}^n X_i m^{-1}(Y_i) ) \\right] \\\\\n& = \\Er\\left[\\left( \\sum_{i=1}^n X_i X_i'\\right)^{-1} \\sum_{i=1}^n X_i (X_i' \\beta + u_i) \\right] \\\\\n& = \\beta + \\Er\\left[\\left( \\sum_{i=1}^n X_i X_i'\\right)^{-1} \\sum_{i=1}^n X_i u_i  \\right] \\\\\n& = \\beta\n\\end{aligned}\n\\] where the final equality is because \\(X\\) and \\(u\\) are independent and \\(\\Er[u]=0\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#efficiency",
    "href": "problemsets/midterm2022/midterm2022_solution.html#efficiency",
    "title": "ECON 626: Midterm Solutions",
    "section": "Efficiency",
    "text": "Efficiency\nLet \\(X = (X_1, ..., X_n)'\\) denote the \\(n \\times k\\) matrix of \\(X_i\\), and \\(m^{-1}(y) = (m^{-1}(Y_i), ..., m^{-1}(Y_n))\\) For this section, you may treat \\(X\\) as non-stochastic.\n\nAssume that \\(\\Er[uu'] = \\sigma^2 I_n\\). What is the minimal variance unbiased estimator for \\(c'\\beta\\) that is a linear function of \\(m^{-1}(y)\\)?\n\n\nSolution. By the Gauss Markov Theorem, \\(\\hat{\\beta}\\) is the best linear unbiased estimator.\n\n\nAssume \\(u_i = S_i \\epsilon_i\\) with \\(S_i\\) observed, \\(\\Er[\\epsilon] =0\\), and \\(\\Er[\\epsilon\\epsilon'] = I_n\\). What is the minimal variance unbiased estimator that is a linear function of \\(m^{-1}(y)?\\)\n\n\nSolution. If we let \\(\\tilde{y}_i = S_i^{-1} m^{-1}(Y_i)\\) and \\(\\tilde{X}_i = S_i^{-1} X_i\\), then we have the linear model \\[\n\\tilde{y}_i = \\tilde{X}_i \\beta + \\underbrace{\\epsilon_i }_{\\equiv S_i^{-1} u_i}\n\\] This model has \\(\\Er[\\epsilon]=0\\) and \\(\\Er[\\epsilon\\epsilon'] = I_n\\), so the Gauss-Markov theorem applies. The best linear (in \\(\\tilde{y}_i)\\) unbiased estimator \\(c'\\beta\\) is \\(c'\\hat{\\beta}^W = c'(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' \\tilde{y}\\). Note that \\[\nc' \\hat{\\beta}^W = c'(X' diag(S)^{-2}X)^{-1} X' diag(S)^{-2} \\tilde{y}\n\\] so this estimator is linear in \\(m^{-1}(y)\\), and thus is the best linear in \\(m^{-1}(y)\\) unbiased estimator.\n\n\nSuppose that \\(u_i \\sim N(0, \\sigma^2)\\). Show that \\(c'(X'X)^{-1}X'm^{-1}(y)\\) is the minimal variance unbiased estimator for \\(c'\\beta\\).\n\n\nSolution. The log likelihood is \\[\n\\ell(\\beta) = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (m^{-1}(y_i) - X_i' \\beta)^2\n\\] The score is \\[\ns(\\beta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n X_i (m^{-1}(y_i) - X_i' \\beta)\n\\] The hessian is \\[\nH(\\beta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n -X_i X_i'\n\\] The Cramer-Rao Lower bound is \\[\nI(\\beta) = \\Er[-H^{-1}] = \\sigma^2 (X'X)^{-1}\n\\] We must compare this with \\(\\var(\\hat{\\beta})\\). \\[\n\\begin{aligned}\n\\var(\\hat{\\beta}) = & \\var( (X'X)^{-1} X'm^{-1}(y)) \\\\\n= & (X'X)^{-1} \\var(X'm^{-1}(y)) (X'X)^{-1} \\\\\n= & (X'X)^{-1} \\var(X'u) (X'X)^{-1} \\\\\n= & (X'X)^{-1} X'X \\sigma^2 (X'X)^{-1} \\\\\n= & (X'X)^{-1}\\sigma^2  = I(\\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#sigma-fields",
    "href": "problemsets/midterm2022/midterm2022_solution.html#sigma-fields",
    "title": "ECON 626: Midterm Solutions",
    "section": "\\(\\sigma\\) fields",
    "text": "\\(\\sigma\\) fields\n\nSuppose \\(K=5\\) and \\(g(u) = |u - (K+1)/2|\\). What is \\(\\sigma(Y)\\)?\n\n\nSolution. The support of \\(Y\\) is \\(\\{0, 1, 2\\}\\). The preimage of these sets are \\(\\{3\\}\\), \\(\\{2,4\\}\\), and \\(\\{1, 5\\}\\). \\(\\sigma(Y)\\) additionally contains unions, complements, and intersections of these sets, so \\[\n\\sigma(Y) = \\{ \\emptyset, \\{3\\}, \\{1, 5\\}, \\{2, 4\\}, \\{2,3,4\\}, \\{1, 3, 5\\}, \\{1, 2, 4, 5\\}, \\{1, 2, 3, 4, 5\\} \\}\n\\]\n(If we interpret \\(u\\) as a random variable, then actually, we should replace the sets listed as \\(\\sigma(Y)\\) with \\(u^{-1}\\) of these sets, so that we end up with sets in some unspecified sample space, \\(\\Omega\\)).\n\n\nSuppose \\(g\\) is one to one. What is \\(\\sigma(Y)\\)?\n\n\nSolution. Then for any \\(A \\subset \\{1, ..., K\\}\\), \\(g^{-1}(g(A)) = A\\), so \\(\\sigma(Y) = \\sigma(U) =\\) power set of \\(\\{u^{-1}(1) ,..., u^{-1}(K) \\}\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#identification-1",
    "href": "problemsets/midterm2022/midterm2022_solution.html#identification-1",
    "title": "ECON 626: Midterm Solutions",
    "section": "Identification",
    "text": "Identification\nShow that if if \\(g\\) is one to one, then \\(\\theta_1, ... , \\theta_k\\) are identified.\n\nSolution. If \\(g\\) is one to one, then \\(\\theta_j = P(Y=g(j))\\) identifies \\(\\theta_j\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#estimation-1",
    "href": "problemsets/midterm2022/midterm2022_solution.html#estimation-1",
    "title": "ECON 626: Midterm Solutions",
    "section": "Estimation",
    "text": "Estimation\nAssuming \\(g\\) is one to one, find the maximum likelihood estimator for \\(\\theta\\), and show whether it is unbiased.\n\nSolution. The likelihood is \\[\n\\begin{aligned}\n\\ell(\\theta; Y) = & \\prod_{i=1}^n \\prod_{j=1}^K \\theta_j^{1\\{Y_i = g(j) \\}} \\\\\n= & \\prod_{j=1}^K \\theta_j^{\\sum_{i=1}^n 1\\{Y_i = g(j) \\}} \\\\\n\\log \\ell (\\theta; Y) = & \\sum_{j=1}^K \\left( \\sum_{i=1}^n 1\\{Y_i = g(j) \\} \\right) \\log \\theta_j\n\\end{aligned}\n\\] To simplify notation, let \\(n_k = \\sum_{i=1}^n 1\\{Y_i = g(k) \\}\\). We want to solve \\[\n\\max_\\theta \\sum_{k=1}^K n_k \\log \\theta_k \\text{ s.t. } \\sum_{k=1}^K \\theta_k = 1\n\\] The first order conditions are \\[\n\\frac{n_k}{\\theta_k} = \\lambda\n\\] or \\(n_k = \\lambda \\theta_k\\). Summing across \\(k\\), and using the constraint we have \\(\\lambda = \\sum_{k} n_k = n\\). Thus, \\[\n\\hat{\\theta}_k = \\frac{n_k}{n}\n\\]\nThis is unbiased because \\[\n\\Er[\\hat{\\theta}_k] = \\Er\\left[ \\frac{1}{n} \\sum_i 1\\{Y_i = g(k) \\}  \\right] = P(U_i = k) = \\theta_k\n\\]"
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#testing",
    "href": "problemsets/midterm2022/midterm2022_solution.html#testing",
    "title": "ECON 626: Midterm Solutions",
    "section": "Testing",
    "text": "Testing\n\nFor \\(K=2\\), find the most powerful test for testing \\(H_0: \\theta_1 = \\theta_1^0\\) against \\(H_1: \\theta_1 = \\theta_1^a\\).\n\n\nSolution. By the Neyman-Pearson Lemma, the likelihood ratio test is most powerfull. The likelihood here is: \\[\n\\begin{aligned}\nf(Y;\\theta) & = \\prod_{i=1}^n \\theta^{1\\{Y_i = g(1)\\}} (1-\\theta)^{1\\{Y_i = g(2) \\}} \\\\\n& = \\theta^{n_1}(1-\\theta)^{n_2}\n\\end{aligned}\n\\] where \\(n_j = \\sum_i 1\\{Y_i = g(j)\\}\\). Note that \\(n_2 = n-n_1\\)\nThe likelihood ratio is then \\[\n\\tau(Y) = \\frac{(\\theta_1^a)^{n_1} (1-\\theta_1^a)^{n-n_1}} { (\\theta_1^0)^{n_1} (1-\\theta_1^0)^{n-n_1}}\n\\] To find a critical region, notice that this only depend on the data through \\(n_1\\). Under \\(H_0\\), the distribution of \\(n_1\\) is \\[\nP(n_1 = m) = \\frac{n!}{m!(n-m)!} (\\theta_1^0)^m (1-\\theta_1^0)^{n - m}\n\\] (After writing the rest of the solution, I suppose this formula isn’t really needed).\nTo proceed further, assume that \\(\\theta_1^a &gt; \\theta_1^0\\), then \\(\\tau(Y) \\equiv \\tau(n_1)\\) is increasing as a function \\(n_1\\), and \\[\nP(\\tau(n_1) &gt; \\tau(c) | H_0) = P(n_1 &gt; c | H_0)\n\\] For a test of size \\(\\alpha\\), we choose \\(c\\) such that \\(P(n_1 &gt; c | H_0) = \\alpha\\), and reject \\(H_0\\) if \\(n_1 &gt; c\\).\nFor \\(\\theta_1^a &lt; \\theta_1^0\\), by similar reasoning, we have \\[\nP(\\tau(n_1) &gt; \\tau(c) | H_0) = P(n_1 &lt; c | H_0)\n\\] where the second inequality is flipped because now \\(\\tau\\) decreases with \\(n_1\\). Thus, the critical region is \\(\\{n_1: n_1 &lt; c\\}\\)\n\n\nIs this test also most powerful against the alternative \\(H_1: \\theta_1 \\neq \\theta_1^0\\)? (Hint: does the critical region depend on \\(\\theta_1^a\\)?)\n\n\nSolution. The critical region above does not depend on the exact value of \\(\\theta_1^a\\) because \\(P(n_1 &gt; c | H_0)\\) does not depend on \\(\\theta_1^a\\). However, it does depend on whether \\(\\theta_1^a\\) is less than or greater than \\(\\theta_1^0\\). Hence, the test is most powerful against \\(H_1: \\theta_1 &gt; \\theta_1^0\\) or \\(H_1: \\theta_1 &lt; \\theta_1^0\\), but not \\(H_1: \\theta_1 \\neq \\theta_1^0\\)."
  },
  {
    "objectID": "problemsets/midterm2022/midterm2022_solution.html#footnotes",
    "href": "problemsets/midterm2022/midterm2022_solution.html#footnotes",
    "title": "ECON 626: Midterm Solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you could not answer that part, suppose you had shown that \\(\\beta = \\Er[X_i X_i']^{-1} \\Er[X_i m^{-1}(Y_i)]\\).↩︎"
  },
  {
    "objectID": "problemsets/07/ps07.html",
    "href": "problemsets/07/ps07.html",
    "title": "ECON 626: Problem Set 7",
    "section": "",
    "text": "\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\n\nProblem 1\nConsider the following linear regression model such that \\[\nY_i = β_0 + X_i β_1 + u_i ,\n\\] where \\(X_i\\) and \\(Y_i\\) are observed random variables. Let us assume that \\(\\Er [u_i ] = 0\\) but \\(\\cov(X_i , u_i ) \\neq 0\\). Suppose that there exists a variable \\(Z_i\\) such that \\(\\cov(X_i , Z_i ) &gt; 0\\) and \\(\\cov(Z_i , u_i ) &gt; 0\\).\nFind the asymptotic bias of the 2SLS estimator of \\(\\hat{\\beta}_1\\). (Recall that the asymptotic bias of an estimator is its probability limit minus the true parameter.) Can you determine unambiguously whether the 2SLS estimator tends to underestimate or overestimate the parameter \\(β_1\\) ? If so, give explanations how.\n\n\nProblem 2\nIn the linear model, \\[\nY_i = \\beta_0 + X_i \\beta_1 + u_i\n\\] assume that \\(\\Er[u_i] = 0\\) and \\(X_i \\in \\R^1\\). Suppose that \\(\\Er[X_i u_i] \\neq 0\\), but, somewhat strangely, you assume \\(\\Er[u_i^2|X_i] = \\sigma^2\\).\n\nShow that a set of two elements that contains \\(\\beta_1\\) is identified. Denote this set by \\(B_1\\). Hint: use the moment condition \\(\\Er[u_i^2 (X_i-\\Er[X_i])]\\).\nDescribe an estimator for \\(B_1\\) and show that it is consistent. State any additional assumptions needed.\nFind the asymptotic distribution of your estimator for \\(B_1\\). State any additional assumptions needed."
  },
  {
    "objectID": "problemsets/05/ps05.html",
    "href": "problemsets/05/ps05.html",
    "title": "ECON 626: Problem Set 5",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\]"
  },
  {
    "objectID": "problemsets/05/ps05.html#footnotes",
    "href": "problemsets/05/ps05.html#footnotes",
    "title": "ECON 626: Problem Set 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy uniformly, we mean that \\(\\sup_{P_x \\in \\mathcal{P}_0} |\\Er[f(T(X_n))] - \\Er[f(Z)] | \\to 0\\) for all \\(f \\in \\mathcal{C}_b\\).↩︎"
  },
  {
    "objectID": "problemsets/03/ps03.html",
    "href": "problemsets/03/ps03.html",
    "title": "ECON 626: Problem Set 3",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\]"
  },
  {
    "objectID": "problemsets/03/ps03.html#footnotes",
    "href": "problemsets/03/ps03.html#footnotes",
    "title": "ECON 626: Problem Set 3",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn assumption like this is used in nonparametric selection estimators. One way to have such \\(E_1\\) and \\(E_0\\) is if \\(D = 1\\{Z \\gamma \\geq U\\}\\) and \\(Z\\) has unbounded supported. Then \\(E_0\\) and \\(E_1\\) are sets where \\(Z \\to \\pm \\infty\\). Accordingly, this is sometimes called an identification at infinity assumption. See Heckman and Navarro-Lozano (2004) for more information, and a comparison between this case, and the (conditional) independence assumption of part 1. (This footnote and that paper are extra information, not hints to help answer the question).↩︎"
  },
  {
    "objectID": "problemsets/01/ps01.html",
    "href": "problemsets/01/ps01.html",
    "title": "ECON 626: Problem Set 1",
    "section": "",
    "text": "Problem 1\nLet \\(\\Omega = \\{a,b,c,d\\}\\).\n\nIs \\(\\{\\{a\\}, \\{c\\}, \\{a,b\\}, \\emptyset\\}\\) a \\(\\sigma\\)-field?\nWhat is the smallest \\(\\sigma\\)-field containing \\(\\{\\{a,b\\},\\{a,c\\}\\}\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem 2\nSong (2021) exercise 4.1.\nFor a collection \\(\\mathscr{C}\\) of sets, we write \\(\\sigma (\\mathscr{C})\\) to denote the smallest \\(\\sigma\\)-field that contains \\(\\mathscr{C}\\), and say that \\(\\sigma (\\mathscr{C})\\) is the \\(\\sigma\\)-field generated by \\(\\mathscr{C}\\).\n\nLet \\(X\\) be a random variable on \\((\\Omega ,\\mathscr{F})\\) and let \\(\\mathscr{G}\\) be the collection of the sets of the form \\(\\{\\omega \\in \\Omega :X(\\omega )\\in B\\}\\) with \\(B\\in \\mathscr{B}(\\mathbf{R})\\). Then show that \\(\\mathscr{G}\\) is a \\(\\sigma\\)-field.\nShow that \\(\\{X^{-1}(A):A\\in \\sigma (\\mathscr{C})\\}=\\sigma(\\{X^{-1}(A):A\\in \\mathscr{C}\\})\\) for any subset \\(\\mathscr{C}\\) of \\(\\mathscr{B}(\\mathbf{R})\\).\n\n\n\nProblem 3\nShow that if for some \\(u ≥ 2\\), \\(E[|X|^u ] &lt; \\infty\\) and \\(E[|Y|^u ] &lt; \\infty\\), then for some \\(s \\in (0, 1)\\), \\[\n\\lim_{a \\to \\infty} a^{1−s} P \\left(|XY| &gt; a\\right) = 0\n\\]\n\n\n\n\n\n\nTip\n\n\n\nUse Markov’s inequality and the Cauchy-Schwarz inequality.\nI won’t change the question now (it comes from a problem set for this course from a few years ago, before I was teaching it), but a perhaps better statement of the problem would be to show that \\[\n\\lim_{a \\to \\infty} a^{q} P \\left(|XY| &gt; a\\right) = 0\n\\] for \\(q&lt;u/2\\). Also, the requirement that \\(u \\geq 2\\) could be replaced with just \\(u&gt;0\\). Of course, this result implies the result the question askes for.\n\n\n\n\n\n\n\nReferences\n\nSong, Kyunchul. 2021. “Introduction to Econometrics.”"
  },
  {
    "objectID": "measure/measure.html#references",
    "href": "measure/measure.html#references",
    "title": "Measure",
    "section": "References",
    "text": "References\n\nSong (2021) chapter 1 (which is the basis for these slides)\nPollard (2002)\nTao (2011)"
  },
  {
    "objectID": "measure/measure.html#why-measure-theory",
    "href": "measure/measure.html#why-measure-theory",
    "title": "Measure",
    "section": "Why Measure Theory?",
    "text": "Why Measure Theory?\n\n\nSimplifies some arguments\n\nExample from Pollard (2002), define independence as factorization of distribution functions \\[ P(X \\leq x \\cap Y \\leq y) = P(X \\leq x) P(Y \\leq y) \\]\nIf \\(X_1,X_2,X_3, X_4\\) are independent, show that \\[ Y = X_1 X_2 \\log\\left(\\frac{X_1^2 + X_2^3}{|X_1| + |X_2|}\\right) \\] is independent of \\[ Z =  sin\\left(X_3 + X_3^2 + X_3X_4 + X_4^2 \\right) \\]"
  },
  {
    "objectID": "measure/measure.html#why-measure-theory-1",
    "href": "measure/measure.html#why-measure-theory-1",
    "title": "Measure",
    "section": "Why Measure Theory?",
    "text": "Why Measure Theory?\n\nSimplifies some arguments\n\n\n\nUnifies treatment\n\ndiscrete vs continuous\nuni- vs multi-variate\n\nResolves some difficulties with infinity"
  },
  {
    "objectID": "measure/measure.html#measure-space",
    "href": "measure/measure.html#measure-space",
    "title": "Measure",
    "section": "Measure Space",
    "text": "Measure Space\n\n\nA set \\(\\Omega\\)\nA collection of subsets, \\(\\mathscr{F}\\), of \\(\\Omega\\) that is a \\(\\sigma\\)-field (aka \\(\\sigma\\)-algebra) , that is\n\n\\(\\Omega \\in \\mathscr{F}\\)\nIf \\(A \\in \\mathscr{F}\\), then \\(A^c \\in \\mathscr{F}\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\), then \\(\\cup_{j=1}^\\infty A_j \\in \\mathscr{F}\\)\n\nA measure, \\(\\mu: \\mathcal{F} \\to [0, \\infty]\\) s.t.\n\n\\(\\mu(\\emptyset) = 0\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\) are pairwise disjoint, then \\(\\mu\\left(\\cup_{j=1}^\\infty A_j \\right) = \\sum_{j=1}^\\infty \\mu(A_j)\\)\n\n\n\n\nExample of sigma fields"
  },
  {
    "objectID": "measure/measure.html#measurable-function",
    "href": "measure/measure.html#measurable-function",
    "title": "Measure",
    "section": "Measurable Function",
    "text": "Measurable Function\n\nGiven a topology on \\(\\Omega\\), the Borel \\(\\sigma\\)-field, \\(\\mathscr{B}(\\Omega)\\), is the smallest \\(\\sigma\\)-field containing all open subsets of \\(\\Omega\\)\n\n\nWhen working with \\(R^d\\), we will generally use \\(\\mathscr{B}(\\mathbf{R}^d)\\) as our \\(\\sigma\\)-field.\n\n\n\\(f: \\Omega \\to \\mathbf{R}\\) is (\\(\\mathscr{F}\\)-)measurable if \\(\\forall\\) \\(B \\in \\mathscr{B}(\\mathbf{R})\\), \\(f^{-1}(B) \\in \\mathscr{F}\\)\na statement holds almost everywhere (a.e.) if the measure of the set where the statement is false is 0"
  },
  {
    "objectID": "measure/measure.html#simple-functions",
    "href": "measure/measure.html#simple-functions",
    "title": "Measure",
    "section": "Simple Functions",
    "text": "Simple Functions\n\nAssume \\(\\mu(\\Omega) &lt; \\infty\\)\n\\(f\\) is a simple function if \\(f = \\sum_{j=1}^n a_j 1\\{\\omega \\in E_j \\}\\) for \\(a_j \\in \\mathbf{R}\\) and \\(E_j \\in \\mathscr{F}\\)\nIntegral of simple functions: \\[ \\int f d \\mu = \\sum_{j=1}^n a_j \\mu(E_j) \\]"
  },
  {
    "objectID": "measure/measure.html#bounded-functions",
    "href": "measure/measure.html#bounded-functions",
    "title": "Measure",
    "section": "Bounded Functions",
    "text": "Bounded Functions\n\nLet \\(E\\) be such that \\(\\mu(E)&lt;\\infty\\)\nLet \\(f\\) be bounded function and \\(f(x) = 0 \\forall x \\in E^c\\)\nDefine: \\[\n\\int f d\\mu \\equiv \\sup_{\\varphi \\leq f: \\varphi \\text{ simple}} \\int \\varphi d\\mu =\\inf_{\\varphi \\geq f: \\varphi \\text{ simple}} \\int \\varphi d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#nonnegative-functions",
    "href": "measure/measure.html#nonnegative-functions",
    "title": "Measure",
    "section": "Nonnegative Functions",
    "text": "Nonnegative Functions\n\nIf \\(f \\geq 0\\), define \\[\n\\int fd\\mu =\\sup_{f_n \\leq f \\text{ simple, bounded+}} \\int f_{n}d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#measurable-functions",
    "href": "measure/measure.html#measurable-functions",
    "title": "Measure",
    "section": "Measurable Functions",
    "text": "Measurable Functions\n\nIf \\(f\\) is measurable, let \\(f^{+} = \\max\\{f, 0\\}\\) and \\(f^{-} = \\max\\{-f, 0\\}\\) and define the Lesbegue integral\n\n\\[ \\int f d\\mu = \\int f^{+} d\\mu - \\int f^{-} d\\mu \\]"
  },
  {
    "objectID": "measure/measure.html#finite-measure",
    "href": "measure/measure.html#finite-measure",
    "title": "Measure",
    "section": "Finite Measure",
    "text": "Finite Measure\n\nMeasure \\(\\mu\\) is finite if \\(\\mu(\\Omega)\\) is finite\n\\(\\mu\\) is \\(\\sigma\\)-finite if \\(\\exists\\) \\(\\{A_n\\}_{n=1}^\\infty \\in \\mathscr{F}\\) s.t. \\(\\mu(A_n)\\) is finite \\(\\forall n\\) and \\(\\cup_{n=1}^\\infty A_n = \\Omega\\)\n\n\n\n\nExercise\n\n\nLet \\(\\Omega\\) be countable with any \\(\\mathscr{F}\\), define \\(\\mu(A)\\) as the number of elements of \\(A\\). Show \\(\\mu\\) is \\(\\sigma\\) finite."
  },
  {
    "objectID": "measure/measure.html#lebesgue-measure",
    "href": "measure/measure.html#lebesgue-measure",
    "title": "Measure",
    "section": "Lebesgue Measure",
    "text": "Lebesgue Measure\n\n\n\nTheorem\n\n\nThere exists a unique \\(\\sigma\\)-finite measure \\(\\mu\\) on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that for any \\(a\\leq b\\) with \\(a,b\\in \\mathbf{R}\\), \\[\n\\mu ((a,b])=b-a\n\\]"
  },
  {
    "objectID": "measure/measure.html#absolute-continuity",
    "href": "measure/measure.html#absolute-continuity",
    "title": "Measure",
    "section": "Absolute Continuity",
    "text": "Absolute Continuity\n\nMeasure \\(\\nu\\) is absolutely continuous with respect to \\(\\mu\\) if for \\(A \\in \\mathscr{F}\\), \\(\\mu(A) = 0\\) implies \\(\\nu(A) = 0\\)\n\ndenotate as \\(\\nu \\ll \\mu\\)\n\\(\\mu\\) is called a dominating measure\n\n\n\nRelation with continuity?"
  },
  {
    "objectID": "measure/measure.html#radon-nikodym-derivative",
    "href": "measure/measure.html#radon-nikodym-derivative",
    "title": "Measure",
    "section": "Radon-Nikodym Derivative",
    "text": "Radon-Nikodym Derivative\n\n\n\nTheorem\n\n\nLet \\((\\Omega,\\mathscr{F},\\mu)\\) be a measure space, and let \\(\\nu\\) and \\(\\mu\\) be \\(\\sigma\\)-finite measures defined on \\(\\mathscr{F}\\) and \\(\\nu \\ll \\mu\\). Then there is a nonnegative measurable function \\(f\\) such that for each set \\(A\\in \\mathscr{F}\\), \\[\n\\nu (A)=\\int_{A}fd\\mu\n\\] For any such \\(f\\) and \\(g\\), \\(\\mu (\\{\\omega \\in \\Omega:f(\\omega )\\neq g(\\omega )\\})=0\\)\n\n\n\n\nDenote $f = \nExercise: show coincides with usual derivative?"
  },
  {
    "objectID": "measure/measure.html#sequences-of-sets",
    "href": "measure/measure.html#sequences-of-sets",
    "title": "Measure",
    "section": "Sequences of Sets",
    "text": "Sequences of Sets\n\n\\(\\{E_n\\}_{n \\geq 1} \\in \\mathscr{F}\\)\n\nincreasing if \\(E_1 \\subset E_2 \\subset ...\\)\ndecreasing if \\(E_1 \\supset E_2 \\supset ...\\)\nmonotone if either increasing or decreasing\n\nFor increasing \\(E_n\\), define \\(\\lim_{n \\to \\infty} E_n =\\cup_{n=1}^\\infty E_n\\)\nFor decreasing \\(E_n\\), define \\(\\lim_{n \\to \\infty} E_n =\\cap_{n=1}^\\infty E_n\\)"
  },
  {
    "objectID": "measure/measure.html#continuity-of-measure",
    "href": "measure/measure.html#continuity-of-measure",
    "title": "Measure",
    "section": "Continuity of Measure",
    "text": "Continuity of Measure\n\n\n\nLemma\n\n\nSuppose that \\(\\{E_{n}\\}\\) is a monotone sequence of events. Then \\[\n\\mu \\left( \\lim_{n\\rightarrow \\infty}E_{n}\\right) =\\lim_{n\\rightarrow \\infty }\\mu (E_{n}).\n\\]"
  },
  {
    "objectID": "measure/measure.html#monotone-convergence-theorem",
    "href": "measure/measure.html#monotone-convergence-theorem",
    "title": "Measure",
    "section": "Monotone Convergence Theorem",
    "text": "Monotone Convergence Theorem\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\uparrow f(\\omega )\\), then \\(\\int f_{n}d\\mu \\uparrow \\int fd\\mu\\) as \\(n\\rightarrow \\infty\\)"
  },
  {
    "objectID": "measure/measure.html#fatous-lemma",
    "href": "measure/measure.html#fatous-lemma",
    "title": "Measure",
    "section": "Fatou’s Lemma",
    "text": "Fatou’s Lemma\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), then \\[\n\\int \\left( \\text{liminf}_{n\\rightarrow \\infty }f_{n}d\\mu \\right) \\leq \\text{liminf}_{n\\rightarrow \\infty }\\int f_{n}d\\mu\n\\]"
  },
  {
    "objectID": "measure/measure.html#dominated-convergence-theorem",
    "href": "measure/measure.html#dominated-convergence-theorem",
    "title": "Measure",
    "section": "Dominated Convergence Theorem",
    "text": "Dominated Convergence Theorem\n\n\n\nLemma\n\n\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\rightarrow f(\\omega ).\\) Furthermore, for some \\(g\\geq 0\\) such that \\(\\int gd\\mu &lt;\\infty\\), \\(|f_{n}|\\leq g\\) for each \\(n\\geq 1\\). Then, \\(\\int f_{n}d\\mu \\rightarrow \\int fd\\mu\\)\n\n\n\n\nThe measurable condition here can be dropped and outermeasure used instead. This simplifies some proofs in asymptotic theory."
  },
  {
    "objectID": "iv/endogeneity.html#reading",
    "href": "iv/endogeneity.html#reading",
    "title": "Endogeneity",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 11\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "iv/endogeneity.html#omitted-variables",
    "href": "iv/endogeneity.html#omitted-variables",
    "title": "Endogeneity",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\nDesired model \\[\nY_i = X_i'\\beta_0 + W_i'\\gamma_0 + u_i\n\\] Assume \\(\\Er[u] = \\Er[Xu] = \\Er[Wu] = 0\\)\nEstimated model \\[\nY_i = X_i'\\beta + u_i\n\\]\nWhat is \\(\\plim \\hat{\\beta}\\)?"
  },
  {
    "objectID": "iv/endogeneity.html#omitted-variables-1",
    "href": "iv/endogeneity.html#omitted-variables-1",
    "title": "Endogeneity",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\n\\(\\plim \\hat{\\beta} \\inprob \\beta_0 + \\Er[X_i X_i']^{-1} \\Er[X_i W_i'] \\gamma_0\\)"
  },
  {
    "objectID": "iv/endogeneity.html#omitted-variables-2",
    "href": "iv/endogeneity.html#omitted-variables-2",
    "title": "Endogeneity",
    "section": "Omitted Variables",
    "text": "Omitted Variables\n\nIf \\(\\gamma_0 = 0\\), what is variance of \\(\\hat{\\beta}\\) when \\(W\\) is and is not included in the model?"
  },
  {
    "objectID": "iv/endogeneity.html#errors-in-variables",
    "href": "iv/endogeneity.html#errors-in-variables",
    "title": "Endogeneity",
    "section": "Errors in Variables",
    "text": "Errors in Variables\n\nSee problem set 6"
  },
  {
    "objectID": "iv/endogeneity.html#simultaneity-bias",
    "href": "iv/endogeneity.html#simultaneity-bias",
    "title": "Endogeneity",
    "section": "Simultaneity Bias",
    "text": "Simultaneity Bias\n\nEquilibrium conditions often lead to variables that are simultaneously determined\nDemand and supply: \\[\n\\begin{align*}\nQ_i^D & = P_i \\beta_D + X_D'\\gamma_D + u_{D,i} \\\\\nQ_i^S & = P_i \\beta_S + X_S'\\gamma_S + u_{S,i} \\\\\nQ_i^S & = Q_i^D\n\\end{align*}\n\\]"
  },
  {
    "objectID": "iv/endogeneity.html#simultaneity-bias-1",
    "href": "iv/endogeneity.html#simultaneity-bias-1",
    "title": "Endogeneity",
    "section": "Simultaneity Bias",
    "text": "Simultaneity Bias\n\nStructural equations: (demand and inverse supply): \\[\n\\begin{align*}\nQ_i & = P_i \\beta_D + X_D'\\gamma_D + u_{D,i} \\\\\nP_i & = Q_i \\frac{1}{\\beta_S}  - X_S'\\gamma_D\\frac{1}{\\beta_S} - u_{S,i}\\frac{1}{\\beta_S} \\\\\n\\end{align*}\n\\]\nReduced form: \\[\n\\begin{align*}\nQ_i = & \\frac{\\beta_D}{\\beta_D - \\beta_S} \\left( -X_{D,i}' \\gamma_D + X_{S,i}'\\gamma_S - u_{D,i} + u_{S,i} \\right) + X_{D,i}'\\gamma_D + u_{D,i} \\\\\nP_i = & \\frac{1}{\\beta_D - \\beta_S}\\left(-X_{D,i}' \\gamma_D + X_{S,i}'\\gamma_S - u_{D,i} + u_{S,i} \\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "identification/identification.html#reading",
    "href": "identification/identification.html#reading",
    "title": "Identification",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 4 (which is the basis for these slides)\nRecommended: Lewbel (2019)\nSupplementary: Matzkin (2013), Molinari (2020) , Imbens (2020)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\]\n\nMotivate by wanting to go from data to parameters …"
  },
  {
    "objectID": "identification/identification.html#example-descriptive-statistics",
    "href": "identification/identification.html#example-descriptive-statistics",
    "title": "Identification",
    "section": "Example: Descriptive Statistics",
    "text": "Example: Descriptive Statistics\n\n\n\\(\\theta_0 =\\) mean of \\(X\\), then \\(\\theta_0\\) is identified by \\[\n\\psi_\\mu(P) = \\int x dP(x)\n\\] in \\(\\mathcal{P} = \\{P : \\int x dP(x) &lt; \\infty \\}\\)\nGenerally, descriptive statistics identified in a broad probability model with just regularity restrictions to ensure the statistics exist"
  },
  {
    "objectID": "identification/identification.html#example-linear-model",
    "href": "identification/identification.html#example-linear-model",
    "title": "Identification",
    "section": "Example: Linear Model",
    "text": "Example: Linear Model\n\\[\nY = \\alpha + \\beta X + \\epsilon\n\\]\n\n\\(\\mathcal{P} = \\{P_{X,Y}:\\) \\(Y=\\alpha + \\beta X + \\epsilon\\),\n\n\\(| \\mathrm{Cov}(X,Y) | &lt; \\infty\\), \\(0 &lt; \\mathrm{Var}(X) &lt; \\infty\\)\n\\(\\mathrm{Cov}(X, \\epsilon) = 0\\) \\(\\}\\)\n\n\\(\\beta\\) identified as\n\n\\[\n\\beta = \\frac{\\int (x - \\Er X) (y - \\Er Y ) dP_{X,Y}(x,y)}\n{\\int (x - \\Er X)^2 dP_{X}(x)}  = \\frac{ \\cov(X,Y) }{ \\var(X) }\n\\]\n\n\nIdentification requires:\n\nUsually innocuous regularity conditions\nSubstantive exogeneity restriction\n\nEvaluating plausibility of exogeneity restrictions requires a priori knowledge of data context and related economic theory\n\n\n\nExamples of production function and returns to schooling here?\n2 is more difficult to get right and more context dependent, so we will first tackle the easier problem of 1."
  },
  {
    "objectID": "identification/identification.html#example-multiple-regression",
    "href": "identification/identification.html#example-multiple-regression",
    "title": "Identification",
    "section": "Example: Multiple Regression",
    "text": "Example: Multiple Regression\n\\[\nY = X'\\beta + \\epsilon\n\\]\n\n\\(\\mathcal{P} = \\{P: \\Er X \\epsilon = 0, \\Er X X' \\text{ invertible} \\}\\)"
  },
  {
    "objectID": "identification/identification.html#example-binary-choice",
    "href": "identification/identification.html#example-binary-choice",
    "title": "Identification",
    "section": "Example: Binary Choice",
    "text": "Example: Binary Choice\n\\[\nY = 1\\{ \\beta_0 + \\beta_1 X &gt; u \\}\n\\]\n\n\\(\\mathcal{P} = \\{P: u \\sim N(0,1), 0&lt; \\var(X) &lt; \\infty \\}\\)\n\n\n\nIs \\(u \\sim N(0,1)\\) innocuous?\n\n\n\nMore generally, think about importance of statistical assumptions in terms of how they affect identifiable quantities and counterfactuals of interest."
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes",
    "href": "identification/identification.html#example-potential-outcomes",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\nData:\n\nTreatment \\(D_i\\)\nPotential outcomes \\((Y_{i,0}, Y_{i,1})\\), observed outcome \\(Y_i = D_i Y_{i,1} + (1-D_i) Y_{i,0}\\)\nCovarites \\(X_i\\)\n\nParameter: \\(\\theta_0 = \\Er[Y_{i,1} - Y_{i,0}] =\\) average treatment effect\nAssume:\n\nUnconfoundedness: \\((Y_{i,0}, Y_{i,1})\\) conditionally independent of \\(D_i\\) given \\(X_i\\)\nOverlap: \\(\\epsilon &lt; P(D=1|X=x) &lt; 1-\\epsilon\\) for some \\(\\epsilon &gt; 0\\) and all \\(x\\)"
  },
  {
    "objectID": "identification/identification.html#causal-diagrams-1",
    "href": "identification/identification.html#causal-diagrams-1",
    "title": "Identification",
    "section": "Causal Diagrams",
    "text": "Causal Diagrams\n\nOriginate with Wright around 1920, e.g. Wright (1934)\nRecently advocated by Pearl, e.g. Pearl (2015), Pearl and Mackenzie (2018)\nRecommended introduction Imbens (2020)\nSometimes useful expository tool for explaining identifying restriction, but should not be your only or primary approach\n\ne.g. Chernozhukov, Kasahara, and Schrimpf (2021)"
  },
  {
    "objectID": "identification/identification.html#example-regression",
    "href": "identification/identification.html#example-regression",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-1",
    "href": "identification/identification.html#example-potential-outcomes-1",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes"
  },
  {
    "objectID": "identification/identification.html#example-regression-1",
    "href": "identification/identification.html#example-regression-1",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\nIn linear model \\(Y_i = X_i'\\beta + \\epsilon_i\\), if just assume \\(\\Er X X'\\) invertible,\nPopulation regression \\[\n\\begin{align*}\n\\theta = & \\Er[ X X']^{-1} \\Er[ X Y]  \\\\\n= & \\Er[X X']^{-1} \\Er[X (X' \\beta + \\epsilon)] \\\\\n= & \\beta + \\Er[X X']^{-1} \\Er[X\\epsilon]\n\\end{align*}\n\\]"
  },
  {
    "objectID": "identification/identification.html#example-regression-2",
    "href": "identification/identification.html#example-regression-2",
    "title": "Identification",
    "section": "Example: Regression",
    "text": "Example: Regression\n\nIf relevant moments exist (no linear model required) population regression solves \\[\n\\Er[ X X']^{-1} \\Er[ X Y]  \\in \\mathrm{arg}\\min_b \\Er[ (X'b - \\Er[Y|X])^2 ]\n\\]"
  },
  {
    "objectID": "identification/identification.html#example-potential-outcomes-2",
    "href": "identification/identification.html#example-potential-outcomes-2",
    "title": "Identification",
    "section": "Example: Potential Outcomes",
    "text": "Example: Potential Outcomes\n\nMatching initially studied with a linear regression model, e.g. Cochran (1953) \\[\nY_i = \\alpha D_i + X_i' \\beta + \\epsilon_i\n\\]\nImplies constant treatment effect \\(Y_{i,1} - Y_{i,0} = \\alpha\\)\n\n\nThe LATE interpretation of IV developed along similar lines – replace a constant treatment effect linear model with a heterogenous effect potential outcomes framework, and see what IV does.\nSimilarly, the recent literature on difference in differences with staggered treatment timing arises from moving from a model with a constant treatment effect over time to one where treatment effects vary with time."
  },
  {
    "objectID": "identification/identification.html#non-constructive-identification-1",
    "href": "identification/identification.html#non-constructive-identification-1",
    "title": "Identification",
    "section": "Non-constructive Identification",
    "text": "Non-constructive Identification\n\nIdentification sometimes defined without explicit mapping from data to parameters, e.g. Hsiao (1983), Matzkin (2007)\n\n\n\n\nDefinition: Observationally Equivalent\n\n\n\nLet \\(\\mathcal{P} = \\{ P(\\cdot; s) : s \\in S \\}\\), two structures \\(s\\) and \\(\\tilde{s}\\) in \\(S\\) are observationally equivalent if they imply the same distribution for the observed data, i.e. \\[ P(B;s) = P(B; \\tilde{s}) \\] for all \\(B \\in \\sigma(X)\\).\nLet \\(\\lambda: S \\to \\R^k\\), \\(\\theta\\) is observationally equivalent to \\(\\tilde{\\theta}\\) if \\(\\exists s, \\tilde{s} \\in S\\) that are observationally equivalent and \\(\\theta = \\lambda(s)\\) and \\(\\tilde{\\theta} = \\lambda(\\tilde{s})\\)\n\nLet \\(\\Gamma(\\theta, S) = \\{P(\\dot; s) | s \\in S, \\theta = \\lambda(s) \\}\\), then \\(\\theta\\) and \\(\\tilde{\\theta}\\) are observationally equivalent iff \\(\\Gamma(\\theta,S) \\cap \\Gamma(\\tilde{\\theta}, S) \\neq \\emptyset\\)"
  },
  {
    "objectID": "identification/identification.html#non-constructive-identification-2",
    "href": "identification/identification.html#non-constructive-identification-2",
    "title": "Identification",
    "section": "Non-constructive Identification",
    "text": "Non-constructive Identification\n\n\n\nDefinition: (Non-Constructive) Identification\n\n\n\n\\(s_0 \\in S\\) is identified if there is no \\(s\\) that is observationally equivalent to \\(s_0\\)\n\\(\\theta_0\\) is identified (in \\(S\\)) if there is no observationally equivalent \\(\\theta \\neq \\theta_0\\)\n\ni.e. \\(\\Gamma(\\theta_0, S) \\cap \\Gamma(\\theta, S) = \\emptyset\\) \\(\\forall \\theta \\neq \\theta_0\\)\n\n\n\n\n\n\nCompared to constructive definition with \\(\\theta_0 = \\psi(P)\\):\n\nLess clear how to use identification to estimate\nEasier to show non-identification"
  },
  {
    "objectID": "identification/identification.html#example-multiple-regression-1",
    "href": "identification/identification.html#example-multiple-regression-1",
    "title": "Identification",
    "section": "Example: Multiple Regression",
    "text": "Example: Multiple Regression\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\\]\n\n\n\\(X = [X_1\\, X_2]'\\), if rank \\(\\Er X X' = 1\\), then \\(\\beta_1, \\beta_2\\) is observationally equivalent to any \\(\\tilde{\\beta}_1, \\tilde{\\beta}_2\\) s.t. \\[\n\\tilde{\\beta}_1 + \\tilde{\\beta}_2 = \\beta_1 + \\beta_2 \\frac{\\cov(X_1, X_2)}{\\var(X_2)}\n\\]\n\\(\\theta_0 = \\lambda( \\beta ) = \\beta_1 + \\beta_2\\) is identified if rank \\(\\Er X X' \\geq 1\\)"
  },
  {
    "objectID": "identification/identification.html#example-random-coefficients-logit",
    "href": "identification/identification.html#example-random-coefficients-logit",
    "title": "Identification",
    "section": "Example: Random Coefficients Logit",
    "text": "Example: Random Coefficients Logit\n\n\\(Y_i = 1\\{\\beta_0 + \\beta_i X_i \\geq U_i \\}\\)\n\n\\(U\\) independent \\(X_i,\\beta_i\\),\n\\(\\beta_i\\) indepedent \\(X_i\\),\n\\(F_u(z) = \\frac{e^z}{1+e^z}\\)\n\n\\(\\Er[Y|X] = \\int \\frac{e^{\\beta_0 + \\beta X_i}} {1+e^{\\beta_0 + \\beta X_i}} dF_\\beta(\\beta)\\)\nNon-constructive and constructive identification of \\(F_\\beta\\) in Fox et al. (2012)\n\n\nHard to give a brief example where non-constructive argument is required. Non constructive identification proofs typically leverage some high level mathematical result.\nChristensen (2015) is another example of non-constructive identification."
  },
  {
    "objectID": "estimation/estimation.html#reading",
    "href": "estimation/estimation.html#reading",
    "title": "Estimation",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 4, sections 1.2 and 2 (which is the basis for these slides)\nSupplemental: Erich Leo Lehmann and Romano (n.d.) , Erich L. Lehmann and Casella (2006)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#estimator",
    "href": "estimation/estimation.html#estimator",
    "title": "Estimation",
    "section": "Estimator",
    "text": "Estimator\n\n\nGiven a parameter of interest \\(\\theta_0\\), an estimator is a measurable function of an observed random vector X, i.e. \\(\\hat{\\theta} = \\tau(X)\\) for some known map \\(\\tau\\)\nAn estimate given \\(X=x\\) is \\(\\tau(x)\\)"
  },
  {
    "objectID": "estimation/estimation.html#sample-analogue-estimation",
    "href": "estimation/estimation.html#sample-analogue-estimation",
    "title": "Estimation",
    "section": "Sample Analogue Estimation",
    "text": "Sample Analogue Estimation\n\ni.i.d. observations from \\(P\\), \\(X = (X_1, ...., X_n)\\)\nconstructively identified parameter \\(\\theta_0 = \\psi(P)\\)\nempirical measure: \\[\n\\hat{P}(B) = \\frac{1}{n} \\sum_{i=1}^n 1\\{X_i \\in B \\}.\n\\]\nSample analogue estimator \\[\n\\hat{\\theta} = \\psi(\\hat{P})\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#sample-analogue-estimation---examples",
    "href": "estimation/estimation.html#sample-analogue-estimation---examples",
    "title": "Estimation",
    "section": "Sample Analogue Estimation - Examples",
    "text": "Sample Analogue Estimation - Examples\n\n\nMean\nOLS\n\n\n\nGo over some examples. Introduce empirical expectation.\nMention idea of conditions on \\(\\psi\\) and \\(\\hat{P}\\) such that estimator has good properties."
  },
  {
    "objectID": "estimation/estimation.html#maximum-likelihood-estimation",
    "href": "estimation/estimation.html#maximum-likelihood-estimation",
    "title": "Estimation",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta \\subset \\R^d \\}\\)\n\\(P_\\theta\\) dominated by \\(\\sigma\\)-finite \\(\\mu\\) with density \\(f_X(\\cdot;\\theta)\\)\nLikelihood \\(\\ell(\\cdot, X): \\Theta \\to [0,\\infty)\\) \\[\n\\ell(\\theta; X)= f(X; \\theta)\n\\]\nMaximum likelihood estimator \\[\n\\hat{\\theta}_{MLE} = \\textrm{arg}\\max_{\\theta \\in \\Theta} \\ell(\\theta;X)\n\\]\n\n\nNormal mean example.\nLog-likelihood."
  },
  {
    "objectID": "estimation/estimation.html#mle-examples",
    "href": "estimation/estimation.html#mle-examples",
    "title": "Estimation",
    "section": "MLE: Examples",
    "text": "MLE: Examples\n\n\n\\(X_i \\sim N(\\mu, 1)\\)\n\\(Y_i = \\alpha_0 + \\beta_0 X_i + \\epsilon_i\\), \\(\\epsilon_i \\sim N(0, \\sigma_0^2)\\)"
  },
  {
    "objectID": "estimation/estimation.html#mle-equivariance",
    "href": "estimation/estimation.html#mle-equivariance",
    "title": "Estimation",
    "section": "MLE: Equivariance",
    "text": "MLE: Equivariance\n\n\n\nTheorem 1.1\n\n\nIf \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\), then for any function \\(g:\\Theta \\to G\\), the MLE of \\(g(\\theta)\\) is \\(g(\\hat{\\theta})\\)."
  },
  {
    "objectID": "estimation/estimation.html#mean-squared-error",
    "href": "estimation/estimation.html#mean-squared-error",
    "title": "Estimation",
    "section": "Mean Squared Error",
    "text": "Mean Squared Error\n\nLoss function \\(L: \\R^d \\times \\Theta \\to [0,\\infty)\\) with \\(L(\\theta,\\theta)=0\\)\nRisk of at \\(\\theta_0\\) \\(\\Er[L(\\hat{\\theta}, \\theta_0)]\\)\nSquared error loss \\(L_2(\\theta, \\theta_0) = (\\theta-\\theta_0)'(\\theta-\\theta_0)\\)\nMean squared error \\[\nMSE(\\hat{\\theta}) = \\Er[ (\\theta-\\theta_0)'(\\theta-\\theta_0) ]\n\\]\nBias-variance decomposition \\[\nMSE(\\hat{\\theta}) = \\textrm{Bias}(\\hat{\\theta})'\\textrm{Bias}(\\hat{\\theta}) + \\textrm{tr}(\\var(\\hat{\\theta}))\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#setup",
    "href": "estimation/estimation.html#setup",
    "title": "Estimation",
    "section": "Setup",
    "text": "Setup\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta \\subset \\R^d \\}\\), likelihood \\(\\ell(\\theta;x) = f_X(x;\\theta)\\)\nQuestion: if an estimator is unbiased, what is the smallest possible variance?"
  },
  {
    "objectID": "estimation/estimation.html#score-equality",
    "href": "estimation/estimation.html#score-equality",
    "title": "Estimation",
    "section": "Score Equality",
    "text": "Score Equality\n\nIf \\(\\frac{\\partial}{\\partial \\theta} \\int f_X(x;\\theta) d\\mu(x) =  \\int \\frac{\\partial}{\\partial \\theta} f_X(x;\\theta) d\\mu(x)\\), then \\[\n\\int \\underbrace{\\frac{\\partial \\log \\ell(\\theta;x)}{\\partial \\theta}}_{\\text{\"score\"}=s(x,\\theta)} dP_\\theta(x) = 0\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#information-equality",
    "href": "estimation/estimation.html#information-equality",
    "title": "Estimation",
    "section": "Information Equality",
    "text": "Information Equality\n\nFischer Information \\(I(\\theta) = \\int s(x,\\theta) s(x,\\theta)' dP_\\theta(x)\\)\nIf \\(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} \\int f_X(x;\\theta) d\\mu(x) =  \\int \\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} f_X(x;\\theta)  d\\mu(x)\\), then \\[\nI(\\theta) = -\\int \\underbrace{\\frac{\\partial^2 \\ell(\\theta;x)}{\\partial \\theta \\partial \\theta'}}_{\\text{\"Hessian\"}=h(x,\\theta)} dP_\\theta(x)\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#section",
    "href": "estimation/estimation.html#section",
    "title": "Estimation",
    "section": "",
    "text": "If \\(T = \\tau(X)\\) is an unbiased estimator for \\(\\theta\\) and \\[\n\\frac{\\partial}{\\partial \\theta} \\int \\tau(x) f_X(x;\\theta) d\\mu(x) =\n\\int \\tau(x) \\frac{\\partial f_X(x,\\theta)}{\\partial \\theta\\partial \\theta'} d\\mu(x)\n\\] then \\[\n\\int \\tau(x) s(x,\\theta)'dP_\\theta(x) = I\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#cramér-rao-bound",
    "href": "estimation/estimation.html#cramér-rao-bound",
    "title": "Estimation",
    "section": "Cramér-Rao Bound",
    "text": "Cramér-Rao Bound\n\n\n\nCramér-Rao Bound\n\n\nLet \\(T = \\tau(X)\\) be an unbiased estimator, and suppose the condition of the previous slide and of the score equality hold. Then, \\[\n\\var_\\theta(\\tau(X)) \\equiv \\int \\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)\\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)' dP\\theta \\geq I(\\theta)^{-1}\n\\]"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-1",
    "href": "estimation/estimation.html#hypothesis-testing-1",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(X \\in \\mathcal{X} \\subset \\R^n\\), distribution \\(P_x \\in \\mathcal{P}\\)\nPartition \\(\\mathcal{P} = \\mathcal{P}_0 \\cup \\mathcal{P}_1\\)\nNull and alternative hypotheses:\n\n\\(H_0: \\; P_x \\in \\mathcal{P}_0\\)\n\\(H_1: \\; P_x \\in \\mathcal{P}_1\\)"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-2",
    "href": "estimation/estimation.html#hypothesis-testing-2",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nTest partitions \\(\\mathcal{X} = \\underbrace{C}_{\\text{critical region}} \\cup A\\)\n\nReject null if \\(X \\in C\\)\nOften \\(C = \\{x \\in \\mathcal{X}:  \\underbrace{\\tau(x)}_{\\text{test statistic}} &gt;  \\underbrace{c}_{\\text{critical value}} \\}\\)"
  },
  {
    "objectID": "estimation/estimation.html#hypothesis-testing-3",
    "href": "estimation/estimation.html#hypothesis-testing-3",
    "title": "Estimation",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_0)\\)=Type I error \\(=P_x(C)\\)\n\\(P(\\text{fail to reject } H_0 | P_x \\in \\mathcal{P}_1)\\)=Type II error\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_1)\\) = power\n\\(\\sup_{P_x \\in \\mathcal{P}_0} P_x(C)\\) = size of test\n\n\nIllustrate schematically how these vary with \\(C\\)"
  },
  {
    "objectID": "estimation/estimation.html#p-value",
    "href": "estimation/estimation.html#p-value",
    "title": "Estimation",
    "section": "p-value",
    "text": "p-value\n\ntest statistic \\(\\tau(X)\\) , define \\[ G_P(t)  =  P(\\tau(X) &gt; t) \\]\np-value is \\[\np= \\sup_{P \\in \\mathcal{P}_0} G_P(\\tau(X))\n\\]\n\n\n\nif \\(\\mathcal{P}_0 = \\{P_0\\}\\), critical value \\(c\\), let \\(\\alpha = G_{P_0}(c)\\), then \\(\\tau(X) &gt; c\\) iff \\(p &lt; \\alpha\\)"
  },
  {
    "objectID": "estimation/estimation.html#testing-in-parametric-family",
    "href": "estimation/estimation.html#testing-in-parametric-family",
    "title": "Estimation",
    "section": "Testing in Parametric Family",
    "text": "Testing in Parametric Family\n\nParametric family \\(\\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta\\}\\)\n\n\\(\\Theta_0 = \\{\\theta \\in \\Theta: P_\\theta \\in \\mathcal{P}_0\\}\\)\n\\(\\Theta_1 = \\{\\theta \\in \\Theta: P_\\theta \\in \\mathcal{P}_1\\}\\)\n\nHypotheses\n\n\\(H_0 : \\theta \\in \\Theta_0\\)\n\\(H_1: \\theta \\in \\Theta_1\\)\n\nPower function of test \\(C\\) \\[\\pi:\\Theta \\to [0,1] , \\;\\;  \\pi(\\theta) = P_\\theta(C)\\]\nSize \\(= \\sup_{\\theta \\in \\Theta_0} \\pi(\\theta)\\)"
  },
  {
    "objectID": "estimation/estimation.html#more-powerful",
    "href": "estimation/estimation.html#more-powerful",
    "title": "Estimation",
    "section": "More Powerful",
    "text": "More Powerful\n\n\n\nDefinition\n\n\n\nFor test \\(C_1\\) and \\(C_2\\) with same size, \\(C_1\\) is more powerful at \\(\\theta \\in \\Theta_1\\) than \\(C_2\\) if \\(P_\\theta(C_1) \\geq P_\\theta(C_2)\\)\n\\(C\\) is most powerful at \\(\\theta \\in \\Theta_1\\) if is more powerfull than any test of the same size\n\\(C\\) is uniformly most powerful if it is most powerful at any \\(\\theta \\in \\Theta_1\\)"
  },
  {
    "objectID": "estimation/estimation.html#neyman-pearson",
    "href": "estimation/estimation.html#neyman-pearson",
    "title": "Estimation",
    "section": "Neyman-Pearson",
    "text": "Neyman-Pearson\n\n\n\nLemma (Neyman-Pearson)\n\n\nLet \\(\\Theta = \\{0, 1\\}\\), \\(f_0\\) and \\(f_1\\) be densities of \\(P_0\\) and \\(P_1\\), $(x) =f_1(x)/f_0(x) $ and \\(C^* =\\{x \\in X: \\tau(x) &gt; c\\}\\). Then among all tests \\(C\\) s.t. \\(P_0(C) = P_0(C^*)\\), \\(C^*\\) is most powerful."
  },
  {
    "objectID": "estimation/estimation.html#example",
    "href": "estimation/estimation.html#example",
    "title": "Estimation",
    "section": "Example",
    "text": "Example\n\n\\(X_i \\sim N(\\mu, 1)\\)\n\\(H_0: \\mu = 0\\) against \\(H_1: \\mu = 1\\)\nFind a most powerful test\n\n\n\nWhat is the most powerful test if \\(H_1: \\mu = a\\) for \\(a&gt;0\\) instead?\n\n\n\n\nWhat is the uniformly most powerful test if \\(H_1: \\mu &gt; 0\\) ?\n\n\n\n\nWhat is the uniformly most powerful test if \\(H_1: \\mu \\neq 0\\) ?"
  },
  {
    "objectID": "asymptotics/inprobability.html#reading",
    "href": "asymptotics/inprobability.html#reading",
    "title": "Convergence in Probability",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 9\nIndependent study: Song (2021) chapters 6-7"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-in-probability-1",
    "href": "asymptotics/inprobability.html#convergence-in-probability-1",
    "title": "Convergence in Probability",
    "section": "Convergence in Probability",
    "text": "Convergence in Probability\n\n\n\nDefinition\n\n\nRandom vectors \\(X_1, X_2, ...\\) converge in probability to the random vector \\(Y\\) if for all \\(\\epsilon&gt;0\\) \\[\n\\lim_{n \\to \\infty} P\\left( \\norm{X_n - Y} &gt; \\epsilon \\right) = 0\n\\] denoted by \\(X_n \\inprob Y\\) or \\(\\plim_{n \\to \\infty} X_n = Y\\)\n\n\n\n\n\nTypical use: \\(X_n = \\hat{\\theta}_n\\), estimator from \\(n\\) observations, \\(Y=\\theta_0\\), a constant.\nHow to show that \\(\\hat{\\theta}_n \\inprob \\theta_0\\)?"
  },
  {
    "objectID": "asymptotics/inprobability.html#lp-convergence",
    "href": "asymptotics/inprobability.html#lp-convergence",
    "title": "Convergence in Probability",
    "section": "\\(L^p\\) convergence",
    "text": "\\(L^p\\) convergence\n\n\n\nDefinition\n\n\nRandom vectors \\(X_1, X_2, ...\\) converge in \\(L^p\\) to the random vector \\(Y\\) if \\[\n\\lim_{n \\to \\infty} \\Er\\left[ \\norm{X_n-Y}^p \\right] \\to 0\n\\]\n\n\n\n\n\\(p=2\\) called convergence in mean square\n\n\n\n\nMarkov’s Inequality\n\n\n\\(P(|X|&gt;\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)\n\n\n\n\n\\(P\\left( \\norm{X_n - Y} &gt; \\epsilon \\right) \\leq \\frac{\\Er[ \\norm{X_n - y}^k]} {\\epsilon_k}\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-in-lp-implies-convergence-in-probability",
    "href": "asymptotics/inprobability.html#convergence-in-lp-implies-convergence-in-probability",
    "title": "Convergence in Probability",
    "section": "Convergence in \\(L^p\\) implies convergence in probability",
    "text": "Convergence in \\(L^p\\) implies convergence in probability\n\n\n\nTheorem 1.1\n\n\nIf \\(X_n\\) converges in \\(L^p\\) to \\(Y\\), then \\(X_n \\inprob Y\\).\n\n\n\n\nAn estimator, \\(\\hat{\\theta}\\) is consistent if \\(\\hat{\\theta} \\inprob \\theta_0\\)\n\n\n\nImplication for estimators: \\[\n\\begin{aligned}\nMSE(\\hat{\\theta}_n) = & \\Er[ \\norm{\\hat{\\theta}_n - \\theta_0}^2 ] \\\\\n= & tr[\\var(\\hat{\\theta}_n)] + Bias(\\hat{\\theta}_n)'Bias(\\hat{\\theta}_n)\n\\end{aligned}\n\\]\nIf \\(MSE(\\hat{\\theta}_n) \\to 0\\), then \\(\\hat{\\theta}_n \\inprob \\theta_0\\)\nIf \\(\\lim_{n \\to \\infty} \\Er[\\hat{\\theta}_n] \\neq \\theta_0\\), then \\(\\plim \\hat{\\theta}_n \\neq \\theta_0\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#consistency-of-least-squares",
    "href": "asymptotics/inprobability.html#consistency-of-least-squares",
    "title": "Convergence in Probability",
    "section": "Consistency of Least-Squares",
    "text": "Consistency of Least-Squares\n\n\nIn \\(y = X \\beta_0 + u\\), when does \\(\\hat{\\beta} = (X'X)^{-1} X' y\\) \\(\\inprob \\beta_0\\)?\nSufficient that \\(MSE(\\hat{\\beta}) = tr[\\var(\\hat{\\beta})] + Bias(\\hat{\\beta})'Bias(\\hat{\\beta}) \\to 0\\)\n\\(\\var(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}\\) (treating \\(X\\) as non-stochastic)\n\\(tr((X'X)^{-1}) \\leq \\frac{k}{\\lambda_{ min}(X'X)}\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-in-probability-of-functions",
    "href": "asymptotics/inprobability.html#convergence-in-probability-of-functions",
    "title": "Convergence in Probability",
    "section": "Convergence in Probability of Functions",
    "text": "Convergence in Probability of Functions\n\n\n\nTheorem 2.2\n\n\nIf \\(X_n \\inprob X\\), and \\(f\\) is continuous, then \\(f(X_n) \\inprob f(X)\\)\n\n\n\n\n\n\nSlutsky’s Lemma\n\n\nIf \\(Y_n \\inprob c\\) and \\(W_n \\inprob d\\), then\n\n\\(Y_n + W_n \\inprob c+ d\\)\n\\(Y_n W_n \\inprob cd\\)\n\\(Y_n / W_n \\inprob c/d\\) if \\(d \\neq 0\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#weak-law-of-large-numbers",
    "href": "asymptotics/inprobability.html#weak-law-of-large-numbers",
    "title": "Convergence in Probability",
    "section": "Weak Law of Large Numbers",
    "text": "Weak Law of Large Numbers\n\n\n\nWeak Law of Large Numbers\n\n\nIf \\(X_1, ..., X_n\\) are i.i.d. and \\(\\Er[X^2]\\) exists, then \\[\n\\frac{1}{n} \\sum X_i \\inprob \\Er[X]\n\\]\n\n\n\n\nProof: use Markov’s inequality\nThis is the simplest to prove WLLN, but there are many variants with alternate assumptions that also imply \\(\\frac{1}{n} \\sum X_i \\inprob \\Er[X]\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#consistency-of-least-squares-revisited",
    "href": "asymptotics/inprobability.html#consistency-of-least-squares-revisited",
    "title": "Convergence in Probability",
    "section": "Consistency of Least Squares Revisited",
    "text": "Consistency of Least Squares Revisited\n\n\nIn \\(y = X \\beta_0 + u\\), when does \\(\\hat{\\beta} \\inprob \\beta_0\\)?\nTreat \\(X\\) as stochastic\n\\(\\hat{\\beta} = \\left(\\frac{1}{n} \\sum_{i=1}^n X_i X_i' \\right)^{-1} \\left(\\frac{1}{n} \\sum_{i=1}^n X_i y_i \\right)\\)\nIf WLLN applies to \\(\\frac{1}{n} \\sum_{i=1}^n X_i X_i'\\) and \\(\\frac{1}{n} \\sum_{i=1}^n X_i y_i\\) (and \\(\\Er[X_i X_i']^{-1}\\) exists)\nSufficient for i.i.d, \\(\\Er[X_i u_i] = 0\\), 4th moments of \\(X_i\\) to exist, \\(\\Er[u_i^2]\\) to exist"
  },
  {
    "objectID": "asymptotics/inprobability.html#convergence-rates-1",
    "href": "asymptotics/inprobability.html#convergence-rates-1",
    "title": "Convergence in Probability",
    "section": "Convergence Rates",
    "text": "Convergence Rates\n\n\n\nDefinition\n\n\nGiven a sequence of random variables, \\(X_1, X_2, ...\\) and constants \\(b_1, b_2, ...\\), then\n\n\\(X_n = O_p(b_n)\\) if for all \\(\\epsilon &gt; 0\\) there exists \\(M_\\epsilon\\) s.t. \\[\n\\lim\\sup P\\left(\\frac{\\norm{X_n}}{b_n} \\geq  M_\\epsilon \\right) &lt; \\epsilon\n\\]\n\\(X_n = o_p(b_n)\\) if \\(\\frac{X_n}{b_n} \\to 0\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#example",
    "href": "asymptotics/inprobability.html#example",
    "title": "Convergence in Probability",
    "section": "Example",
    "text": "Example\n\n\nReal valued \\(X_1, ..., X_n\\) i.i.d., with \\(\\Er[X] = \\mu\\), \\(\\var(X_i) = \\sigma^2\\)\nMarkov’s inequality \\[\n\\begin{aligned}\nP\\left( |\\overbrace{\\En X_i}^{\\equiv \\frac{1}{n} \\sum_{i=1}^n X_i} - \\mu | &gt; a \\right) \\leq & \\frac{\\var(\\En X_i - \\mu)}{a^2}  \\\\\n\\leq &  \\frac{\\sigma^2}{n a^2}\n\\end{aligned}\n\\]\nLet \\(a = \\epsilon n^{-\\alpha}\\), then \\[\nP\\left( \\frac{|\\En X_i - \\mu |}{n^{-\\alpha}} &gt; \\epsilon \\right) \\leq \\frac{\\sigma^2}{n^{1 - 2\\alpha}\\epsilon^2}\n\\]\n\\(|\\En X_i - \\mu | = o_p(n^{-\\alpha})\\) for \\(\\alpha \\in (0, 1/2)\\)"
  },
  {
    "objectID": "asymptotics/inprobability.html#example-1",
    "href": "asymptotics/inprobability.html#example-1",
    "title": "Convergence in Probability",
    "section": "Example",
    "text": "Example\n\nReal valued \\(X_1, ..., X_n\\) i.i.d., with \\(\\Er[X] = \\mu\\), \\(\\var(X_i) = \\sigma^2\\)\nMarkov’s inequality \\[\n\\begin{aligned}\nP\\left( |\\overbrace{\\En X_i}^{\\equiv \\frac{1}{n} \\sum_{i=1}^n X_i} - \\mu | &gt; a \\right) \\leq & \\frac{\\var(\\En X_i - \\mu)}{a^2}  \\\\\n\\leq &  \\frac{\\sigma^2}{n a^2}\n\\end{aligned}\n\\]\nLet \\(a = \\sigma \\epsilon^{-1/2} n^{-1/2}\\), \\[\nP\\left( \\frac{|\\En X_i - \\mu |}{n^{-1/2}} &gt; \\underbrace{\\sigma \\epsilon^{-1/2}}_{M_\\epsilon} \\right) \\leq \\epsilon\n\\] so \\(|\\En X_i - \\mu | = O_p(n^{-\\alpha})\\) for \\(\\alpha \\in (0, 1/2]\\)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the website for ECON 626: Econometric Theory I at UBC in 2022. The course aims to equip students with introductory knowledge in probability, statistics, and econometrics so that students become familiar with basic concepts underlying empirical and methodological research in economics.\nThis site was created using Quarto.\nSource code is available on github with a CC-by-SA license."
  },
  {
    "objectID": "asymptotics/indistribution.html#reading",
    "href": "asymptotics/indistribution.html#reading",
    "title": "Convergence in Distribution",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 10\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#convergence-in-distribution-1",
    "href": "asymptotics/indistribution.html#convergence-in-distribution-1",
    "title": "Convergence in Distribution",
    "section": "Convergence in Distribution",
    "text": "Convergence in Distribution\n\n\n\nDefinition\n\n\nRandom vectors \\(X_1, X_2, ...\\) converge in distribution to the random vector \\(X\\) if for all \\(f \\in \\underbrace{\\mathcal{C}_b}\\) (continuous and bounded) \\[\n\\Er[ f(X_n) ] \\to \\Er[f(X)]\n\\] denoted by \\(X_n \\indist X\\)\n\n\n\n\n\n\n\nTheorem 1.1\n\n\nIf \\(X_n \\indist X\\) if and only if \\(P(X_n \\leq t) \\to P(X \\leq t)\\) for all \\(t\\) where \\(P(X \\leq t)\\) is continuous"
  },
  {
    "objectID": "asymptotics/indistribution.html#convergence-in-distribution-2",
    "href": "asymptotics/indistribution.html#convergence-in-distribution-2",
    "title": "Convergence in Distribution",
    "section": "Convergence in Distribution",
    "text": "Convergence in Distribution\n\n\n\nTheorem 1.2\n\n\nIf \\(X_n \\indist X\\) and \\(X\\) is continuous, then \\[\n\\sup_{t \\in \\R^d} | P(X_n \\leq t) - P(X \\leq t) | \\to 0\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#characterizing-convergence-in-distribution",
    "href": "asymptotics/indistribution.html#characterizing-convergence-in-distribution",
    "title": "Convergence in Distribution",
    "section": "Characterizing Convergence in Distribution",
    "text": "Characterizing Convergence in Distribution\n\n\n\nLemma 1.2\n\n\n\\(X_n \\indist X\\) iff for any open \\(G \\subset \\R^d\\), \\[\n\\liminf P(X_n \\in G) \\geq P(X \\in G)\n\\]\n\n\n\n\nThis and additional characterizations of convergence in distribution are called the Portmanteau Theorem"
  },
  {
    "objectID": "asymptotics/indistribution.html#continuous-mapping-theorem",
    "href": "asymptotics/indistribution.html#continuous-mapping-theorem",
    "title": "Convergence in Distribution",
    "section": "Continuous Mapping Theorem",
    "text": "Continuous Mapping Theorem\n\n\n\nContinuous Mapping Theorem\n\n\nLet \\(X_n \\indist X\\) and \\(g\\) be continuous on a set \\(C\\) with \\(P(X \\in C) = 1\\), then \\[\ng(X_n) \\indist g(X)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#relation-to-convergence-in-probability",
    "href": "asymptotics/indistribution.html#relation-to-convergence-in-probability",
    "title": "Convergence in Distribution",
    "section": "Relation to Convergence in Probability",
    "text": "Relation to Convergence in Probability\n\n\n\nTheorem 1.4\n\n\n\nIf \\(X_n \\indist X\\), then \\(X_n = O_p(1)\\)\nIf \\(c\\) is a constant, then \\(X_n \\inprob c\\) iff \\(X_n \\indist c\\)\nIf \\(Y_n \\inprob c\\) and \\(X_n \\indist X\\), then \\((Y_n, X_n) \\indist (c, X)\\)\nIf \\(X_n \\inprob X\\), then \\(X_n \\indist X\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#slutskys-lemma",
    "href": "asymptotics/indistribution.html#slutskys-lemma",
    "title": "Convergence in Distribution",
    "section": "Slutsky’s Lemma",
    "text": "Slutsky’s Lemma\n\n\n\nTheorem 1.5 (Generalized Slutsky’s Lemma)\n\n\nIf \\(Y_n \\inprob c\\), \\(X_n \\indist X\\), and \\(g\\) is continuous, then \\[\ng(Y_n, X_n) \\indist g(c,X)\n\\]\n\n\n\n\nImplies:\n\n\\(Y_n + X_n \\indist c + X\\)\n\\(Y_n X_n \\indist c X\\)\n\\(X_n/Y_n \\indist X/c\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#levys-continuity-theorem",
    "href": "asymptotics/indistribution.html#levys-continuity-theorem",
    "title": "Convergence in Distribution",
    "section": "Levy’s Continuity Theorem",
    "text": "Levy’s Continuity Theorem\n\n\n\nLemma 2.1 (Levy’s Continuity Theorem)\n\n\n\\(X_n \\indist X\\) iff \\(\\Er[e^{i t'X_n} ] \\to \\Er[e^{i t' X} ]\\) for all \\(t \\in \\R^d\\)\n\n\n\n\nsee Döbler (2022) for a short proof\n\\(\\Er[e^{i t' X}] \\equiv \\varphi(t)\\) is the characteristic function of \\(X\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#law-of-large-numbers-revisited",
    "href": "asymptotics/indistribution.html#law-of-large-numbers-revisited",
    "title": "Convergence in Distribution",
    "section": "Law of Large Numbers Revisited",
    "text": "Law of Large Numbers Revisited\n\n\n\nLemma 2.2 (Weak Law of Large Numbers)\n\n\nIf \\(X_1, ..., X_n\\) are i.i.d. with \\(\\Er[|X_1|] &lt; \\infty\\), then \\(\\frac{1}{n} \\sum_{i=1}^n X_i \\inprob \\Er[X_1]\\)\n\n\n\n\n\n\n\nTheorem 2.2 (non-iid WLLN)\n\n\nIf \\(\\Er[X_i]=0\\), \\(\\Er[X_i X_j] = 0\\) for all \\(i \\neq j\\) and \\(\\frac{1}{n} \\max_{1 \\leq  j \\leq n} \\Er[X_j^2] \\to 0\\), then \\(\\frac{1}{n} \\sum_{i=1}^n X_i \\inprob 0\\)\n\n\n\n\n\nApply to OLS again and show \\(\\hat{\\sigma}^2\\) is consistent"
  },
  {
    "objectID": "asymptotics/indistribution.html#central-limit-theorem-1",
    "href": "asymptotics/indistribution.html#central-limit-theorem-1",
    "title": "Convergence in Distribution",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\n\n\nTheorem 2.3\n\n\nSuppose \\(X_1, ..., X_n \\in \\R\\) are i.i.d. with \\(\\Er[X_1] = \\mu\\) and \\(\\var(X_1) = \\sigma^2\\), then \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\frac{X_i - \\mu}{\\sigma} \\indist N(0,1)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#cdf",
    "href": "asymptotics/indistribution.html#cdf",
    "title": "Convergence in Distribution",
    "section": "CDF",
    "text": "CDF\n\n\nplotting code\nusing Cobweb\nplt = Plot()\nplt.layout = Config()\nfor i in axes(N)[1]\n  plt(x=x, y=Fn[i].(x), name=\"N=$(N[i])\")\nend\nplt(x=x, y=cdf.(Normal(),x), name=\"Normal CDF\")\nfig = plt()\nCobweb.save(Page(fig), \"cdf.html\")\nHTML(\"&lt;iframe src=\\\"cdf.html\\\" width=\\\"1200\\\"  height=\\\"700\\\"&gt;&lt;/iframe&gt;\")"
  },
  {
    "objectID": "asymptotics/indistribution.html#size-distortion",
    "href": "asymptotics/indistribution.html#size-distortion",
    "title": "Convergence in Distribution",
    "section": "Size Distortion",
    "text": "Size Distortion\n\n\nplotting code\np = range(0,1,length=200)\nplt = Plot()\nplt.layout = Config()\nplt.layout.yaxis.title.text=\"p - Fn(Φ^{-1}(p))\"\nplt.layout.xaxis.title.text=\"p\"\nfor i in axes(N)[1]\n  plt(x=p,y=p - Fn[i].(quantile.(Normal(),p)), name=\"N=$(N[i])\")\nend\nfig = plt()\nCobweb.save(Page(fig), \"size.html\")\nHTML(\"&lt;iframe src=\\\"size.html\\\" width=\\\"1200\\\"  height=\\\"700\\\"&gt;&lt;/iframe&gt;\")"
  },
  {
    "objectID": "asymptotics/indistribution.html#histogram",
    "href": "asymptotics/indistribution.html#histogram",
    "title": "Convergence in Distribution",
    "section": "Histogram",
    "text": "Histogram\n\n\nplotting code\nbins = range(-2.5,2.5,length=21)\nx = range(-2.5,2.5, length=1000)\nplt = Plot();\nplt.layout=Config()\nfunction Hn(x, F)\n  if (x &lt;= bins[1] || x&gt;bins[end])\n    return 0.0\n  end\n  j = findfirst( x .&lt;= bins )\n  return (F(bins[j]) - F(bins[j-1]))\nend\nplt(x=x,y=Hn.(x,x-&gt;cdf(Normal(),x)), name=\"Normal\")\nfor i in axes(N)[1]\n  plt(x=x,y=Hn.(x, Fn[i]), name=\"N=$(N[i])\")\nend\nfig = plt()\nCobweb.save(Page(fig), \"hist.html\")\nHTML(\"&lt;iframe src=\\\"hist.html\\\" width=\\\"1200\\\"  height=\\\"700\\\"&gt;&lt;/iframe&gt;\")"
  },
  {
    "objectID": "asymptotics/indistribution.html#cramér-wold-device",
    "href": "asymptotics/indistribution.html#cramér-wold-device",
    "title": "Convergence in Distribution",
    "section": "Cramér-Wold Device",
    "text": "Cramér-Wold Device\n\n\n\nLemma 2.2\n\n\nFor \\(X_n, X \\in \\R^d\\), \\(X_n \\indist X\\) iff \\(t' X_n \\indist t' X\\) for all \\(t \\in \\R^d\\)"
  },
  {
    "objectID": "asymptotics/indistribution.html#multivariate-central-limit-theorem",
    "href": "asymptotics/indistribution.html#multivariate-central-limit-theorem",
    "title": "Convergence in Distribution",
    "section": "Multivariate Central Limit Theorem",
    "text": "Multivariate Central Limit Theorem\n\n\n\nTheorem 2.4\n\n\nSuppose \\(X_1, ..., X_n\\) are i.i.d. with \\(\\Er[X_1] = \\mu \\in \\R^d\\), &lt; $ and \\(\\var(X_1) = \\Sigma &gt; 0\\), then \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n (X_i - \\mu) \\indist N(0,\\Sigma)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#i.-non-i.d.-central-limit-theorem",
    "href": "asymptotics/indistribution.html#i.-non-i.d.-central-limit-theorem",
    "title": "Convergence in Distribution",
    "section": "i. non i.d. Central Limit Theorem",
    "text": "i. non i.d. Central Limit Theorem\n\nTriangular array \\[\n\\begin{array}{ccc}\nX_{1,1}, & ..., & X{1,k(1)} \\\\\nX_{2,1}, & ..., & X{2,k(2)} \\\\\n\\vdots & & \\vdots \\\\\nX_{n,1}, & ..., & X{n,k(n)}\n\\end{array}\n\\] with \\(k(n) \\to \\infty\\) as \\(n \\to \\infty\\)\n\n\n\n\nTheorem 2.5 (Lindeberg’s Theorem)\n\n\nAssume that for each \\(n\\), \\(X_{n,1}, ..., X_{n,k(n)}\\) are independent with \\(\\Er[X_{nj}] = 0\\), and \\[\n\\frac{1}{k(n)} \\sum_{j=1}^{k(n)} \\Er[X_{nj}^2]  = 1\n\\] and for any \\(\\epsilon&gt;0\\), \\[\n\\lim_{n \\to \\infty} \\frac{1}{k(n)} \\sum_{j=1}^{k(n)} \\Er\\left[ X_{nj}^2 1\\{|X_{nj}|&gt;\\epsilon \\sqrt{k(n)}  \\right]  = 0\n\\] Then, \\[\n\\frac{1}{\\sqrt{k(n)}} \\sum_{j=1}^{k(n)} X_{n,j} \\indist N(0,1)\n\\]"
  },
  {
    "objectID": "asymptotics/indistribution.html#delta-method",
    "href": "asymptotics/indistribution.html#delta-method",
    "title": "Convergence in Distribution",
    "section": "Delta Method",
    "text": "Delta Method\n\n\n\nLemma 3.1\n\n\nSuppose \\(h:\\R^d \\to \\R\\) with \\(h(0) = 0\\), and \\(X_n \\inprob 0\\). Then for every \\(p&gt;0\\), 1. If \\(h(y) = o(\\norm{y}^p)\\) as \\(y \\to 0\\), then \\(h(X_n) = o_p(\\norm{X}_n^p)\\) 2. If \\(h(y) = O(\\norm{y}^p)\\) as \\(y \\to 0\\), then \\(h(X_n) = O_p(\\norm{X}_n^p)\\)\n\n\n\n\n\n\nTheorem 3.1 (Delta Method)\n\n\nSuppose that \\(\\hat{\\theta}\\) is a sequence of estimators of \\(\\theta_0 \\in \\R^d\\), and \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist S\n\\] Also, assume that \\(h: \\R^d \\to \\R^k\\) is differentiable at \\(\\theta_0\\), then \\[\n\\sqrt{n} \\left( h(\\hat{\\theta}) - h(\\theta_0) \\right) \\indist Dh(\\theta_0) S\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#reading",
    "href": "asymptotics/ols.html#reading",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 10\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#consistency",
    "href": "asymptotics/ols.html#consistency",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Consistency",
    "text": "Consistency\n\\[\n\\begin{align*}\n\\hat{\\beta} = & (X'X)^{-1} X' y \\\\\n= & (X'X)^{-1} X' (X \\beta + \\epsilon) \\\\\n= & \\beta + (X'X)^{-1} X' \\epsilon\n\\end{align*}\n\\]\nConsistent, \\(\\hat{\\beta} \\inprob \\beta\\), if\n\n\n\\((X'X)^{-1} X' \\epsilon \\inprob 0\\) (“high level assumption”), or\n\\(\\frac{1}{n} X'X \\inprob C\\) and \\(\\frac{1}{n} X' \\epsilon \\inprob 0\\), or"
  },
  {
    "objectID": "asymptotics/ols.html#consistency-1",
    "href": "asymptotics/ols.html#consistency-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Consistency",
    "text": "Consistency\n\\[\n\\begin{align*}\n\\hat{\\beta} = & (X'X)^{-1} X' y \\\\\n= & (X'X)^{-1} X' (X \\beta + \\epsilon) \\\\\n= & \\beta + (X'X)^{-1} X' \\epsilon\n\\end{align*}\n\\]\nConsistent, \\(\\hat{\\beta} \\inprob \\beta\\), if\n\nUsing non-iid WLLN from convergence in distribution slides (“low level assumption”):\n\nFor both \\(Z_i = X_i'\\) and \\(Z_i = \\epsilon_i\\),\n\n\\(\\Er\\left[\\left((X_i Z_i) - \\Er[X_i Z_i'] \\right) \\left((X_j Z_j) - \\Er[X_j Z_j]\\right)\\right] = 0\\) and\n\\(\\frac{1}{n} \\max_{1 \\leq i \\leq n} \\Er[ (X_i Z_i - \\Er[X_i Z_i]) (X_i Z_i - \\Er[X_i Z_i])'] \\to 0\\)\n\n\\(\\Er[X_i \\epsilon_i] = 0\\)"
  },
  {
    "objectID": "asymptotics/ols.html#asymptotic-distribution",
    "href": "asymptotics/ols.html#asymptotic-distribution",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\beta} - \\beta) = & \\sqrt{n} (X'X)^{-1} (X' \\epsilon) \\\\\n= & (\\frac{1}{n} X'X)^{-1} \\frac{1}{\\sqrt{n}} X' \\epsilon\n\\end{align*}\n\\]\n\\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\) if:\n\n\n\\(\\frac{1}{n} X'X \\inprob C\\) nonsingular, and \\(\\frac{1}{\\sqrt{n}} X' \\epsilon \\indist N(0,V)\\), or\nUsing i.i.d. CLT and WLLN:\n\n\\((X_i, \\epsilon_i)\\) i.i.d.\n\\(\\Er[X_i X_i' ]\\) is nonsingular, \\(\\Er[X_i \\epsilon_i] = 0\\), and \\(\\var(X_i \\epsilon_i) = \\Omega &gt; 0\\)\nExercise: what is \\(\\Sigma\\) under these assumptions?"
  },
  {
    "objectID": "asymptotics/ols.html#asymptotic-distribution-1",
    "href": "asymptotics/ols.html#asymptotic-distribution-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Asymptotic Distribution",
    "text": "Asymptotic Distribution\n\\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\beta} - \\beta) = & \\sqrt{n} (X'X)^{-1} (X' \\epsilon) \\\\\n= & (\\frac{1}{n} X'X)^{-1} \\frac{1}{\\sqrt{n}} X' \\epsilon\n\\end{align*}\n\\]\n\\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\) if:\n\n\nUsing Lindeberg’s CLT and non-iid WLLN:\n\n\\((X_i, \\epsilon_i) \\perp (X_j, \\epsilon_j)\\) if \\(i \\neq j\\), and\n\\(\\frac{1}{n} \\max_{1 \\leq i \\leq n} \\Er[ (X_i X_i' - \\Er[X_i X_i']) (X_i X_i' - \\Er[X_i X_i])'] \\to 0\\), and\n\\(\\frac{1}{n} \\sum_{i=1}^n \\Er[X_i \\epsilon_i^2 X_i'] = \\Omega_n\\) is non singular, and \\(\\Omega_n \\to \\Omega\\), and\n\\(\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\Er\\left[ c'( X_i \\epsilon_i^2 X_i') c 1\\{ | c'( X_i \\epsilon_i)| &gt; \\delta \\sqrt{n} \\} \\right] = 0\\) for all \\(\\delta &gt; 0\\) and \\(c \\in \\R^k\\)\nExercise: what is \\(\\Sigma\\) under these assumptions?"
  },
  {
    "objectID": "asymptotics/ols.html#estimated-variance",
    "href": "asymptotics/ols.html#estimated-variance",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Estimated Variance",
    "text": "Estimated Variance\n\nKnowing \\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\) isn’t useful, unless we know or can estimate \\(\\Sigma\\)\n\n\n\n\nLemma\n\n\nIf \\(\\hat{\\Sigma} \\inprob \\Sigma\\), \\(\\Sigma\\) is nonsingular, and \\(\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\Sigma)\\), then \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta)\\hat{\\Sigma}^{-1/2} \\indist N(0, I)\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedasticity",
    "href": "asymptotics/ols.html#heteroskedasticity",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWith i.i.d. data, \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N(0, \\overbrace{\\Er[X_i X_i']^{-1} \\var(X_i \\epsilon_i) \\Er[X_i X_i']^{-1}}^{\\Sigma} )\n\\]\n\\(Var(X_i \\epsilon_i) = \\Er[\\epsilon_i^2 X_i X_i'] = \\Er[\\Er[\\epsilon_i^2 |X_i] X_i X_i']\\)\n\n\n\n\nDefinition\n\n\n\\(\\epsilon\\) is homoskedastic if \\(\\Er[\\epsilon_i^2 | X_i] = \\sigma^2\\), otherwise \\(\\epsilon\\) is heteroskedastic."
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedasticity-1",
    "href": "asymptotics/ols.html#heteroskedasticity-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\n\nWith homoskedasticity, = ^{-1} ^2$\nWith heteroskedasticity, \\(\\Sigma = \\Er[X_i X_i']^{-1} \\var(X_i \\epsilon_i) \\Er[X_i X_i']^{-1}\\) and can be (with appropriate assumptions) consistently estimated by \\[\n\\hat{\\Sigma}^{robust} = (\\frac{1}{n} X ' X)^{-1} \\left(\\frac{1}{n} \\sum_{i=1}^n X_i X_i' \\epsilon_i^2 \\right) (\\frac{1}{n} X'X)^{-1}\n\\]\nEven with homoskedasticity, there is little downside to using \\(\\hat{\\Sigma}^{robust}\\), so always used in practice"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data\n\nusing Statistics, LinearAlgebra\n\nfunction sim(n,k; β=ones(k), σ = x-&gt;1)\n  X = randn(n,k)\n  ϵ = randn(n).*mapslices(σ, X, dims=[2])\n  y = X*β + ϵ\n  return(X,y)\nend\n\nfunction ols(X,y)\n  XXfactored = cholesky(X'*X)\n  β̂ = XXfactored \\ X'*y\n  Vr, Vh = olsvar(X, y - X*β̂, XXfactored)\n  return(β̂, Vr, Vh)\nend\n\nfunction olsvar(X, ϵ, XXf)\n  n, k = size(X)\n  iXX = inv(XXf)\n  @views @inbounds Vr = n/(n-k)*iXX*sum(X[i,:]*X[i,:]'*ϵ[i]^2 for i ∈ axes(X)[1])*iXX\n  Vh = n/(n-k)*iXX*var(ϵ)\n  return(Vr,Vh)\nend\n\nolsvar (generic function with 1 method)"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-1",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data\n\n\nCode\nusing PlotlyLight, Distributions, Cobweb\nfunction simsize(n,k,S; β=ones(k), σ=x-&gt;1)\n  z = zeros(S,2)\n  for s ∈ 1:S\n    X,y = sim(n,k,β=β,σ=σ)\n    β̂, Vr, Vh = ols(X,y)\n    z[s,1] = (β̂[1] - β[1])/sqrt(Vr[1,1])\n    z[s,2] = (β̂[1] - β[1])/sqrt(Vh[1,1])\n  end\n  p = cdf.(Normal(),z)\n  return(p, z)\nend\nfunction makeplot(p,z,n;   u = range(0,1,length=100))\n  plt = Plot()\n  plt.layout = Config()\n  plt.layout.title=\"N=$n\"\n  plt.layout.yaxis.title.text=\"x - P(asymptotic p-value &lt; x)\"\n  plt.layout.xaxis.title.text=\"x\"\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,1].&lt;=x)).(u)), name=\"Heteroskedasticity Robust\")\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,2].&lt;=x)).(u)), name=\"Homoskedasticity\")\n  return(plt())\nend\nN = [100, 500, 2500, 10_000]\nk = 2\nfor n ∈ N\n  if !isfile(\"size_$n.html\")\n    fig = makeplot(simsize(n,k,10_000)...,n)\n    Cobweb.save(Page(fig), \"size_$n.html\")\n  end\nend"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-2",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-3",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-3",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-4",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-homoskedastic-data-4",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Homoskedastic Data",
    "text": "Heteroskedastic Robust Errors with Homoskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#plot-of-residuals-vs-predictions",
    "href": "asymptotics/ols.html#plot-of-residuals-vs-predictions",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Plot of Residuals vs Predictions",
    "text": "Plot of Residuals vs Predictions\n\n\nCode\nn = 250\nk = 3\nXh,yh = sim(n,k)\nbh, _, _ = ols(Xh,yh)\neh = yh - Xh*bh\nX,y = sim(n,k, σ=x-&gt;(0.1 + norm(x'*ones(k) .+ 3)/3))\nb, _, _ = ols(X,y)\ne = y - X*b\n\nplt = Plot()\nplt.layout = Config()\nplt(x = Xh*bh, y=eh, name=\"Homoskedastic\", mode=\"markers\")\nfig =plt(x = X*b, y = e, name=\"Heteroskedastic\", mode=\"markers\")\nCobweb.save(Page(fig), \"resid.html\")\nHTML(\"&lt;iframe src=\\\"resid.html\\\" width=\\\"1000\\\"  height=\\\"650\\\"&gt;&lt;/iframe&gt;\\n\")"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data\n\n\nCode\nN = [100, 500, 2500, 10_000]\nk = 2\nfor n ∈ N\n  if !isfile(\"size_het_$n.html\")\n    fig = makeplot(simsize(n,k,10_000, σ=x-&gt;(0.1 + norm(x'*ones(k) .+ 3)/3))...,n)\n    Cobweb.save(Page(fig), \"size_het_$n.html\")\n  end\nend"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-1",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-2",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-3",
    "href": "asymptotics/ols.html#heteroskedastic-robust-errors-with-heteroskedastic-data-3",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Heteroskedastic Robust Errors with Heteroskedastic Data",
    "text": "Heteroskedastic Robust Errors with Heteroskedastic Data"
  },
  {
    "objectID": "asymptotics/ols.html#dependence",
    "href": "asymptotics/ols.html#dependence",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Dependence",
    "text": "Dependence\n\n\\(\\sqrt{n} (\\hat{\\beta} - \\beta) \\indist N(0, \\Er[X_iX_i']^{-1} V \\Er[X_i X_i']^{-1})\\) if \\(\\frac{1}{n} X'X \\inprob \\Er[X_iX_i']\\) and \\(\\frac{1}{n} X'\\epsilon \\indist N(0,V)\\)\nGenerally, \\(V = \\lim_{n \\to \\infty} \\var\\left( \\frac{1}{n} X'\\epsilon \\right)\\) \\[\n\\var\\left( \\frac{1}{n} X'\\epsilon \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\cov(X_i \\epsilon_i, X_j \\epsilon_j)\n\\]\n\\(n(n-1)/2\\) different \\(\\cov(X_i \\epsilon_i, X_j \\epsilon_j)\\), so need some restriction"
  },
  {
    "objectID": "asymptotics/ols.html#clustered-standard-errors",
    "href": "asymptotics/ols.html#clustered-standard-errors",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustered Standard Errors",
    "text": "Clustered Standard Errors\n\nPartition data into \\(G\\) groups, \\(\\{g_h\\}_{h=1}^G\\), denote group of \\(i\\) as \\(g(i)\\)\nAssume \\(\\cov(X_i \\epsilon_i, X_j \\epsilon_j) = 0\\) if \\(g(i) \\neq g(j)\\), and \\(\\Er[X_i \\epsilon_i] = 0\\)\n\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n \\sum_{j=1}^n \\cov(X_i \\epsilon_i, X_j \\epsilon_j) = & \\sum_{h=1}^G \\sum_{i \\in g_h} \\sum_{j \\in g_h} \\cov(X_i \\epsilon_i, X_j, \\epsilon_j) \\\\\n= & \\sum_{h=1}^G \\Er\\left[\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)^2 \\right]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#clustered-standard-errors-1",
    "href": "asymptotics/ols.html#clustered-standard-errors-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustered Standard Errors",
    "text": "Clustered Standard Errors\n\nStrengthening some assumptions to apply Lindeberg’s CLT to \\(\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)\\) we get\n\n\\[\n\\frac{1}{\\sqrt{G}} \\left(\\sum_{h = 1}^G \\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right) \\right) \\left( \\frac{1}{G} \\sum_{h=1}^G \\Er\\left[\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)^2 \\right] \\right)^{-1/2} \\indist N(0, I)\n\\]\n\nThus,\n\n\\[\n\\sqrt{G} (\\hat{\\beta} - \\beta) \\indist N\\left(0, \\Er[X_i X_i']^{-1} \\left(\\lim_{G \\to \\infty} \\frac{1}{G} \\sum_{h=1}^G \\Er\\left[\\left(\\sum_{i \\in g_h} X_i \\epsilon_i \\right)^2 \\right] \\right)  \\Er[X_i X_i']^{-1} \\right)\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#clustering",
    "href": "asymptotics/ols.html#clustering",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering\n\nfunction olscl(X,y, group=group)\n  XXfactored = cholesky(X'*X)\n  β̂ = XXfactored \\ X'*y\n  V = olsclvar(X, y - X*β̂, XXfactored, group=group)\n  return(β̂, V)\nend\n\nfunction olsclvar(X, ϵ, XXf; group=axes(X)[1])\n  n, k = size(X)\n  groups=unique(group)\n  G = length(groups)\n  iXX = inv(XXf)\n  @views @inbounds Vr = G/(G-k)*iXX*\n    sum(X[group.==g,:]'*ϵ[group.==g]*ϵ[group.==g]'*X[group.==g,:] for  g in groups)*iXX\n  return(Vr)\nend\n\nfunction simcluster(n,k,G; β=ones(k), σ = x-&gt;1, ρ=0.7)\n  X = randn(n,k)\n  group = rand(1:G,n)\n  for g ∈ 1:G # ensure all groups included\n    if sum(group.==g)==0\n      group[g]=g\n    end\n  end\n  X[:,1] .+= (group.&gt;(G/2))\n  u = randn(G)\n  ϵ = (ρ*u[group] + sqrt(1-ρ^2)*randn(n)).*mapslices(σ, X, dims=[2])\n  y = X*β + ϵ\n  return(X,y, group)\nend\n\nfunction simsizecluster(n,k,G,S; β=ones(k), σ=x-&gt;1, ρ=0.7)\n  z = zeros(S,2)\n  for s ∈ 1:S\n    X,y,group = simcluster(n,k,G,β=β,σ=σ, ρ=ρ)\n    β̂, Vcr = olscl(X,y, group)\n    _, Vr, _ = ols(X,y)\n    z[s,1] = (β̂[1] - β[1])/sqrt(Vcr[1,1])\n    z[s,2] = (β̂[1] - β[1])/sqrt(Vr[1,1])\n  end\n  p = hcat(cdf.(Normal(),z), cdf.(TDist(G-k),z))\n  return(p, z)\nend\n\nfunction makeplotcl(p,z,n,g; u = range(0,1,length=100))\n  plt = Plot()\n  plt.layout = Config()\n  plt.layout.title=\"N=$n, G=$g\"\n  plt.layout.yaxis.title.text=\"x - P(asymptotic p-value &lt; x)\"\n  plt.layout.xaxis.title.text=\"x\"\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,1].&lt;=x)).(u)), name=\"Clustered (Normal Distribution)\")\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,2].&lt;=x)).(u)), name=\"Heteroskedasticity Robust\")\n  plt(x=u, y=(u .- (x-&gt;mean(p[:,3].&lt;=x)).(u)), name=\"Clustered (t-Distribution)\")\n  return(plt())\nend\n\nN = [200, 200, 5000, 5000]\nG = [10, 100, 10, 100]\nk = 2\nfor (n,g) ∈ zip(N,G)\n  if !isfile(\"size_cluster_$(n)_$g.html\")\n    fig = makeplotcl(simsizecluster(n,k,g,10_000)...,n,g)\n    Cobweb.save(Page(fig), \"size_cluster_$(n)_$g.html\")\n  end\nend"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-1",
    "href": "asymptotics/ols.html#clustering-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-2",
    "href": "asymptotics/ols.html#clustering-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-3",
    "href": "asymptotics/ols.html#clustering-3",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#clustering-4",
    "href": "asymptotics/ols.html#clustering-4",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Clustering",
    "text": "Clustering"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence",
    "href": "asymptotics/ols.html#time-dependence",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence",
    "text": "Time Dependence\n\nReferences:\n\nMikusheva and Schrimpf (2007) lectures 2 and 3 and recitation 2\nDedecker et al. (2007) for comprehensive treatment"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---lln",
    "href": "asymptotics/ols.html#time-dependence---lln",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - LLN",
    "text": "Time Dependence - LLN\n\nRecall simplest LLN proof via Markov’s inequality and showing \\(\\var\\left(\\frac{1}{n} \\sum_{i=1}^n x_i\\right) \\to 0\\)\nWith dependence,\n\n\\[\n\\begin{align*}\n\\var\\left(\\frac{1}{n} \\sum_{i=1}^n x_i\\right) = & \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n \\cov(x_i,x_j) \\\\\n& \\text{assume covariance stationarity:} \\cov(x_i, x_j) = \\gamma_{|i-j|} \\\\\n= & \\frac{1}{n^2} \\left(n \\gamma_0 + 2(n-1) \\gamma_1 + \\cdots\\right) \\\\\n= & \\frac{1}{n} \\left[ \\gamma_0 + 2 \\sum_{k=1}^n \\gamma_k \\left(1 - \\frac{k}{n} \\right)  \\right]\n\\end{align*}\n\\]\nif \\(\\sum_{j=-\\infty}^{\\infty} |\\gamma_j| &lt; \\infty\\), then \\(\\frac{1}{n} \\left[ \\gamma_0 + 2 \\sum_{k=1}^n \\gamma_k \\left(1 - \\frac{k}{n}  \\right) \\right]\\to 0\\) and LLN holds"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---clt",
    "href": "asymptotics/ols.html#time-dependence---clt",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - CLT",
    "text": "Time Dependence - CLT\n\nLong run variance: \\[\n\\begin{align*}\n\\frac{1}{\\sqrt{n}} \\var\\left( \\sum_{i=1}^n x_i\\right) = &  \\gamma_0 + 2 \\sum_{k=1}^n \\gamma_k \\left(1 - \\frac{k}{n} \\right)  \\\\\n\\to & \\gamma_0 + 2 \\sum_{k=1}^\\infty \\gamma_k \\equiv \\mathcal{J} = \\text{long-run variance}\n\\end{align*}\n\\]\nWith appropriate assumptions if \\(\\Er[x_i] = 0\\) and \\(\\mathcal{J} &lt; \\infty\\), then \\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n x_i \\indist N(0,\\mathcal{J})\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---gordins-clt",
    "href": "asymptotics/ols.html#time-dependence---gordins-clt",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - Gordin’s CLT",
    "text": "Time Dependence - Gordin’s CLT\n\n\n\nCLT\n\n\nAssume \\(\\Er[y_t] = 0\\). Let \\(I_t\\) be the sigma-algebra generated by \\(\\{y_j\\}_{j=-\\infty}^t\\). Further assume:\n\n\\(y_t\\) is strictly stationary: the distribution of \\(y_{t_1}, ... , y_{t_k}\\) equals the distribution of \\(y_{t_1 + s} , ... y_{t_k+s}\\) for all \\(t_j\\) and \\(s\\)\n\\(y_t\\) is ergodic \\(\\lim_{s\\to\\infty} \\cov(g(y_t,..., y_{t+k}), h(y_{t+k+s}, ..., y_{t+k+s+l})) = 0\\) for all bounded \\(g\\), \\(h\\)\n\\(\\sum_{j=1}^\\infty \\left(\\Er\\left[ (\\Er[y_t|I_{t-j}] -  \\Er[y_t|I_{t-j-1}])^2\\right] \\right)^{1/2} &lt; \\infty\\)\n\\(\\Er[y_t | I_{t-j}] \\to 0\\) as \\(j \\to \\infty\\)\n\nThen, \\[\n\\frac{1}{\\sqrt{T}} \\sum_{t=1}^T y_t \\indist N(0,\\mathcal{J})\n\\]\n\n\n\n\nMany variations of assumptions possible, see e.g. Dedecker et al. (2007) for more"
  },
  {
    "objectID": "asymptotics/ols.html#time-dependence---ols",
    "href": "asymptotics/ols.html#time-dependence---ols",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Time Dependence - OLS",
    "text": "Time Dependence - OLS\n\nIf LLN applies \\(X'X\\) and CLT to \\(X \\epsilon\\), then \\[\n\\sqrt{n} (\\hat{\\beta} - \\beta) \\indist N(0, M^{-1} \\mathcal{J} M^{-1})\n\\] with \\(M = \\plim \\frac{1}{n} X'X\\) and \\(\\mathcal{J} = \\var(X_i \\epsilon_i) + 2\\sum_{k=1}^\\infty \\cov(X_i \\epsilon_i, X_{i+k} \\epsilon_{i+k})\\)\nConsistently estimate \\(\\mathcal{J}\\) by (Newey and West (1987)) \\[\n\\hat{\\mathcal{J}} = \\sum_{-S_n}^{S_n}k_n(j) \\underbrace{\\hat{\\gamma}_j}_{=\\frac{1}{n} \\sum_{i=1}^{n-j} (X_i \\hat{\\epsilon}_i) (X_{i+j} \\hat{\\epsilon}_{i+j})'}\n\\] with \\(k_n(j) \\to 1\\), and \\(S_n \\to \\infty\\) and \\(S_n^3/n \\to 0\\)"
  },
  {
    "objectID": "asymptotics/ols.html#wald",
    "href": "asymptotics/ols.html#wald",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Wald",
    "text": "Wald\n\nIf \\(\\sqrt{n}(\\hat{\\beta} - \\beta_0) \\indist N(0, V)\\), then \\[\n\\sqrt{n}(R \\hat{\\beta} - \\underbrace{R\\beta_0}_{=r} ) \\indist N(0, RVR')\n\\] and \\[\nW \\equiv n(R \\hat{\\beta} - r )' (RVR')^{-1} (R \\hat{\\beta} - r ) \\indist \\chi^2_{rank(R)}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#restricted-mle",
    "href": "asymptotics/ols.html#restricted-mle",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Restricted MLE",
    "text": "Restricted MLE\n\nRestricted MLE with \\(\\epsilon \\sim N(0, \\sigma^2 I_n)\\) \\[\n\\hat{\\beta}_R = \\mathrm{arg}\\max_{b: Rb = r, \\sigma} -\\frac{n}{2}(\\log 2\\pi + \\log \\sigma^2) + \\sum_{i=1}^n \\frac{-1}{2\\sigma^2} (y_i - X_i b)^2\n\\]\nFOC \\[\n\\begin{align*}\n-X'y/\\sigma^2 + X'X\\hat{\\beta}_R/\\sigma^2 + R'\\hat{\\lambda} & = 0\\\\\nR\\hat{\\beta}_R - r & = 0\n\\end{align*}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#lagrange-multiplier",
    "href": "asymptotics/ols.html#lagrange-multiplier",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Lagrange Multiplier",
    "text": "Lagrange Multiplier\n\nUnder \\(H_0\\), \\(\\lambda = 0\\), so form test statistic based on \\(\\hat{\\lambda} \\approx 0\\)\nFrom FOC: \\[\nR'\\hat{\\lambda} = X'(y - X'\\hat{\\beta}_R)/\\hat{\\sigma}^2_R = X'\\hat{\\epsilon}_r/\\hat{\\sigma}^2_R\n\\]\nTo find distribution, note that \\[\n\\begin{pmatrix}\n\\hat{\\beta}_R \\\\\n\\hat{\\lambda}/2\n\\end{pmatrix} =\n\\begin{pmatrix}\nX'X/\\sigma^2 & R' \\\\\n-R & 0\n\\end{pmatrix}^{-1}\n\\begin{pmatrix}\nX'y/\\sigma^2 \\\\\n-r\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#lagrange-multiplier-1",
    "href": "asymptotics/ols.html#lagrange-multiplier-1",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Lagrange Multiplier",
    "text": "Lagrange Multiplier\n\nso (using partitioned inverse), \\[\n\\hat{\\lambda} = (R\\hat{\\sigma}_R^2 (X'X)^{-1} R')^{-1} (R \\hat{\\beta} - r)\n\\] and \\[\n\\hat{\\beta}_R = \\hat{\\beta} - (X'X)^{-1}(R (X'X)^{-1} R)^{-1} (R \\hat{\\beta} - r)\n\\]"
  },
  {
    "objectID": "asymptotics/ols.html#lagrange-multiplier-2",
    "href": "asymptotics/ols.html#lagrange-multiplier-2",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Lagrange Multiplier",
    "text": "Lagrange Multiplier\n\nNote that \\[\n\\begin{align*}\n\\hat{\\lambda} & = (R\\hat{\\sigma}_R^2 (X'X)^{-1} R')^{-1} (R \\hat{\\beta} - r) \\\\\n& = (R\\hat{\\sigma}_R^2 (X'X)^{-1} R')^{-1} (R (X'X)^{-1} X'\\epsilon)\n\\end{align*}\n\\]\nso, with homoskedasticity, \\[\nLM = \\hat{\\lambda}'R \\hat{\\sigma}^2_R (X'X)^{-1} R' \\hat{\\lambda} \\indist \\chi^2_{rank(R)}\n\\]\nCan modify for heteroskedasticity and/or dependence"
  },
  {
    "objectID": "asymptotics/ols.html#likelihood-ratio",
    "href": "asymptotics/ols.html#likelihood-ratio",
    "title": "Asymptotic Theory of Least Squares",
    "section": "Likelihood Ratio",
    "text": "Likelihood Ratio\n\nIf \\(\\epsilon_i \\sim N(0, \\sigma^2)\\), twice the log likelihood ratio for \\(H_0: R\\beta = r\\), \\[\n\\begin{align*}\n2\\max_{b,\\sigma} & \\left[-\\frac{n}{2} \\log \\sigma^2 + \\sum_{i=1}^n \\frac{-1}{2\\sigma^2} (y_i - X_i b)^2 \\right] - \\\\\n& - 2\\max_{b,\\sigma: Rb = r} \\left[-\\frac{n}{2} \\log \\sigma^2 + \\sum_{i=1}^n \\frac{-1}{2\\sigma^2} (y_i - X_i b)^2 \\right] = \\\\\n= & -n\\log \\hat{\\sigma}^2 + n \\log\\hat{\\sigma}_R^2 \\\\\n= & -n\\log\\left(\\frac{1}{n} \\norm{y-X\\hat{\\beta}}^2\\right) + n \\log \\left(\\frac{1}{n}\\norm{y - X \\hat{\\beta}_R}^2 \\right) \\\\\n= & n \\log \\left(\\frac{\\frac{1}{n}\\norm{y-X\\hat{\\beta}}^2 + \\frac{1}{n}(\\hat{\\beta}_R-\\hat{\\beta}) X'X (\\hat{\\beta}_R-\\hat{\\beta})}{\\frac{1}{n}\\norm{y-X\\hat{\\beta}}^2}\\right) \\\\\n= & n \\log (1 + W/n) \\indist \\chi^2_{rank(R)} (\\text{with homoskedasticity})\n\\end{align*}\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#reading",
    "href": "gmm/gmm.html#reading",
    "title": "Generalized Method of Moments",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 13\nSuggested:\n\nOriginated in Hansen (1982), building on method of moments which has a longer history\nImportant early application Hansen and Singleton (1982)\nReview from Hansen (2010) ungated version\n\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\def\\arg{{\\mathrm{arg}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#moment-conditions",
    "href": "gmm/gmm.html#moment-conditions",
    "title": "Generalized Method of Moments",
    "section": "Moment Conditions",
    "text": "Moment Conditions\n\\[\n\\Er\\left[g(Z_i,\\theta_0) \\right] = 0\n\\]\n\nParameter \\(\\theta_0 \\in \\R^d\\)\nData \\(\\tilde{Z}_i \\in \\R^m\\)\nMoment function \\(g: \\R^m \\times \\R^d \\to \\R^k\\)\nIdentification: \\(\\Er\\left[g(Z_i,\\theta) \\right] = 0\\) iff \\(\\theta=\\theta_0\\)"
  },
  {
    "objectID": "gmm/gmm.html#example-iv",
    "href": "gmm/gmm.html#example-iv",
    "title": "Generalized Method of Moments",
    "section": "Example: IV",
    "text": "Example: IV\n\\[\nY_i = X_i' \\beta_0 + u_i\n\\] \\[\n\\Er\\left[Z_i(Y_i - X_i'\\beta_0) \\right] = 0\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-iv-for-nonlinear-regression",
    "href": "gmm/gmm.html#example-iv-for-nonlinear-regression",
    "title": "Generalized Method of Moments",
    "section": "Example: IV for Nonlinear Regression",
    "text": "Example: IV for Nonlinear Regression\n\\[\n\\Er\\left[Z_i(Y_i - h(X_i,\\beta_0)) \\right] = 0\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-binary-choice",
    "href": "gmm/gmm.html#example-binary-choice",
    "title": "Generalized Method of Moments",
    "section": "Example: Binary Choice",
    "text": "Example: Binary Choice\n\n\\(D_i = 1\\{X_i'\\beta_0 &gt; u_i\\}\\)\n\\(u_i \\sim N(0,1)\\) \\[\n\\Er\\left[ \\left(D_i - \\Phi(X_i'\\beta_0) \\right) h(X_i) \\right] = 0\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-consumption-and-assets",
    "href": "gmm/gmm.html#example-consumption-and-assets",
    "title": "Generalized Method of Moments",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nHansen and Singleton (1982)\nModel \\[\n\\begin{align*}\n\\max_{c_t, q_t} & \\Er\\left[ \\sum_{t=0}^\\infty \\beta^t u(c_t) | \\mathcal{I}_0 \\right] \\\\\n\\text{s.t. } & \\;\\; p_t q_t + c_t \\leq (p_t + d_t)q_{t-1} + y_t\n\\end{align*}\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-consumption-and-assets-1",
    "href": "gmm/gmm.html#example-consumption-and-assets-1",
    "title": "Generalized Method of Moments",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nCleverly rearrange first order conditions: \\[\n\\Er\\left[\\beta \\frac{u'(c_{t+1})}{u'(c_t)} \\underbrace{\\frac{p_{t+1} + d_{t+1}}{p_t}}_{R_t} | \\mathcal{I}_s \\right] = 1 \\text{ for } s \\leq t\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#example-consumption-and-assets-2",
    "href": "gmm/gmm.html#example-consumption-and-assets-2",
    "title": "Generalized Method of Moments",
    "section": "Example: Consumption and Assets",
    "text": "Example: Consumption and Assets\n\nAssume \\(u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}\\) \\[\n\\Er\\left[\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t | \\mathcal{I}_s \\right] = 1 \\text{ for } s \\leq t\n\\]\nModel implies \\[\n\\Er\\left[\\left(\\beta \\frac{c_{t+1}^{-\\gamma}}{c_t^{-\\gamma}} R_t -1 \\right)Z_t \\right] = 0\n\\] for any \\(Z_t \\in \\mathcal{I}_t\\)"
  },
  {
    "objectID": "gmm/gmm.html#identification",
    "href": "gmm/gmm.html#identification",
    "title": "Generalized Method of Moments",
    "section": "Identification",
    "text": "Identification\n\nAssumption: ID\n\\(\\exists \\theta_0 \\in \\Theta\\) s.t. \\(\\forall \\epsilon&gt;0\\), \\[\n\\inf_{\\theta: \\norm{\\theta-\\theta_0} &gt; \\epsilon} \\norm{\\Er[g(Z_i,\\theta)]} &gt; \\norm{\\Er[g(Z_i,\\theta_0)]}\n\\]\n\n\nSlightly easier assumption to verify is (loosely) \\(g(z,\\theta)\\) continuous and \\(\\theta_0\\) is unique minimizer of \\(\\norm{\\Er[g(Z_i,\\theta)]}\\), see lemma 1 in Song (2021)"
  },
  {
    "objectID": "gmm/gmm.html#estimation",
    "href": "gmm/gmm.html#estimation",
    "title": "Generalized Method of Moments",
    "section": "Estimation",
    "text": "Estimation\n\nPopulation objective function \\[\nQ^{GMM}(\\theta) = \\frac{1}{2} \\norm{\\Er[g(Z_i,\\theta)]}^2_s = \\frac{1}{2} \\Er[g(Z_i,\\theta)]'S'S\\Er[g(Z_i,\\theta)]\n\\]\nSample objective function \\[\n\\hat{Q}^{GMM}(\\theta) = \\frac{1}{2} \\En[g(Z_i,\\theta)]'S_n'S_n\\En[g(Z_i,\\theta)]\n\\]\nEstimator \\[\n\\hat{\\theta} = \\arg\\min_{\\theta \\in \\Theta}\\hat{Q}^{GMM}(\\theta)\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#consistency",
    "href": "gmm/gmm.html#consistency",
    "title": "Generalized Method of Moments",
    "section": "Consistency",
    "text": "Consistency\n\n\n\nGMM Consistency\n\n\nSuppose:\n\n\\(\\exists \\theta_0 \\in \\Theta\\) s.t. \\(\\forall \\epsilon&gt;0\\), \\(\\inf_{\\theta: \\norm{\\theta-\\theta_0} &gt; \\epsilon} \\norm{\\Er[g(Z_i,\\theta)]} &gt; \\norm{\\Er[g(Z_i,\\theta_0)]}\\)\n\\(\\sup_{\\theta \\in \\Theta} \\norm{\\En[g(Z_i,\\theta)] - \\Er[g(Z_i,\\theta)]} \\inprob 0\\)\n\\(S_n \\inprob S\\)\n\nThen \\(\\hat{\\theta} \\inprob \\theta_0\\)"
  },
  {
    "objectID": "gmm/gmm.html#asymptotic-normality",
    "href": "gmm/gmm.html#asymptotic-normality",
    "title": "Generalized Method of Moments",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\n\n\nGMM Asymptotic Normality\n\n\nSuppose:\n\n\\(\\theta_0 \\in int(\\Theta)\\), & \\(g(z,\\theta)\\) is twice continuously differentiable\n\\(\\sqrt{n} \\frac{\\partial}{\\partial \\theta} \\hat{Q}^{GMM}(\\theta_0) \\indist N(0,\\Omega)\\)\n\\(\\sup_{\\theta \\in int(\\Theta)} \\norm{\\frac{\\partial^2}{\\partial  \\theta \\partial \\theta'} \\hat{Q}^{GMM}(\\theta) - B(\\theta)} \\inprob 0\\) with \\(B(\\cdot)\\) continuous at \\(\\theta_0\\) and \\(B(\\theta_0) &gt; 0\\)\n\nThen, \\[\n\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\indist N(0, B_0^{-1} \\Omega_0 B_0^{-1})\n\\]"
  },
  {
    "objectID": "gmm/gmm.html#optimal-weighting-matrix",
    "href": "gmm/gmm.html#optimal-weighting-matrix",
    "title": "Generalized Method of Moments",
    "section": "Optimal Weighting Matrix",
    "text": "Optimal Weighting Matrix\n\n\n\nLemma 3\n\n\nLet \\[\n\\begin{align*}\nM & = (\\Gamma'C\\Gamma)^{-1} \\Gamma' C \\Sigma C \\Gamma (\\Gamma'C\\Gamma)^{-1} \\\\\nM^* & = (\\Gamma' \\Sigma^{-1} \\Gamma)^{-1}\n\\end{align*}\n\\] then \\(M \\geq M^*\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ECON 626: Econometric Theory I",
    "section": "",
    "text": "Measure slides\n\nRequired reading: Song (2021) Chapter 1\nSupplementary reading: Pollard (2002), Tao (2011)\n\nProbability slides\n\nRequired reading: Song (2021) Chapter 2\nSupplementary reading: Pollard (2002), Çinlar (2011)\nRandom Variables, and Distributions\nConditional Expectations and Conditional Distributions\n\nFamily of Distributions : we will not cover in class, read and refer to Song (2021) Chapter 3 as needed\nBasics of Inference\n\nRequired reading: Song (2021) Chapter 4\nIdentification\nEstimation\nHypothesis Testing"
  },
  {
    "objectID": "index.html#measure-and-integration",
    "href": "index.html#measure-and-integration",
    "title": "ECON 626: Econometric Theory I",
    "section": "",
    "text": "Measure slides\n\nRequired reading: Song (2021) Chapter 1\nSupplementary reading: Pollard (2002), Tao (2011)\n\nProbability slides\n\nRequired reading: Song (2021) Chapter 2\nSupplementary reading: Pollard (2002), Çinlar (2011)\nRandom Variables, and Distributions\nConditional Expectations and Conditional Distributions\n\nFamily of Distributions : we will not cover in class, read and refer to Song (2021) Chapter 3 as needed\nBasics of Inference\n\nRequired reading: Song (2021) Chapter 4\nIdentification\nEstimation\nHypothesis Testing"
  },
  {
    "objectID": "iv/iv.html#reading",
    "href": "iv/iv.html#reading",
    "title": "Instrumental Variables Estimation",
    "section": "Reading",
    "text": "Reading\n\nRequired: Song (2021) chapter 12\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]"
  },
  {
    "objectID": "iv/iv.html#model",
    "href": "iv/iv.html#model",
    "title": "Instrumental Variables Estimation",
    "section": "Model",
    "text": "Model\n\\[\nY_i = \\underbrace{X_i}_{\\in \\R^k}' \\beta_0 + u_i\n\\]\n\n\\(\\Er[u_i] = 0\\), but \\(\\Er[X_i u_i] \\neq 0\\)\nInstrument \\(Z_i \\in \\R^d\\) s.t.\n\nRelevant \\(rank(\\Er[Z_i X_i']) = k\\)\nExogenous \\(\\Er[Z_i u_i] = 0\\)"
  },
  {
    "objectID": "iv/iv.html#identification",
    "href": "iv/iv.html#identification",
    "title": "Instrumental Variables Estimation",
    "section": "Identification",
    "text": "Identification\n\nExogeneity implies \\[\n\\Er[Z_i Y_i] = \\Er[Z_i X_i']\\beta_0\n\\]\n\n\n\nIf \\(d=k\\) (exactly identified), then relevance implies \\(\\Er[Z_i X_i']\\) invertible, so \\[\n\\beta_0 = \\Er[Z_i X_i']^{-1} \\Er[Z_i Y_i]\n\\]\n\n\n\n\nFor \\(d&gt;k\\), relevance implies \\(\\Er[Z_iX_i']'\\Er[Z_iX_i']\\) invertible, so \\[\n\\beta_0 = (\\Er[Z_i X_i]' \\Er[Z_i X_i'])^{-1} \\Er[Z_i X_i']' \\Er[Z_i Y_i]\n\\]"
  },
  {
    "objectID": "iv/iv.html#method-of-moments-estimation",
    "href": "iv/iv.html#method-of-moments-estimation",
    "title": "Instrumental Variables Estimation",
    "section": "Method of Moments Estimation",
    "text": "Method of Moments Estimation\n\nWe assume \\(\\Er[Z_i u_i] = 0\\), so \\[\n\\Er[Z_i(Y_i - X_i'\\beta_0)] = 0\n\\]\nEstimate by replacing \\(\\Er\\) with \\(\\frac{1}{n}\\sum_{i=1}n\\)\n\\(d\\) equations, \\(k \\geq d\\) unknowns, so find \\[\n\\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\hat{\\beta}^{IV}) \\approx 0\n\\] by solving \\[\n\\begin{align*}\n\\hat{\\beta}^{IV} & = \\mathrm{arg}\\min_\\beta \\norm{ \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta }_{W^{1/2}}^2 \\\\\n& = \\mathrm{arg}\\min_\\beta \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)' W \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "iv/iv.html#method-of-moments-estimation-1",
    "href": "iv/iv.html#method-of-moments-estimation-1",
    "title": "Instrumental Variables Estimation",
    "section": "Method of Moments Estimation",
    "text": "Method of Moments Estimation\n\\[\n\\hat{\\beta}^{IV}\n= \\mathrm{arg}\\min_\\beta \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)' W \\left( \\frac{1}{n} \\sum_{i=1}^n Z_i(Y_i - X_i'\\beta\\right)\n\\]\n\n\n\\(\\hat{\\beta}^{IV}_W = (X'Z W Z'W)^{-1}(X'Z W Z'y)\\)"
  },
  {
    "objectID": "iv/iv.html#consistency",
    "href": "iv/iv.html#consistency",
    "title": "Instrumental Variables Estimation",
    "section": "Consistency",
    "text": "Consistency\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV}_W - \\beta_0 = & (X'Z W Z'W)^{-1}(X'Z W Z'u) \\\\\n= & \\left[ \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\right) \\right]^{-1}\n    \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i u_i\\right)\n\\end{align*}\n\\]\n\nConsistent if LLN applies to \\(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\) and \\(\\frac{1}{n}\\sum_{i=1}^n Z_i u_i\\)\n\nE.g. if i.i.d. with \\(\\Er[\\norm{X_i}^4]\\) and \\(\\Er[\\norm{Z_i}^4]\\) finite and \\(\\Er[u_i^2|Z_i=z] = \\sigma^2\\) 1\n\n\nthese are stronger than needed, for LLN, but needed for CLT"
  },
  {
    "objectID": "iv/iv.html#asymptotic-normality",
    "href": "iv/iv.html#asymptotic-normality",
    "title": "Instrumental Variables Estimation",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV}_W - \\beta_0 = & (X'Z W Z'W)^{-1}(X'Z W Z'u) \\\\\n= & \\left[ \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\right) \\right]^{-1}\n    \\left(\\frac{1}{n}\\sum_{i=1}^n X_i Z_i'\\right) W \\left(\\frac{1}{n}\\sum_{i=1}^n Z_i u_i\\right)\n\\end{align*}\n\\]\n\n\\(\\sqrt{n}(\\hat{\\beta}^{IV} - \\beta_0) \\indist N(0, V)\\) if LLN applies to \\(\\frac{1}{n}\\sum_{i=1}^n Z_i X_i'\\) and CLT to \\(\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n Z_i u_i\\)\n\nE.g. if i.i.d. with \\(\\Er[\\norm{X_i}^4]\\) and \\(\\Er[\\norm{Z_i}^4]\\) finite and \\(\\Er[u_i^2|Z_i=z] = \\sigma^2\\)\nthen \\(\\frac{1}{\\sqrt{n}} \\sum Z_i u_i \\indist N(0, \\sigma^2 \\Er[Z_iZ_i'])\\)\n\\(V = \\sigma^2 (\\Er[Z_iX_i]' W \\Er[Z_iX_i'])^{-1} (\\Er[Z_iX_i']' W \\Er[Z_i Z_i'] W \\Er[Z_i X_i']) (\\Er[Z_iX_i]' W \\Er[Z_iX_i'])^{-1}\\)"
  },
  {
    "objectID": "iv/iv.html#optimal-w",
    "href": "iv/iv.html#optimal-w",
    "title": "Instrumental Variables Estimation",
    "section": "Optimal \\(W\\)",
    "text": "Optimal \\(W\\)\n\n\n\nTheorem 2.1\n\n\n\\(W^* = \\Er[Z_iZ_i']^{-1}\\) minimizes the asymptotic variance of \\(\\hat{\\beta}^{IV}_W\\)\n\n\n\n\nEstimate \\(\\hat{W}^* = \\left(\\frac{1}{n} Z'Z\\right)^{-1}\\) \\[\n\\hat{\\beta}^{IV}  = (X'Z (Z'Z)^{-1} Z' X)^{-1} (X'Z(Z'Z)^{-1}Z'y)\n\\]"
  },
  {
    "objectID": "iv/iv.html#two-stage-least-squares",
    "href": "iv/iv.html#two-stage-least-squares",
    "title": "Instrumental Variables Estimation",
    "section": "Two Stage Least Squares",
    "text": "Two Stage Least Squares\n\\[\n\\begin{align*}\n\\hat{\\beta}^{IV} & = (X'Z (Z'Z)^{-1} Z' X)^{-1} (X'Z(Z'Z)^{-1}Z'y) \\\\\n& = (X'P_Z X)^{-1} (X' P_Z y) \\\\\n& = ((P_Z X)'(P_Z X))^{-1} ((P_Z X)'y)\n\\end{align*}\n\\]\n\nRegress \\(X\\) on \\(Z\\), let \\(\\hat{X} = P_Z X\\)\nRegress \\(y\\) on \\(\\hat{X}\\)"
  },
  {
    "objectID": "iv/iv.html#testing-overidentifying-restrictions",
    "href": "iv/iv.html#testing-overidentifying-restrictions",
    "title": "Instrumental Variables Estimation",
    "section": "Testing Overidentifying Restrictions",
    "text": "Testing Overidentifying Restrictions\n\n\\(H_0: \\Er[Z_i(Y_i - X_i'\\beta_0)] = 0\\)\n\\(k=d\\), have \\(\\En[Z_i(Y_i - X_i'\\hat{\\beta}^{IV})] = 0\\) exactly, and \\(H_0\\) is untestable\n\\(k&gt;d\\), can test\nTest statistic \\[\nJ = n \\left(\\frac{1}{n} Z'(y-X\\hat{\\beta}^{IV}) \\right)' \\hat{C} \\left(\\frac{1}{n} Z'(y-X\\hat{\\beta}^{IV}) \\right)\n\\]"
  },
  {
    "objectID": "iv/iv.html#testing-overidentifying-restrictions-1",
    "href": "iv/iv.html#testing-overidentifying-restrictions-1",
    "title": "Instrumental Variables Estimation",
    "section": "Testing Overidentifying Restrictions",
    "text": "Testing Overidentifying Restrictions\n\n\n\nTheorem 2.3\n\n\nLet \\(\\hat{C} = \\left(\\frac{1}{n} \\sum_{i=1}^n Z_iZ_i' \\hat{u}_i^2\\right)^{-1}\\). Assume:\n\n\\(\\Er[ \\norm{X_i}^4] + \\Er[\\norm{Z_i}^4] &lt; \\infty\\)\n\\(\\Er[u|Z] = \\sigma^2\\)\n\\(\\Er[Z_i Z_i']\\) is positive definite\n\nThen, \\[\nJ \\indist \\chi^2_{d-k}\n\\]"
  },
  {
    "objectID": "iv/iv.html#over-identifying-test",
    "href": "iv/iv.html#over-identifying-test",
    "title": "Instrumental Variables Estimation",
    "section": "Over-identifying Test",
    "text": "Over-identifying Test\n\nOnly has power when instruments have different covariances with \\(u\\)\n\n\n\nCode\nusing PlotlyLight, Distributions, LinearAlgebra, Cobweb\nfunction sim(n; d=3, EZu = zeros(d), Exu = 0.5, beta = 1, gamma = ones(d))\n  zu = randn(n,d)\n  Z = randn(n,d) + mapslices(x-&gt;x.*EZu, zu, dims=2)\n  xu = randn(n)\n  X = Z*gamma + xu*Exu\n  u = vec(sum(zu,dims=2) + xu + randn(n))\n  y = X*beta + u\n  return(y,X,Z)\nend\n\nbiv(y,X,Z) = (X'*Z*inv(Z'*Z)*Z'*X) \\ (X'*Z*inv(Z'*Z)*Z'*y)\n\nfunction J(y,X,Z)\n  n = length(y)\n  bhat = biv(y,X,Z)\n  uhat = y - X*bhat\n  C = inv(1/n*sum(z*z'*u^2 for (z,u) in zip(eachrow(Z),uhat)))\n  Zu = Z'*uhat/n\n  J = n*Zu'*C*Zu\nend\n\nS = 1_000\nn = 100\nj0s = [J(sim(n)...) for _ in 1:S]\nj1s = [J(sim(n,EZu=[0.,0., 3.])...) for _ in 1:S]\nj2s = [J(sim(n,EZu=[1.,1., 1.])...) for _ in 1:S]\n\nplt = Plot()\nplt(x=j0s, type=\"histogram\", name=\"E[Zu] = 0\")\nplt(x=j1s, type=\"histogram\", name=\"E[Zu] = [0,0,3]\")\nfig=plt(x=j2s, type=\"histogram\", name=\"E[Zu] = [1,1,1]\")\nCobweb.save(Page(fig), \"J.html\")\nHTML(\"&lt;iframe src=\\\"J.html\\\" width=\\\"1000\\\"  height=\\\"650\\\"&gt;&lt;/iframe&gt;\\n\")"
  },
  {
    "objectID": "iv/iv.html#simulated-distribution-of-hatbetaiv",
    "href": "iv/iv.html#simulated-distribution-of-hatbetaiv",
    "title": "Instrumental Variables Estimation",
    "section": "Simulated Distribution of \\(\\hat{\\beta}^{IV}\\)",
    "text": "Simulated Distribution of \\(\\hat{\\beta}^{IV}\\)\n\nFirst stage \\(X = Z\\gamma + e\\), simulation with \\(\\Er[Z_i Z_i] = I\\) and \\(e \\sigma N(0,0.25)\\), so first stage \\(t \\approx \\sqrt{n}\\gamma/0.5\\)\nDistribution of \\(\\hat{\\beta}^IV\\) with \\(\\gamma = 1\\), \\(\\gamma=0.2\\), and \\(\\gamma=0.1\\)\n\n\n\nCode\nfunction tiv(y,X,Z; b0 = ones(size(X,2)))\n  b = biv(y,X,Z)\n  u = y - X*b\n  V = var(u)*inv(X'*Z*inv(Z'*Z)*Z'*X)\n  (b - b0)./sqrt.(diag(V))\nend\nn = 100\nS = 10_000\nplt = Plot()\nfor g in [1, 0.2, 0.1]\n  b = [tiv(sim(n,d=1,EZu=0,gamma=g)...)[1] for _ in 1:S]\n  # crop outliers so figure looks okay\n  b .= max.(b, -4)\n  b .= min.(b, 4)\n  plt(x=b, type=\"histogram\",name=\"γ=$g\")\nend\nfig=plt(x=randn(S), type=\"histogram\", name=\"Normal\")\n\nCobweb.save(Page(fig), \"weak.html\")\nHTML(\"&lt;iframe src=\\\"weak.html\\\" width=\\\"1000\\\"  height=\\\"650\\\"&gt;&lt;/iframe&gt;\\n\")"
  },
  {
    "objectID": "iv/iv.html#weak-instruments-1",
    "href": "iv/iv.html#weak-instruments-1",
    "title": "Instrumental Variables Estimation",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nLessons from simulation:\n\nWhen \\(\\Er[Z_i X_i']\\) is small, usual asymptotic distribution is a poor approximation for the finite sample distribution of \\(\\hat{\\beta}^{IV}\\)\nThe approximation can be poor even when \\(H_0: \\gamma = 0\\) in \\(X = Z\\gamma + e\\) would be rejected\n\nCan we find a better approximation to the finite sample distribution when \\(\\Er[Z_i X_i']\\) is small?"
  },
  {
    "objectID": "iv/iv.html#irrelevant-instrument-asymptotics",
    "href": "iv/iv.html#irrelevant-instrument-asymptotics",
    "title": "Instrumental Variables Estimation",
    "section": "Irrelevant Instrument Asymptotics",
    "text": "Irrelevant Instrument Asymptotics\n\nSuppose \\(\\Er[Z_i X_i'] = 0\\)\nCLT \\[\n\\frac{1}{\\sqrt{n}} \\begin{pmatrix}\nvec(Z'X) \\\\\nZ'u\n\\end{pmatrix} \\indist \\begin{pmatrix} \\zeta_1 \\\\ \\zeta_2 \\end{pmatrix} \\sim N(0, \\Sigma)\n\\]\nThen \\[\n\\begin{align*}\n\\hat{\\beta}^{IV} - \\beta_0 = & \\left((Z'X)'(Z'Z)^{-1}(Z'X)\\right)^{-1} (Z'X)'(Z'Z)^{-1}(Z'u) \\\\\n\\indist & \\left(H' \\Er[Z_i Z_i]^{-1} H\\right)^{-1} \\left(H \\Er[Z_i Z_i']^{-1} \\zeta_2\\right)\n\\end{align*}\n\\] where \\(vec(H) = \\zeta_1\\)"
  },
  {
    "objectID": "iv/iv.html#weak-instrument-asymptotics",
    "href": "iv/iv.html#weak-instrument-asymptotics",
    "title": "Instrumental Variables Estimation",
    "section": "Weak Instrument Asymptotics",
    "text": "Weak Instrument Asymptotics\n\nLet \\(\\Er[Z_i X_i'] = \\frac{1}{\\sqrt{n}} \\Gamma\\)\nThen \\(\\frac{1}{\\sqrt{n}} Z' X = \\Gamma + H\\)\nand \\[\n\\hat{\\beta}^{IV} - \\beta_0 \\indist\n\\left((\\Gamma + H)' \\Er[Z_i Z_i]^{-1} H\\right)^{-1} \\left((\\Gamma + H) \\Er[Z_i Z_i']^{-1} \\zeta_2\\right)\n\\]\n\\(\\Gamma\\) cannot be estimated, but we can try to develop estimators and inference methods for \\(\\beta\\) that work for any \\(\\Gamma\\)"
  },
  {
    "objectID": "iv/iv.html#testing-for-relevance",
    "href": "iv/iv.html#testing-for-relevance",
    "title": "Instrumental Variables Estimation",
    "section": "Testing for Relevance",
    "text": "Testing for Relevance\n\nModel , assume \\(\\Er[W_i u_i] = 0\\) and \\(\\Er[Z_i u_i] = 0\\) \\[\nY_i = X_i'\\beta + W_i'\\beta_W + u_i\n\\]\nFirst stage \\[\nX_i = Z_i' \\pi_z + W_i' \\pi_W + \\nu_i\n\\]\nCan test \\(H_0 : \\pi_z = 0\\) vs \\(H_1 : \\pi_z \\neq 0\\) using F-test\n\nWith one instrument, \\(F = t^2\\)\nRejecting \\(H_0\\) at usual significance level is not enough for \\(\\hat{\\beta}^{IV}\\) to be well aproximated by its asymptotic normal distribution"
  },
  {
    "objectID": "iv/iv.html#testing-for-relevance-1",
    "href": "iv/iv.html#testing-for-relevance-1",
    "title": "Instrumental Variables Estimation",
    "section": "Testing for Relevance",
    "text": "Testing for Relevance\n\nStock and Yogo (2002) (table from Stock, Wright, and Yogo (2002)): first stage F &gt; threshold \\(\\approx 10\\) implies \\(Bias(\\hat{\\beta}^{IV}) &lt; 10\\% Bias(\\hat{\\beta}^{OLS})\\) and size of 5% test &lt; 15%\n\n\nswy-tab1.png"
  },
  {
    "objectID": "iv/iv.html#testing-for-relevance-2",
    "href": "iv/iv.html#testing-for-relevance-2",
    "title": "Instrumental Variables Estimation",
    "section": "Testing for Relevance",
    "text": "Testing for Relevance\n\nLee et al. (2022) : F\\(&gt;&gt;10\\) is needed in practice1\n\nThe argument here is that practitioners misuse the \\(F&gt;10\\) threshold, not that Stock and Yogo (2002) is wrong."
  },
  {
    "objectID": "iv/iv.html#identification-robust-inference",
    "href": "iv/iv.html#identification-robust-inference",
    "title": "Instrumental Variables Estimation",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\nOpinion: always do this, testing for relevance not needed\nTest \\(H_0: \\beta = \\beta_0\\) vs \\(\\beta \\neq \\beta_0\\) with Anderson-Rubin test \\[\nAR(\\beta) = n\\left(\\frac{1}{n} Z'(y-X\\beta) \\right)' \\Sigma(\\beta)^{-1} \\left(\\frac{1}{n} Z'(y - X\\beta)\\right)\n\\] where \\(\\Sigma(\\beta) = \\frac{1}{n} \\sum_{i=1}^n Z_iZ_i' (y_i - X_i'\\beta)^2\\)\n\\(\\AR(\\beta) \\indist \\chi^2_d\\) (under either weak instrument or usual asymptotics)\nSee my other notes for simulations and references"
  },
  {
    "objectID": "iv/iv.html#identification-robust-inference-1",
    "href": "iv/iv.html#identification-robust-inference-1",
    "title": "Instrumental Variables Estimation",
    "section": "Identification Robust Inference",
    "text": "Identification Robust Inference\n\nTwo downsides of AR test:\n\nAR statistic is similar to over-identifying test (\\(AR(\\hat{\\beta}^{IV}) = J\\))\n\n\nSmall (even empty) confidence region if model is misspecified\n\n\nOnly gives confidence region for all of \\(\\beta\\), not confidence intervals for single co-ordinates\n\nKleibergen’s LM and Moreira CLR tests address 1, see my other notes for simulations and references\ntF test from Lee et al. (2022) addresses 2"
  },
  {
    "objectID": "probability/probability.html#reading",
    "href": "probability/probability.html#reading",
    "title": "Probability",
    "section": "Reading",
    "text": "Reading\n\nSong (2021) chapter 2 (which is the basis for these slides)\nPollard (2002)\n\n\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\R{{\\mathbb{R}}}\n\\]"
  },
  {
    "objectID": "probability/probability.html#probability-space",
    "href": "probability/probability.html#probability-space",
    "title": "Probability",
    "section": "Probability Space",
    "text": "Probability Space\n\n\n\nDefinitions\n\n\n\nGiven a measure space \\((\\Omega ,\\mathscr{F})\\), a probability (or probability measure)\\(\\ P\\) is a measure s.t. \\(P(\\Omega )=1\\)\n\\((\\Omega ,\\mathscr{F}, P)\\) is a probability space\n\\(\\Omega\\) is a sample space\n\\(\\omega \\in \\Omega\\) is an outcome\n\\(A \\in \\mathscr{F}\\) is an event"
  },
  {
    "objectID": "probability/probability.html#random-variable",
    "href": "probability/probability.html#random-variable",
    "title": "Probability",
    "section": "Random Variable",
    "text": "Random Variable\n\n\n\nDefinition\n\n\nA random variable \\(X\\) is a measurable function from \\(\\Omega\\) to \\(\\mathbf{R}\\)"
  },
  {
    "objectID": "probability/probability.html#distribution",
    "href": "probability/probability.html#distribution",
    "title": "Probability",
    "section": "Distribution",
    "text": "Distribution\n\n\n\nDefinition\n\n\nLet \\((\\Omega ,\\mathscr{F},P)\\) be a probability space, \\(X\\) a random variable on \\((\\Omega ,\\mathscr{F})\\). A distribution \\(P_{X}\\) induced by \\(X\\) is a probability measure on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that : \\(\\forall B\\in \\mathscr{B}(\\mathbf{R})\\), \\[\nP_{X}(B)\\equiv P\\left\\{ \\omega \\in \\Omega :X(\\omega )\\in B\\right\\}\n\\]"
  },
  {
    "objectID": "probability/probability.html#distribution-function",
    "href": "probability/probability.html#distribution-function",
    "title": "Probability",
    "section": "Distribution Function",
    "text": "Distribution Function\n\n\n\nDefinition\n\n\nThe CDF of a random variable \\(X\\) with distribution \\(P_{X}\\) is defined to be a function \\(F:\\mathbf{R}\\rightarrow [0,1]\\) such that \\[\nF(t)=P_{X}\\left( (-\\infty ,t]\\right) .\n\\]"
  },
  {
    "objectID": "probability/probability.html#stochastic-dominance",
    "href": "probability/probability.html#stochastic-dominance",
    "title": "Probability",
    "section": "Stochastic Dominance",
    "text": "Stochastic Dominance\n\n\n\nDefinition\n\n\nSuppose that two random variables \\(X_1\\) and \\(X_2\\) have CDFs \\(F_1\\) and \\(F_2\\).\n\nSuppose that \\(F_1(x) \\le F_2(x)\\) for all \\(x \\in \\mathbf{R}\\). Then \\(X_1\\) first order stochastically dominates (FOSD) \\(X_2\\).\nSuppose that for all \\(y \\in \\mathbf{R}\\), \\[\n     \\int_{-\\infty}^y F_1(x) dx \\le \\int_{-\\infty}^y F_2(x) dx.\n\\] Then \\(X_1\\) second order stochastically dominates (SOSD) \\(X_2\\)"
  },
  {
    "objectID": "probability/probability.html#density",
    "href": "probability/probability.html#density",
    "title": "Probability",
    "section": "Density",
    "text": "Density\n\n\n\nDefinition\n\n\n\nLet \\(X\\) be a random variable with distribution \\(P_{X}\\). When \\(P_{X}\\ll \\lambda\\), we call \\(X\\) a continuous random variable, and call the Radon-Nikodym derivative \\(f\\equiv dP_{X}/d\\lambda\\) the (probability) density function of \\(P_{X}\\).\nWe say that \\(X\\) is a discrete random variable, if there exists a countable set \\(A\\subset \\mathbf{R}\\) and such that \\(P_{X}A^{c}=0\\)\n\n\n\n\n\nDensity wrt to other measures? e.g. dirac?"
  },
  {
    "objectID": "probability/probability.html#markovs",
    "href": "probability/probability.html#markovs",
    "title": "Probability",
    "section": "Markov’s",
    "text": "Markov’s\n\n\n\nMarkov’s Inequality\n\n\n\\(P(|X|&gt;\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)"
  },
  {
    "objectID": "probability/probability.html#jensens",
    "href": "probability/probability.html#jensens",
    "title": "Probability",
    "section": "Jensen’s",
    "text": "Jensen’s\n\n\n\nJensen’s Inequality\n\n\nSuppose that \\(g\\) is convex and \\(X\\) and \\(g(X)\\) are integrable, then \\(g(\\Er X) \\leq \\Er[g(X)]\\)\n\n\n\n\n\n\nExercise\n\n\nShow \\(\\Er[|X|^p] \\leq \\left(\\Er[|X|^q] \\right)^{p/q}\\) for all \\(0 &lt; p \\leq q\\)."
  },
  {
    "objectID": "probability/probability.html#cauchy-schwarz",
    "href": "probability/probability.html#cauchy-schwarz",
    "title": "Probability",
    "section": "Cauchy-Schwarz",
    "text": "Cauchy-Schwarz\n\n\n\nCauchy-Schwarz Inequality\n\n\n\\(\\left(\\Er[XY]\\right)^2 \\leq \\Er[X^2] \\Er[Y^2]\\)"
  },
  {
    "objectID": "probability/probability.html#generated-sigma-field",
    "href": "probability/probability.html#generated-sigma-field",
    "title": "Probability",
    "section": "Generated \\(\\sigma\\)-field",
    "text": "Generated \\(\\sigma\\)-field\n\n\\(\\sigma(X)\\) is \\(\\sigma\\)-field generated by \\(X\\)\n\nsmallest \\(\\sigma\\)-field w.r.t. which \\(X\\) is measurable\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R)\\}\\)"
  },
  {
    "objectID": "probability/probability.html#information",
    "href": "probability/probability.html#information",
    "title": "Probability",
    "section": "Information",
    "text": "Information\n\n\\(\\forall E \\in \\sigma(X)\\), observing value \\(x\\) of \\(X\\), tells us whether \\(E\\) occurred\nif \\(\\sigma(X_1) \\subset \\sigma(X_2)\\), then \\(\\sigma(X_2)\\) has more information than \\(\\sigma(X_1)\\)\n\n\nExample 4.3 in Song (2021)."
  },
  {
    "objectID": "probability/probability.html#dependence",
    "href": "probability/probability.html#dependence",
    "title": "Probability",
    "section": "Dependence",
    "text": "Dependence\n\n\n\nTheorem 4.2\n\n\nSuppose \\(\\sigma(W) \\subset \\sigma(X)\\), then \\(\\exists\\) Borel measurable \\(g\\) s.t. \\(W=g(X)\\)\n\n\n\n\nProof in Çinlar (2011) chapter 2, section 4."
  },
  {
    "objectID": "probability/probability.html#independence-1",
    "href": "probability/probability.html#independence-1",
    "title": "Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\nDefinition\n\n\n\nEvents \\(A_1, ..., A_m\\) are independent if for any sub-collection \\(A_{i_1}, ..., A_{i_s}\\) \\[\nP\\left(\\cap_{j=1}^s A_{i_j}\\right) = \\prod_{j=1}^s P(A_{i_j})\n\\]\n\\(\\sigma\\)-fields, \\(\\mathscr{F}_1, .., \\mathscr{F}_m \\subset \\mathscr{F}\\) are independent if for any \\(\\mathscr{F}_{i_1}, .., \\mathscr{F}_{i_s}\\) and \\(E_j \\in \\mathscr{F}_j\\), \\[\nP\\left(\\cap_{j=1}^s E_{i_j}\\right) = \\prod_{j=1}^s P(E_{i_j})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are independent if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are independent"
  },
  {
    "objectID": "probability/probability.html#random-vectors",
    "href": "probability/probability.html#random-vectors",
    "title": "Probability",
    "section": "Random Vectors",
    "text": "Random Vectors\n\nmeasurable \\(X: \\Omega \\to \\R^n\\)\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R^n)\\} =\\) smallest \\(\\sigma\\)-field containing \\(\\cup_{i=1}^n \\sigma(X_i)\\)\n\n\n\n\nTheorem\n\n\nSuppose that \\(X=(X_1, X_2)\\) and \\(Y=(Y_1, Y_2)\\) are independent, then \\(f(X)\\) and \\(g(Y)\\) are independent\n\n\n\n\nAs promised, an easy way to show obvious independence without any tedious calculations."
  },
  {
    "objectID": "probability/probability.html#conditional-expectation",
    "href": "probability/probability.html#conditional-expectation",
    "title": "Probability",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\n\n\n\nDefinition\n\n\nLet \\(\\mathscr{G} \\subset \\mathscr{F}\\) be \\(\\sigma\\)-fields, \\(Y\\) a random variable with \\(\\Er |Y| &lt; \\infty\\), then the conditional expectation of \\(Y\\) given \\(\\mathscr{G}\\) is \\(\\Er[Y|\\mathscr{G}](\\cdot): \\Omega \\to \\R\\) s.t.\n\n\\(\\Er[Y|\\mathscr{G}](\\cdot)\\) is \\(\\mathscr{G}\\) measurable\n\\(\\int_A \\Er[Y|\\mathscr{G}] dP = \\int_A Y dP\\) \\(\\forall A \\in \\mathscr{G}\\)\n\n\n\n\n\n\nEx: \\(\\{E_k\\}_{k=1}^m\\) partition of \\(\\Omega\\), let \\(\\mathscr{G} = \\sigma(\\{E_k\\}_{k=1}^m)\\)\n\n\\(\\Er[Y | \\mathscr{G}](\\omega) = \\sum_{k=1}^m c_k 1\\{\\omega \\in E_k\\}\\)\n\nExistence from Radon-Nikodym theorem\n\\(\\Er[Y|X] \\equiv \\Er[Y|\\sigma(X)]\\)"
  },
  {
    "objectID": "probability/probability.html#properties-of-conditional-expectation",
    "href": "probability/probability.html#properties-of-conditional-expectation",
    "title": "Probability",
    "section": "Properties of Conditional Expectation",
    "text": "Properties of Conditional Expectation\n\n\nIf \\(X\\) is \\(\\mathscr{G}\\) measurable, then \\(\\Er[XY| \\mathscr{G}] = X \\Er[Y|\\mathscr{G}]\\) a.e.\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[XY|Z] = X \\Er[Y|Z]\\)\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[\\Er[Y|Z]|X] = \\Er[Y|X]\\)\nIf \\(Y\\) and \\(X\\) are independent, then \\(\\Er[Y | X ] = \\Er[Y]\\)"
  },
  {
    "objectID": "probability/probability.html#erymathscrg-as-orthogonal-projection",
    "href": "probability/probability.html#erymathscrg-as-orthogonal-projection",
    "title": "Probability",
    "section": "\\(\\Er[Y|\\mathscr{G}]\\) as Orthogonal Projection",
    "text": "\\(\\Er[Y|\\mathscr{G}]\\) as Orthogonal Projection\n\n\n\nWarning\n\n\nLet \\((\\Omega, \\mathscr{F}, P)\\) be a probability space, \\(\\mathscr{G}\\) a sub \\(\\sigma\\)-field, then for any \\(Y \\in \\mathcal{L}^2(\\Omega, \\mathscr{F}, P) = \\{X: \\Omega \\to \\mathbb{R} \\text{ s.t. } X \\text{ }\\mathscr{F}\\text{-measurable, } \\int X^2 dP &lt; \\infty \\}\\), \\[\n\\inf_{W \\in \\mathcal{L}^2(\\Omega, \\mathscr{G}, P)} \\Er[(Y-W)^2] = \\Er[ (Y - \\Er[Y | \\mathscr{G}])^2]\n\\]"
  },
  {
    "objectID": "probability/probability.html#conditional-measure",
    "href": "probability/probability.html#conditional-measure",
    "title": "Probability",
    "section": "Conditional Measure",
    "text": "Conditional Measure\n\n\n\nDefinition\n\n\nLet \\(\\mathscr{G}\\) be a sub \\(\\sigma\\)-field of \\(\\mathscr{F}\\). Tthe conditional probability measure given \\(\\mathscr{G}\\) is defined to be a map \\(P(\\cdot \\mid \\mathscr{G})(\\cdot ):\\mathscr{F}\\times \\Omega \\rightarrow [0,1]\\) such that\n\nFor each \\(A\\in \\mathscr{F}\\), \\(P(A \\mid \\mathscr{G})(\\cdot )=\\mathbf{E}\\left[ 1\\{\\omega \\in A\\} \\mid \\mathscr{G}\\right] (\\cdot )\\), a.e.\nfor each \\(\\omega \\in \\Omega\\), \\(P(\\cdot \\mid \\mathscr{G})(\\omega )\\) is a probability measure on \\((\\Omega ,\\mathscr{F}).\\)"
  },
  {
    "objectID": "probability/probability.html#conditional-independence",
    "href": "probability/probability.html#conditional-independence",
    "title": "Probability",
    "section": "Conditional Independence",
    "text": "Conditional Independence\n\n\n\nDefinition\n\n\n\nEvents \\(A_1, ..., A_m \\in \\mathscr{F}\\) are conditionally independent given \\(\\mathscr{G}\\) if for any sub-collection, \\[\nP\\left( \\cap_{j=1}^s A_{i_j} | \\mathscr{G} \\right) = \\prod_{j=1}^s P(A_{i_j} | \\mathscr{G})\n\\]\nSub \\(\\sigma\\)-fields \\(\\mathscr{F}_1, ..., \\mathscr{F}_m\\) are conditionally independent given \\(\\mathscr{G}\\) if for any sub-collection and events, \\(E_i \\in \\mathscr{F}_i\\), \\[\nP\\left( \\cap_{j=1}^s E_{i_j} | \\mathscr{G} \\right) = \\prod_{j=1}^s P(E_{i_j} | \\mathscr{G})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are conditionally independent given \\(\\mathscr{G}\\) if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are conditionally independent given \\(\\mathscr{G}\\)"
  },
  {
    "objectID": "problemsets/02/ps02.html",
    "href": "problemsets/02/ps02.html",
    "title": "ECON 626: Problem Set 2",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\]\n\nProblem 1\nLet \\(\\Omega = \\{1, 2, 3, 4\\}\\), \\(\\mathscr{F} = 2^\\Omega\\), and \\(X = 1\\{\\omega &gt; 2 \\}\\). What is \\(\\sigma(X)\\)?\n\n\nProblem 2\nLet \\(f: \\R^2 \\to \\R\\). Assume \\(f(\\cdot, y): \\R \\to \\R\\) is measurable for all \\(y : |y - y_0| &lt; \\epsilon\\) for some \\(\\epsilon&gt;0\\). Also assume that \\(f(x, \\dot): \\R \\to \\R\\) is differentiable at \\(y_0\\) for all \\(x \\in A \\subset \\R\\) and \\(\\left\\vert \\frac{\\partial f}{\\partial y}(x, y_0) \\right \\vert \\leq M(x)\\) for all \\(x \\in A\\) and \\(M(x)\\) is integrable. Show that \\[\n\\frac{d}{dy} \\int_A f(\\cdot, y) d\\mu \\vert_{y=y_0} = \\int_A \\frac{\\partial f}{\\partial y} f(\\cdot, y_0) d\\mu.\n\\]\n\n\nProblem 3\nLet \\(X: \\Omega \\to \\R\\), \\(W: \\Omega \\to \\R\\), \\(Z: \\Omega \\to \\R\\), and \\(D: \\Omega \\to \\{0, 1\\}\\) be random variables. Let \\(Y = D X + (1-D) W\\). Suppose that \\(Y\\), \\(D\\), and \\(Z\\) are observed, but \\(X\\) and \\(W\\) are not.\n\nSuppose \\(D\\) is independent of \\(X\\), \\(W\\). Then show that \\(\\Er[X - W]\\) is identified.\nSuppose \\(Z\\) is independent of \\(X\\) and \\(W\\), and \\(\\exists E_1, E_0 \\in \\sigma(Z)\\) such that \\(P(D=1 | E_1) = 1\\) and \\(P(D=0|E_0) = 1\\). Then show that \\(\\Er[X-W]\\) is identified."
  },
  {
    "objectID": "problemsets/04/ps04.html",
    "href": "problemsets/04/ps04.html",
    "title": "ECON 626: Problem Set 4",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\]\n\nProblem 1\nThe exponential distribution has density (with respect to Lesbegue measure) \\[\nf(X;\\lambda) = \\frac{1}{\\lambda} e^{-X/\\lambda} 1\\{X &gt; 0\\}\n\\] for \\(\\lambda&gt;0\\). Suppose \\(X_1, ... , X_n\\) are independently exponential\\((\\lambda)\\) distributed .\n\nShow that the maximum likelihood estimator for \\(\\lambda\\) is \\(\\hat{\\lambda}^{MLE} = \\frac{1}{n} \\sum_{i=1}^n X_i\\)\nDerive the Cramér Rao lower bound for any unbiased estimator for \\(\\lambda\\). Is \\(\\hat{\\lambda}^{MLE}\\) a minimum variance unbiased estimator?\nFind the most powerful test of size \\(\\alpha\\) for testing \\(H_0: \\lambda = \\lambda_0\\) versus \\(H_1:\\lambda = \\lambda_1\\).\n\n\n\nProblem 2\nSuppose \\(X_1, ... , X_n\\) are independently uniformly distributed on \\((0,\\theta)\\).\n\nShow that \\(2 \\bar{X} = \\frac{2}{n} \\sum_{i=1}^n X_i\\) is an unbiased estimator for \\(\\theta\\).\nDerive the Cramér Rao lower bound for any unbiased estimator for \\(\\theta\\). Is \\(2 \\bar{X}\\) a minimum variance unbiased estimator?\nShow that \\(\\hat{\\theta} = \\frac{n+1}{n} \\max_{1 \\leq i \\leq n} X_i\\) is a minimum variance unbiased estimator for \\(\\theta\\)."
  },
  {
    "objectID": "problemsets/06/ps06.html",
    "href": "problemsets/06/ps06.html",
    "title": "ECON 626: Problem Set 6",
    "section": "",
    "text": "\\[\n\\def\\Er{{\\mathrm{E}}}\n\\def\\En{{\\mathbb{En}}}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\var{{\\mathrm{Var}}}\n\\def\\R{{\\mathbb{R}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\\]"
  },
  {
    "objectID": "problemsets/06/ps06.html#section",
    "href": "problemsets/06/ps06.html#section",
    "title": "ECON 626: Problem Set 6",
    "section": "1",
    "text": "1\nFind \\(\\plim \\hat{\\beta}_1\\), where \\(\\hat{\\beta}_1\\) is the OLS etimator."
  },
  {
    "objectID": "problemsets/06/ps06.html#section-1",
    "href": "problemsets/06/ps06.html#section-1",
    "title": "ECON 626: Problem Set 6",
    "section": "2",
    "text": "2\nIn ``Do Low Levels of Blood Lead Reduce Children’s Future Test Scores?’’ Aizer et al. (2018) examine the relationship between blood lead levels and children’s test scores. Table 4 shows estimates of \\(\\beta_1\\) from regressions of\n\\[\ntest_i = \\beta_0 + \\beta_1 lead_i + \\text{ other controls} + \\epsilon_i\n\\tag{1}\\]\nwhere \\(test_i\\) is a 3rd grade reading or math test and \\(lead_i\\) is a blood lead level measurement taken before the age of six. Some children had multiple measurements of their blood lead levels taken. Each blood lead level measurement has some error. In comparing columns (1) and (2), note that venous tests are known to have less measurement error than capillary tests, and in comparing columns (3) and (4) the average of all blood lead levels has less measurement error than a single one. Are the changes in the estimates across columns what you would expect from part part 1? Why or why not?\n\n\n\nAizer et al. (2018) table 4"
  },
  {
    "objectID": "problemsets/06/ps06.html#section-2",
    "href": "problemsets/06/ps06.html#section-2",
    "title": "ECON 626: Problem Set 6",
    "section": "3",
    "text": "3\nSuppose \\(z_i\\) is a second measurement of \\(x^*\\), [ z_i = x^*_i + e_i ] with \\(\\Er[e] = 0\\), \\(\\Er[x^* e] = 0\\), \\(\\Er[\\epsilon e] = 0\\) and \\(\\Er[e u] = 0\\). Show that \\[\n\\hat{\\beta}_1^{IV} = \\frac{\\sum_{i=1}^n (z_i - \\bar{z}) y_i} {\\sum_{i=1}^n\n   (z_i - \\bar{z}) x_i}\n\\] is a consistent estimate of \\(\\beta_1\\)."
  },
  {
    "objectID": "problemsets/06/ps06.html#section-3",
    "href": "problemsets/06/ps06.html#section-3",
    "title": "ECON 626: Problem Set 6",
    "section": "4",
    "text": "4\nTable 5 from Aizer et al. (2018), shows additional estimates of the model Equation 1. Column (1) shows standard multiple regression estimates. Columns (2) and (3) show estimates using the estimator \\(\\hat{\\beta}_1^{IV}\\) from part 3. Is the change in the estimates between columns (1) and (2) what you would expect based on parts 1 and 3? Why or why not?\n\n\n\nAizer et al. (2018) table 5"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html",
    "href": "problemsets/final2022/final2022solution.html",
    "title": "ECON 626: Final - Solutions",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\def\\En{{\\mathbb{E}_n}}\n\\def\\rank{{\\mathrm{rank}}}\n\\newcommand{\\inpr}{ \\overset{p^*_{\\scriptscriptstyle n}}{\\longrightarrow}}\n\\def\\inprob{{\\,{\\buildrel p \\over \\rightarrow}\\,}}\n\\def\\indist{\\,{\\buildrel d \\over \\rightarrow}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\]\nYou have 150 minutes to complete the exam. The last two pages have some possibly useful formulas.\nThere are 100 total points. There are three questions labeled more difficult that might take longer than others, and you should not spend too much time on these questions until you have answered all the others."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#ols-5-points",
    "href": "problemsets/final2022/final2022solution.html#ols-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "OLS (5 points)",
    "text": "OLS (5 points)\nMore difficult\nTable 2 shows OLS estimates of Equation 1. The row labeled ``Effect of 1 SD change in frequency downwind’’ reports \\(\\hat{\\beta} \\times \\sqrt{ \\frac{1}{n} \\sum_i (w_i -  \\bar{w})^2}\\). Sadly, there are no standard errors reported for these estimates. Find the asymptotic distribution of \\(\\hat{\\beta} \\times \\sqrt{ \\frac{1}{n} \\sum_i (w_i -  \\bar{w})^2}\\). To simplify this part, you may pretend that \\(x_i\\) is not in the model, assume that observations are independent, and assume that \\(\\Er[\\epsilon_i | w_i] = 0\\).\n\nSolution. With the simplifications, \\(\\hat{\\beta} = (W'W)^{-1} (W'y)\\), and \\(\\hat{\\beta} - \\beta = (W'W)^{-1} W'\\epsilon\\).\nLet \\(\\hat{\\sigma}^2 = \\frac{1}{n} \\sum (w_i - \\bar{w})^2\\). Assume that the data is iid, \\(\\Er[w_i^2 \\epsilon_i^2] &lt; \\infty\\) and \\(\\Er[w_i^4]&lt;\\infty\\). Then, the CLT applies to \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\), so\n\\[\n\\begin{aligned}\n\\sqrt{n} \\begin{pmatrix} \\hat{\\beta} - \\beta \\\\ \\hat{\\sigma}^2 - \\var(w) \\end{pmatrix} \\indist & N\\left(0,\n\\begin{pmatrix} \\Er[w^2]^{-2}\\var(w\\epsilon) & \\cov((w_i - \\Er[w])^2, \\Er[w^2]^{-1}w_i\\epsilon_i) \\\\\n& \\var((w_i - \\Er[w])^2) \\end{pmatrix} \\right) \\\\\n\\indist & N\\left(0, \\begin{pmatrix} \\Er[w^2]^{-2}\\var(w\\epsilon) & 0 \\\\\n0 & \\var((w_i - \\Er[w])^2) \\end{pmatrix} \\right)\n\\end{aligned}\n\\]\nwhere the second line follows from the assumption that \\(\\Er[\\epsilon|w]=0\\).\nNow, we can use the delta method on \\(f(\\beta,\\sigma^2) = \\beta \\sqrt{\\sigma^2}\\), so \\[\n\\sqrt{n} (\\hat{\\beta} \\sqrt{\\hat{\\sigma}^2} - \\beta\\sigma) \\indist N\\left(0, \\sqrt{\\sigma^2}\\Er[w^2]^{-2} \\var(w\\epsilon)+ \\frac{\\beta}{4 \\sigma^2} \\var((w_i - \\Er[w])^4)\\right)\n\\]"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#endogeneity-5-points",
    "href": "problemsets/final2022/final2022solution.html#endogeneity-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Endogeneity (5 points)",
    "text": "Endogeneity (5 points)\nGive one reason why \\(\\Er[w_i \\epsilon_i]\\) might not be zero, and speculate on whether \\(\\Er[w_i \\epsilon_i]\\) is likely to be positive or negative.\n\nSolution. Locations near highways are likely cheaper and have lower income residents. Income could also affect mortality directly. Thus, \\(w\\) could be negatively correlated with income, and income negatively related to mortality. This suggest \\(\\Er[w_i \\epsilon_i] &gt; 0\\)."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#instrument-5-points",
    "href": "problemsets/final2022/final2022solution.html#instrument-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument (5 points)",
    "text": "Instrument (5 points)\nAs an instrument for \\(w_i\\), Anderson (2019) uses ``orientation to the nearest major highway, [encoded] as a set of seven dummy variables. Each dummy variable represents a 45-degree range (e.g., 22.5 degrees to 67.5 degrees, 67.5 degrees to 112.5 degrees, etc.).’’ Let \\(z_i\\) denote these instruments. Does this instrument address the reason for endogeneity you gave in the previous part? Do you believe that \\(\\Er[z_i\\epsilon_i] = 0\\)?\n\nSolution. No this instrument does not address the possible endogeneity of location. It seems unlikely that \\(\\Er[z_i \\epsilon_i] = 0\\).\nTo be fair to Anderson (2019), he addresses this sort of concern by controlling for location fixed effects, and census block characteristics, including income."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#first-stage-5-points",
    "href": "problemsets/final2022/final2022solution.html#first-stage-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "First-Stage (5 points)",
    "text": "First-Stage (5 points)\nTable 3 shows estimates of \\[\nw_i = z_i \\alpha + x_i \\gamma + \\nu_i\n\\tag{2}\\] What important assumption about the instruments can we check with this regression? Should we be concerned about this assumption? What should we do about it?\n\nSolution. We can check for instrument relevance in the first stage. Although, the first stage F-statistic of 30 exceeds the Stock and Yogo (2002) rule of thumb of 10, 30 is now considered too low for standard inference methods to be appropriate. We should use an identification robust inference method. Since the model is overidentified, the KLM or CLR test would be the best choices.\nAny answer that mentions weak identification and using identification robust inference is fine here. For example, mentioning just the AR test would okay."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#sls-5-points",
    "href": "problemsets/final2022/final2022solution.html#sls-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "2SLS (5 points)",
    "text": "2SLS (5 points)\nTable 4 shows 2SLS estimates of Equation 1. One reason Anderson gives for using an instrument is that not all census blocks have weather stations, so there is measurement error in \\(w_i\\). Is the change in estimates between Table 2 and Table 4 what would be expected from measurement error? (Your answer can freely refer to results shown in lecture or problem sets without proof).\n\nSolution. Yes, this is what we would expect. Measurement error biases OLS towards 0. We generally see that the estimates in Table 2 have smaller magnitude than table 4."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#dependence-5-points",
    "href": "problemsets/final2022/final2022solution.html#dependence-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Dependence (5 points)",
    "text": "Dependence (5 points)\nThe observations are from many adjacent census blocks (roughly the same as a city block) in Los Angeles. Briefly, how does this affect the consistency and asymptotic distribution of \\(\\hat{\\beta}\\)? What, if anything, in the tables above needs to be calculated differently than with independent observations?\n\nSolution. The observations are likely dependent. As long as the dependence is not too extreme, a LLN will still apply and consistency is unaffected. However, the asymptotic variance will be different than in the independent case. The standard errors should be (and are) adjusted for spatial dependence."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#estimated-alpha-10-points",
    "href": "problemsets/final2022/final2022solution.html#estimated-alpha-10-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Estimated \\(\\alpha\\) (10 points)",
    "text": "Estimated \\(\\alpha\\) (10 points)\nAssume that \\(\\theta\\) is known, \\(\\Er[\\epsilon_i g(X_i,\\theta)] = 0\\), and \\(\\Er[\\epsilon_i]=0\\). Let \\(\\tilde{X}_i = g(X_i,\\theta)\\). Assume \\(\\Er[\\tilde{X}_i\\tilde{X}_i']\\) is nonsingular, and \\(\\var(\\epsilon_i | \\tilde{X}_i) = \\sigma^2\\).\nSuppose you have an estimate \\(\\hat{\\alpha}\\), such that \\(\\sqrt{n}(\\hat{\\alpha}-\\alpha) \\indist N(0,\\Omega)\\) and, for simplicity, is independent of \\(X\\) and \\(Y\\). Find the asymptotic distribution of \\[\n\\hat{\\beta}^{OLS} = (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' h(Y,\\hat{\\alpha})\n\\] where, \\(h(Y,\\hat{\\alpha}) = \\left(h(Y_1,\\hat{\\alpha}), ...,  h(Y_n,\\hat{\\alpha}) \\right)'\\)\nHint: consider subtracting and adding \\((\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}' h(Y,\\alpha)\\) to \\(\\hat{\\beta} - \\beta\\).\n\nSolution. Proceeding as suggested by the hint: \\[\n\\begin{align*}\n\\hat{\\beta} - \\beta = & (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' \\left(h(Y, \\hat{\\alpha}) - h(Y,\\alpha) \\right) + (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}' h(Y,\\alpha) - \\beta \\\\\n= & (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' \\left(h(Y, \\hat{\\alpha}) - h(Y,\\alpha) \\right) + (\\tilde{X}'\\tilde{X})^{-1}\\tilde{X}' \\epsilon \\\\\n= & (\\tilde{X}'\\tilde{X})^{-1} \\tilde{X}' \\left(D_\\alpha h(Y, \\alpha)(\\hat{\\alpha} - \\alpha) + \\epsilon + o_p(\\hat{\\alpha}-\\alpha) \\right)\n\\end{align*}\n\\] where the last line used a first order expansion, and \\(D_\\alpha(Y,\\alpha)\\) denotes the \\(n \\times dim(\\alpha)\\) matrix of derivatives of \\(h\\) with respect to \\(\\alpha\\) at each \\(Y_i\\).\nMultiplying by \\(\\sqrt{n}\\), using the convergence of \\(\\hat{\\alpha}\\), its independence from \\(\\epsilon\\), and applying a CLT to \\(\\tilde{X}'\\epsilon\\), we have \\[\n\\sqrt{n}(\\hat{\\beta} - \\beta) \\indist N\\left(0, \\Er[\\tilde{X}_i\\tilde{X}_i]^{-1} \\Er[\\tilde{X}_i D_\\alpha h(Y,\\alpha)] \\Omega \\Er[\\tilde{X}_i D_\\alpha h(Y,\\alpha)]'\\Er[\\tilde{X}_i\\tilde{X}_i]^{-1} + \\Er[\\tilde{X}_i\\tilde{X}_i]^{-1} \\sigma^2 \\right)\n\\]\nYou need not have made this extra observation, but interestingly, simply ignoring the estimated \\(\\alpha\\) and using the usual heteroskedasticity robust standard error would lead to the same distribution. Doing this, the error terms becomes \\(\\epsilon + h(Y_i,\\hat{\\alpha}) - h(Y_i, \\alpha)\\), and then everything just happens to work out."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#estimated-theta-5-points",
    "href": "problemsets/final2022/final2022solution.html#estimated-theta-5-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Estimated \\(\\theta\\) (5 points)",
    "text": "Estimated \\(\\theta\\) (5 points)\nMore difficult\nNow, assume \\(\\alpha\\) is known, and suppose you have an estimate \\(\\hat{\\theta}\\), such that \\(\\sqrt{n}(\\hat{\\theta}-\\theta) \\indist N(0,\\Omega)\\) and, for simplicity, is independent of \\(X\\) and \\(Y\\). Find the asymptotic distribution of \\[\n\\hat{\\beta}^{OLS} = (g(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} g(X,\\hat{\\theta})' h(Y,\\alpha)\n\\]\nHint: adding and subtracting is useful here too. Note that: \\[\n\\begin{aligned}\ng(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} g(X,\\hat{\\theta})' h(Y,\\alpha) = &\n\\left[(g(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} -\n(g(X,\\theta)'g(X,\\theta))^{-1} \\right]g(X,\\hat{\\theta})' h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} \\left[g(X,\\hat{\\theta}) - g(X,\\theta)\\right]'h(Y,\\alpha) +\n(g(X,\\theta)'g(X,\\theta))^{-1} g(X,\\theta) h(Y,\\alpha)\n\\end{aligned}\n\\]\n\nSolution. The idea here is very similar to part a, but the notation gets heavy.\n\\(g(X,\\theta)\\) is already a matrix, and we haven’t defined a way to denote the derivative of a matrix. Here, I’ll just write everything in terms of partial derivatives with respect to components of \\(\\theta\\), which avoids the problem.\nUsing the hint, we have \\[\n\\begin{aligned}\n\\hat{\\beta} - \\beta = &\n\\left[(g(X,\\hat{\\theta})'g(X,\\hat{\\theta}))^{-1} -\n(g(X,\\theta)'g(X,\\theta))^{-1} \\right]g(X,\\hat{\\theta})' h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} \\left[g(X,\\hat{\\theta}) - g(X,\\theta)\\right]'h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} g(X,\\theta) \\epsilon \\\\\n= & \\left[\\sum_{j=1}^J (\\hat{\\theta}_j - \\theta_j) \\underbrace{(-2) (g(X,\\theta)'g(X,\\theta))^{-1} \\frac{\\partial g(X,\\theta)}{\\partial \\theta_j}' g(X,\\theta) (g(X,\\theta)'g(X,\\theta))^{-1}}_{\\equiv D_{j}^{gg-}} + o_p(\\hat{\\theta} - \\theta)\\right]g(X,\\hat{\\theta})' h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} \\left[\\sum_{j=1}^J \\frac{\\partial g(X,\\theta)}{\\partial \\theta_j}(\\hat{\\theta}_j - \\theta_j) + o_p(\\hat{\\theta} - \\theta)\\right]'h(Y,\\alpha) + \\\\\n& + (g(X,\\theta)'g(X,\\theta))^{-1} g(X,\\theta) \\epsilon \\\\\n\\end{aligned}\n\\] \\[\n\\begin{aligned}\n\\indist & N\\left(0, \\sum_{j=1}^J \\sum_{\\ell=1}^J \\omega_{j,l} \\left( \\begin{array}{l} \\Er[D^{gg-}_j] \\Er[g(X,\\theta)'h(Y,\\alpha)] \\Er[g(X,\\theta)'h(Y,\\alpha)]'\\Er[D^{gg-}_\\ell]' + \\\\\n+ \\Er[g(X,\\theta)'g(X,\\theta)]^{-1} \\Er[\\frac{\\partial g(X,\\theta)}{\\partial \\theta_j}'h(Y,\\alpha)] \\Er[\\frac{\\partial g(X,\\theta)}{\\partial \\theta_\\ell}'h(Y,\\alpha)] \\Er[g(X,\\theta)'g(X,\\theta)]^{-1}  \\end{array} \\right) + \\Er[g(X,\\theta)'g(X,\\theta)]^{-1} \\sigma^2 \\right)\n\\end{aligned}\n\\] where \\(\\omega_{j,\\ell}\\) are the entries of \\(\\Omega\\)."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#instrument-9-points",
    "href": "problemsets/final2022/final2022solution.html#instrument-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument (9 points)",
    "text": "Instrument (9 points)\nYou suspect \\(x_i\\) is related to \\(u_i\\), but you have another variable, \\(z_i \\in \\R\\), such that \\(\\Er[u_i | z_i] = 0\\). Use this assumption to construct an estimator for \\(\\beta = (\\beta_0, \\cdots, \\beta_k)'\\).\n\nSolution. Mean independence, \\(\\Er[u_i | z_i] = 0\\) , implies \\(\\Er[u_i f(z_i)] = 0\\) for any function \\(f\\). Since the model has \\(k+1\\) parameters, we should choose at least \\(k+1\\) functions to use. For concreteness, let \\[\nZ_i = \\left(1, \\cos(z_i), \\cos(2z_i), \\cdots \\cos(k z_i) \\right)'\n\\] and use the usual IV estimator, \\[\n\\hat{\\beta} = (Z'X)^{-1}(Z'y)\n\\] where \\(X\\) is defined similarly as \\(Z\\).\nFor this to identify \\(\\beta\\), we must also assume relevance, \\(\\rank(\\Er[Z_i X_i']) = k+1\\)."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#consistency-9-points",
    "href": "problemsets/final2022/final2022solution.html#consistency-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Consistency (9 points)",
    "text": "Consistency (9 points)\nShow that your estimator from part a is consistent.1 State any additional assumptions.\n\nSolution. Substituting the model in for \\(y\\), we have \\[\n\\begin{align*}\n\\hat{\\beta} = & \\beta + (Z'X)^{-1}(Z'u) \\\\\n\\hat{\\beta} = & \\beta + (\\frac{1}{n}Z'X)^{-1}(\\frac{1}{n} Z'u) \\\\\n\\plim \\hat{\\beta} = & \\beta\n\\end{align*}\n\\] where, for the last line, we need an LLN to apply to \\(Z'X\\) and \\(Z'u\\). For this, it is sufficient that data be i.i.d. and \\(\\Er[\\norm{Z_i X_i'}]\\) and \\(\\Er[\\norm{Z_i u_i}]\\) are finite. We also need the relevance condition as in part a."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#asymptotic-distribution-9-points",
    "href": "problemsets/final2022/final2022solution.html#asymptotic-distribution-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Asymptotic Distribution (9 points)",
    "text": "Asymptotic Distribution (9 points)\nFind the asymptotic distribution of your estimator from part a.2 State any additional assumptions.\n\nSolution. As in the previous part, \\[\n\\begin{align*}\n\\sqrt{n}(\\hat{\\beta} - \\beta) =  & (\\frac{1}{n}Z'X)^{-1}(\\frac{1}{\\sqrt{n}} Z'u) \\\\\n\\indist & N\\left(0, \\Er[Z_i X_i']^{-1} \\Er[Z_iZ_i'u_i^2] \\Er[X_i Z_i']^{-1} \\right)\n\\end{align*}\n\\] where we again need an LLN for \\(Z'X\\) and the CLT to apply to \\(Z'u\\). A sufficient assumption, along with what was assumed for previous parts, is that \\(\\Er[\\norm{Z_i u_i}^2]\\) is finite."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#convergence-rate-3-points",
    "href": "problemsets/final2022/final2022solution.html#convergence-rate-3-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Convergence Rate (3 points)",
    "text": "Convergence Rate (3 points)\nMore difficult\nSuppose now that \\(k\\) increases with \\(n\\). To simplify, assume that \\(\\Er[u_i | x_i] = 0\\), and \\(x_i \\sim U(-\\pi,\\pi)\\), so that \\[\n\\Er[\\cos(j x_i)\\cos( \\ell x_i)] = \\begin{cases} 0 & \\text{ if } j \\neq \\ell \\\\\n1 & \\text{ if } j = \\ell\n\\end{cases}\n\\]. Let \\[\nX = \\begin{pmatrix} 1 & \\cos(x_i) & \\cdots & \\cos(k x_i) \\\\\n\\vdots & & & \\vdots \\\\\n1 & \\cos(x_n) & \\cdots & \\cos(k x_n)\n\\end{pmatrix}\n\\] Find \\(c(n,k)\\) such that \\(\\norm{\\hat{\\beta}^{OLS} - \\beta} = O_p(c(n,k))\\).\n\nSolution. A technically correct, but not useful answer, would be to give some increasing \\(c(n,k)\\), like \\(c(n,k) = n!k!\\). Then \\(\\norm{\\hat{\\beta} - \\beta}/c(n,k) \\inprob 0\\), so \\(\\norm{\\hat{\\beta} -\\beta} = o_p(c(n,k))\\) which also implies \\(O_p(c(n,k))\\). I guess such an answer is fine.\nMy intention was to ask for the smallest \\(c(n,k)\\) such that \\(\\norm{\\hat{\\beta} -\\beta} = O_p(c(n,k))\\). Let’s do that.\nNote that \\[\n\\begin{aligned}\n\\norm{\\hat{\\beta} - \\beta} = & \\norm{(\\frac{1}{n} X'X)^{-1}\\frac{1}{n}X'u} \\\\\nP\\left( \\norm{\\hat{\\beta} - \\beta} &gt; a \\right) = & P\\left( \\norm{(\\frac{1}{n} X'X)^{-1}\\frac{1}{n}X'u} &gt; a \\right) \\\\\n\\leq & \\frac{1}{a^j} \\Er\\left[\\norm{(\\frac{1}{n} X'X)^{-1}\\frac{1}{n}X'u}^j \\right]\n\\end{aligned}\n\\] where the last line follows from Markov’s inequality.\nUsing the fact that for a square matrix and any vector, \\(\\norm{A x} \\leq \\lambda^\\max(A) \\norm{x}\\) where \\(\\lambda^\\max(A)\\) is the maximal eigenvalue of \\(A\\), we have\n\\[\n\\begin{aligned}\nP\\left( \\norm{\\hat{\\beta} - \\beta} &gt; a \\right) \\leq & \\frac{1}{a^j} \\Er\\left[\\lambda^\\max(\\frac{1}{n} X'X)^{-j} \\norm{\\frac{1}{n}X'u}^j \\right] \\\\\n\\leq & \\frac{1}{a^j} \\left(\\Er\\left[\\lambda^\\max(\\frac{1}{n} X'X)^{-2j} \\right] \\Er\\left[\\norm{\\frac{1}{n} X'u}^{2j}\\right] \\right)^{1/2} \\\\\n\\leq & \\frac{1}{a^j} \\left(\\lambda^\\max\\left(\\Er\\left[\\frac{1}{n} X'X \\right]\\right)^{-2j} \\Er[\\norm{\\frac{1}{n} X'u}^{2j}] \\right)^{1/2} \\\\\n\\leq & \\frac{1}{a^j} \\left(\\Er[(\\frac{1}{n^2} u'X X'u)^{j}] \\right)^{1/2} \\\\\n\\leq & \\frac{1}{a} \\sqrt{\\frac{k \\sigma^2}{n}}\n\\end{aligned}\n\\]\nwhere we used the Cauchy-Schwarz inequality, then Jensen’s inequality (\\(\\lambda^\\max(A)^{-1}\\) is concave), and then the fact that \\(\\Er[X'X/n] = I_{k+1}\\). Finally, we set \\(j=1\\).\nThus, we get that \\(\\norm{\\hat{\\beta} - \\beta} = O_p(\\sqrt{k/n})\\)"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#ols-is-inconsistent-8-points",
    "href": "problemsets/final2022/final2022solution.html#ols-is-inconsistent-8-points",
    "title": "ECON 626: Final - Solutions",
    "section": "OLS is Inconsistent (8 points)",
    "text": "OLS is Inconsistent (8 points)\nLet \\(\\hat{\\beta}^{OLS} = \\frac{\\sum X_i Y_i}{\\sum X_i^2}\\) be the least squares estimator. Let \\(\\pi = P(X_i^*=1)\\). Compute \\(\\plim \\hat{\\beta}^{OLS}\\) in terms of \\(p\\), \\(\\pi\\), and \\(\\beta\\).\n\nSolution. \\[\n\\begin{aligned}\n\\plim \\hat{\\beta}^{OLS} = & \\plim \\frac{\\sum X_i Y_i}{\\sum X_i^2} \\\\\n= & \\plim \\frac{\\sum X_i (X_i^* \\beta + \\epsilon_i)}{\\sum X_i^2} \\\\\n= & \\frac{\\Er[X_i X_i^*]}{\\Er[X_i^2]} \\beta \\\\\n= & \\frac{\\Er[X_i | X_i^*=1]P(X_i^*=1)}{P(X_i=1)} \\beta \\\\\n= & \\frac{p \\pi}{P(X_i=1|X_i^*=1)P(X_i^*=1) +  P(X_i=1|X_i^*=0)P(X_i^*=0)} \\beta \\\\\n= & \\frac{p \\pi}{p \\pi + (1-p)(1-\\pi)} \\beta\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#instrument-8-points",
    "href": "problemsets/final2022/final2022solution.html#instrument-8-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Instrument? (8 points)",
    "text": "Instrument? (8 points)\nSuppose you also observe \\(Z_i \\in \\{0,1\\}\\) with \\[\nP(Z_i = 1 | X_i^* = 1) = P(Z_i=0 | X_i^*=0) = q\n\\] where \\(q&gt;1/2\\), and \\(Z_i\\) and \\(X_i\\) are independent conditional on \\(X_i^*\\). Let \\(\\hat{\\beta}^{IV} = \\frac{\\sum Z_i Y_i}{\\sum Z_i X_i}\\) be the instrumental variable estimator. Compute \\(\\plim \\hat{\\beta}^{IV}\\)\n\nSolution. \\[\n\\begin{aligned}\n\\plim \\hat{\\beta}^{OLS} = & \\plim \\frac{\\sum Z_i Y_i}{\\sum Z_i X_i} \\\\\n= & \\plim \\frac{\\sum Z_i (X_i^* \\beta + \\epsilon_i)}{\\sum Z_i X_i} \\\\\n= & \\frac{\\Er[Z_i X_i^*]}{\\Er[Z_i X_i]} \\beta \\\\\n= & \\frac{q \\pi}{pq \\pi + (1-p)(1-q)(1-\\pi)} \\beta\n\\end{aligned}\n\\]\nSo with this form of classification error, neither OLS nor IV is consistent."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#or-something-else-9-points",
    "href": "problemsets/final2022/final2022solution.html#or-something-else-9-points",
    "title": "ECON 626: Final - Solutions",
    "section": "Or Something Else? (9 points)",
    "text": "Or Something Else? (9 points)\nDescribe how \\(X\\), \\(Y\\), and \\(Z\\) could be used to estimate \\(\\beta\\).\nHint: think of \\(\\beta\\), \\(p\\), \\(q\\), and \\(\\pi = P(X^*_i = 1)\\) as four parameters to estimate, and come up with four moment conditions that involve these parameters.\n\nSolution. The idea here is to use GMM. There are many possible combinations of moments to use. From the previous two parts, we know that \\(\\Er[X_i Y_i] = p\\pi\\beta\\) and \\(\\Er[Z_i Y_i] = q \\pi \\beta\\). We also know that \\(\\Er[X_i] = p \\pi + (1-p)(1-\\pi)\\) and \\(\\Er[Z_i] = q \\pi + (1-q)(1-\\pi)\\). Hence, we can estimate \\(\\beta\\) by solving \\[\n\\begin{aligned}\n\\En[X_i Y_i] & = \\hat{p}\\hat{\\pi} \\hat{\\beta} \\\\\n\\En[Z_i Y_i] & = \\hat{q}\\hat{\\pi} \\hat{\\beta} \\\\\n\\En[X_i] & = \\hat{p} \\hat{\\pi} + (1-\\hat{p})(1-\\hat{\\pi}) \\\\\n\\En[Z_i] & = \\hat{q} \\hat{\\pi} + (1-\\hat{q})(1-\\hat{\\pi})\n\\end{aligned}\n\\] It’s not clear whether this system of equations has a unique solution. In general, it might not. The restrictions that \\(p,q \\in [1/2,1]\\) and \\(\\pi \\in (0,1)\\) might help pin down the correct solution.\nYour answer need not have gone any further, but to ensure identification, note that \\(\\Er[X_i Z_i Y_i] = p q \\pi \\beta\\), so we can identify \\(p\\) (and \\(q\\)) by \\(\\Er[XZY] / \\Er[ZY]\\). Given \\(p\\), \\(\\pi\\) can be identified from \\(\\Er[X_i]\\). Given \\(p\\) and \\(\\pi\\), we can recover \\(\\beta\\) from \\(\\Er[XY]\\). This constructive identification argument can be turned into an estimator by replacing expecations with sample averages."
  },
  {
    "objectID": "problemsets/final2022/final2022solution.html#footnotes",
    "href": "problemsets/final2022/final2022solution.html#footnotes",
    "title": "ECON 626: Final - Solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you could not solve part a, suppose your estimator was \\(\\hat{\\beta} = (W'X)^{-1} (W'y)\\) for some \\(n \\times k+1\\) matrix of variables of \\(W\\).↩︎\nIf you could not solve part a, suppose your estimator was \\(\\hat{\\beta} = (W'X)^{-1} (W'y)\\) for some \\(n \\times (k+1)\\) matrix of variables, \\(W\\).↩︎"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html",
    "href": "problemsets/midterm_review/midterm_review.html",
    "title": "ECON 626: Midterm Review",
    "section": "",
    "text": "\\[\n\\def\\R{{\\mathbb{R}}}\n\\def\\Er{{\\mathrm{E}}}\n\\def\\var{{\\mathrm{Var}}}\n\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert}\n\\def\\cov{{\\mathrm{Cov}}}\n\\]"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#measure-theory",
    "href": "problemsets/midterm_review/midterm_review.html#measure-theory",
    "title": "ECON 626: Midterm Review",
    "section": "Measure Theory",
    "text": "Measure Theory\n\nMeasure Space\n\nA set \\(\\Omega\\)\nA collection of subsets, \\(\\mathscr{F}\\), of \\(\\Omega\\) that is a \\(\\sigma\\)-field (aka \\(\\sigma\\)-algebra) , that is\n\n\\(\\Omega \\in \\mathscr{F}\\)\nIf \\(A \\in \\mathscr{F}\\), then \\(A^c \\in \\mathscr{F}\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\), then \\(\\cup_{j=1}^\\infty A_j \\in \\mathscr{F}\\)\n\nA measure, \\(\\mu: \\mathcal{F} \\to [0, \\infty]\\) s.t.\n\n\\(\\mu(\\emptyset) = 0\\)\nIf \\(A_1, A_2, ... \\in \\mathscr{F}\\) are pairwise disjoint, then \\(\\mu\\left(\\cup_{j=1}^\\infty A_j \\right) = \\sum_{j=1}^\\infty \\mu(A_j)\\)\n\n\nGiven a topology on \\(\\Omega\\), the Borel \\(\\sigma\\)-field, \\(\\mathscr{B}(\\Omega)\\), is the smallest \\(\\sigma\\)-field containing all open subsets of \\(\\Omega\\).\n\n\\(f: \\Omega \\to \\mathbf{R}\\) is (\\(\\mathscr{F}\\)-)measurable if \\(\\forall\\) \\(B \\in \\mathscr{B}(\\mathbf{R})\\), \\(f^{-1}(B) \\in \\mathscr{F}\\)\n\n\n\n\n\n\n\nTip\n\n\n\nChapter 1, exercises 1.1, 1.2,\n\n\n\n\nLesbegue Integral\nThe Lesbegue integral satisfies:\n\nIf \\(f \\geq 0\\) a.e., then \\(\\int f d\\mu \\geq 0\\)\nLinearity: \\(\\int (af + bg) d\\mu = a\\int f d\\mu + b \\int g d \\mu\\)\n\n\nMeasure \\(\\nu\\) is absolutely continuous with respect to \\(\\mu\\) if for \\(A \\in \\mathscr{F}\\), \\(\\mu(A) = 0\\) implies \\(\\nu(A) = 0\\)\n\ndenotate as \\(\\nu \\ll \\mu\\)\n\\(\\mu\\) is called a dominating measure\n\n\n\nRadon-Nikodym Derivative\nLet \\((\\Omega,\\mathscr{F},\\mu)\\) be a measure space, and let \\(\\nu\\) and \\(\\mu\\) be \\(\\sigma\\)-finite measures defined on \\(\\mathscr{F}\\) and \\(\\nu \\ll \\mu\\). Then there is a nonnegative measurable function \\(f\\) such that for each set \\(A\\in \\mathscr{F}\\), \\[\n\\nu (A)=\\int_{A}fd\\mu\n\\] For any such \\(f\\) and \\(g\\), \\(\\mu (\\{\\omega \\in \\Omega:f(\\omega )\\neq g(\\omega )\\})=0\\)\n\n\n\n\n\n\nTip\n\n\n\nChapter 1, exercises 3.1, 3.2\n\n\n\n\n\nConvergence Theorems\n\nContinuity of Measure\nSuppose that \\(\\{E_{n}\\}\\) is a monotone sequence of events. Then \\[\n\\mu \\left( \\lim_{n\\rightarrow \\infty}E_{n}\\right) =\\lim_{n\\rightarrow \\infty }\\mu (E_{n}).\n\\]\n\n\nMonotone Convergence Theorem\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\uparrow f(\\omega )\\), then \\(\\int f_{n}d\\mu \\uparrow \\int fd\\mu\\) as \\(n\\rightarrow \\infty\\)\n\n\nFatou’s Lemma\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, \\(f_{n}\\geq 0\\), then \\[\n\\int \\left( \\text{liminf}_{n\\rightarrow \\infty }f_{n}d\\mu \\right) \\leq \\text{liminf}_{n\\rightarrow \\infty }\\int f_{n}d\\mu\n\\]\n\n\nDominated Convergence Theorem\nIf \\(f_n:\\Omega \\to \\mathbf{R}\\) are measurable, and for each \\(\\omega \\in \\Omega\\), \\(f_{n}(\\omega )\\rightarrow f(\\omega ).\\) Furthermore, for some \\(g\\geq 0\\) such that \\(\\int gd\\mu &lt;\\infty\\), \\(|f_{n}|\\leq g\\) for each \\(n\\geq 1\\). Then, \\(\\int f_{n}d\\mu \\rightarrow \\int fd\\mu\\)"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#random-variables",
    "href": "problemsets/midterm_review/midterm_review.html#random-variables",
    "title": "ECON 626: Midterm Review",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable \\(X\\) is a measurable function from \\(\\Omega\\) to \\(\\mathbf{R}\\)\n\nDistribution\nLet \\((\\Omega ,\\mathscr{F},P)\\) be a probability space, \\(X\\) a random variable on \\((\\Omega ,\\mathscr{F})\\). A distribution \\(P_{X}\\) induced by \\(X\\) is a probability measure on \\((\\mathbf{R},\\mathscr{B}(\\mathbf{R}))\\) such that : \\(\\forall B\\in \\mathscr{B}(\\mathbf{R})\\), \\[\nP_{X}(B)\\equiv P\\left\\{ \\omega \\in \\Omega :X(\\omega )\\in B\\right\\}\n\\]\n\n\nCDF\nThe CDF of a random variable \\(X\\) with distribution \\(P_{X}\\) is defined to be a function \\(F:\\mathbf{R}\\rightarrow [0,1]\\) such that \\[\nF(t)=P_{X}\\left( (-\\infty ,t]\\right) .\n\\]\n\n\nPDF\nLet \\(X\\) be a random variable with distribution \\(P_{X}\\). When \\(P_{X}\\ll \\lambda\\), we call \\(X\\) a continuous random variable, and call the Radon-Nikodym derivative \\(f\\equiv dP_{X}/d\\lambda\\) the (probability) density function of \\(P_{X}\\).\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercise 2.1, 2.2, 2.3"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#inequalities",
    "href": "problemsets/midterm_review/midterm_review.html#inequalities",
    "title": "ECON 626: Midterm Review",
    "section": "Inequalities",
    "text": "Inequalities\n\nMarkov’s Inequality\n\\(P(|X|&gt;\\epsilon) \\leq \\frac{\\Er[|X|^k]}{\\epsilon^k}\\) \\(\\forall \\epsilon &gt; 0, k &gt; 0\\)\n\n\nJensen’s Inequality\nSuppose that \\(g\\) is convex and \\(X\\) and \\(g(X)\\) are integrable, then \\(g(\\Er X) \\leq \\Er[g(X)]\\)\n\n\nCauchy-Schwarz Inequality\n\\(\\left(\\Er[XY]\\right)^2 \\leq \\Er[X^2] \\Er[Y^2]\\)\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercise 3.1"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#dependence-and-information",
    "href": "problemsets/midterm_review/midterm_review.html#dependence-and-information",
    "title": "ECON 626: Midterm Review",
    "section": "Dependence and Information",
    "text": "Dependence and Information\n\nGenerated \\(\\sigma\\)-field\n\n\\(\\sigma(X)\\) is \\(\\sigma\\)-field generated by \\(X\\)\n\nsmallest \\(\\sigma\\)-field w.r.t. which \\(X\\) is measurable\n\\(\\sigma(X) = \\{X^{-1}(B): B \\in \\mathscr{B}(\\R)\\}\\)\n\n\n\n\nInformation\n\n\\(\\forall E \\in \\sigma(X)\\), observing value \\(x\\) of \\(X\\), tells us whether \\(E\\) occurred\nif \\(\\sigma(X_1) \\subset \\sigma(X_2)\\), then \\(\\sigma(X_2)\\) has more information than \\(\\sigma(X_1)\\)\n\n\n\nDependence\nSuppose \\(g:\\R \\to \\R\\) is Borel measurable, then \\(\\sigma(g(X)) \\subset \\sigma(X)\\)\nSuppose \\(\\sigma(W) \\subset \\sigma(X)\\), then \\(\\exists\\) Borel measurable \\(g\\) s.t. \\(W=g(X)\\)\n\n\nIndependence\n\nEvents \\(A_1, ..., A_m\\) are independent if for any sub-collection \\(A_{i_1}, ..., A_{i_s}\\) \\[\nP\\left(\\cap_{j=1}^s A_{i_j}\\right) = \\prod_{j=1}^s P(A_{i_j})\n\\]\n\\(\\sigma\\)-fields, \\(\\mathscr{F}_1, .., \\mathscr{F}_m \\subset \\mathscr{F}\\) are independent if for any \\(\\mathscr{F}_{i_1}, .., \\mathscr{F}_{i_s}\\) and \\(E_j \\in \\mathscr{F}_j\\), \\[\nP\\left(\\cap_{j=1}^s E_{i_j}\\right) = \\prod_{j=1}^s P(E_{i_j})\n\\]\nRandom variables \\(X_1, ..., X_m\\) are independent if \\(\\sigma(X_1), ..., \\sigma(X_m)\\) are independent\n\nSuppose that \\(X=(X_1, X_2)\\) and \\(Y=(Y_1, Y_2)\\) are independent, then \\(f(X)\\) and \\(g(Y)\\) are independent\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercises 4.1-4.8"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#conditional-expectation",
    "href": "problemsets/midterm_review/midterm_review.html#conditional-expectation",
    "title": "ECON 626: Midterm Review",
    "section": "Conditional Expectation",
    "text": "Conditional Expectation\nLet \\(\\mathscr{G} \\subset \\mathscr{F}\\) be \\(\\sigma\\)-fields, \\(Y\\) a random variable with \\(\\Er |Y| &lt; \\infty\\), then the conditional expectation of \\(Y\\) given \\(\\mathscr{G}\\) is \\(\\Er[Y|\\mathscr{G}](\\cdot): \\Omega \\to \\R\\) s.t.\n\n\\(\\Er[Y|\\mathscr{G}](\\cdot)\\) is \\(\\mathscr{G}\\) measurable\n\\(\\int_A \\Er[Y|\\mathscr{G}] dP = \\int_A Y dP\\) \\(\\forall A \\in \\mathscr{G}\\)\n\n\nProperties\n\nIf \\(X\\) is \\(\\mathscr{G}\\) measurable, then \\(\\Er[XY| \\mathscr{G}] = X \\Er[Y|\\mathscr{G}]\\) a.e.\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[XY|Z] = X \\Er[Y|Z]\\)\nIf \\(\\sigma(X) \\subset \\sigma(Z)\\), then \\(\\Er[\\Er[Y|Z]|X] = \\Er[Y|X]\\)\nIf \\(Y\\) and \\(X\\) are independent, then \\(\\Er[Y | X ] = \\Er[Y]\\)\n\n\n\n\n\n\n\nTip\n\n\n\nChapter 2, exercise 5.1, 5.2, 5.3"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#observationally-equivalent",
    "href": "problemsets/midterm_review/midterm_review.html#observationally-equivalent",
    "title": "ECON 626: Midterm Review",
    "section": "Observationally Equivalent",
    "text": "Observationally Equivalent\n\nLet \\(\\mathcal{P} = \\{ P(\\cdot; s) : s \\in S \\}\\), two structures \\(s\\) and \\(\\tilde{s}\\) in \\(S\\) are observationally equivalent if they imply the same distribution for the observed data, i.e. \\[ P(B;s) = P(B; \\tilde{s}) \\] for all \\(B \\in \\sigma(X)\\).\nLet \\(\\lambda: S \\to \\R^k\\), \\(\\theta\\) is observationally equivalent to \\(\\tilde{\\theta}\\) if \\(\\exists s, \\tilde{s} \\in S\\) that are observationally equivalent and \\(\\theta = \\lambda(s)\\) and \\(\\tilde{\\theta} = \\lambda(\\tilde{s})\\)\n\nLet \\(\\Gamma(\\theta, S) = \\{P(\\dot; s) | s \\in S, \\theta = \\lambda(s) \\}\\), then \\(\\theta\\) and \\(\\tilde{\\theta}\\) are observationally equivalent iff \\(\\Gamma(\\theta,S) \\cap \\Gamma(\\tilde{\\theta}, S) \\neq \\emptyset\\)"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#non-constructive-identification",
    "href": "problemsets/midterm_review/midterm_review.html#non-constructive-identification",
    "title": "ECON 626: Midterm Review",
    "section": "(Non-Constructive) Identification",
    "text": "(Non-Constructive) Identification\n\n\\(s_0 \\in S\\) is identified if there is no \\(s\\) that is observationally equivalent to \\(s_0\\)\n\\(\\theta_0\\) is identified (in \\(S\\)) if there is no observationally equivalent \\(\\theta \\neq \\theta_0\\)\n\ni.e. \\(\\Gamma(\\theta_0, S) \\cap \\Gamma(\\theta, S) = \\emptyset\\) \\(\\forall \\theta \\neq \\theta_0\\)\n\n\n\n\n\n\n\n\nTip\n\n\n\nChapter 3, exercise 1.1, 1.2"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#sample-analogue-estimation",
    "href": "problemsets/midterm_review/midterm_review.html#sample-analogue-estimation",
    "title": "ECON 626: Midterm Review",
    "section": "Sample Analogue Estimation",
    "text": "Sample Analogue Estimation\n\n\n\n\n\n\nTip\n\n\n\nChapter 3, exercise 1.3, 1.4"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#maximum-likelihood-estimation",
    "href": "problemsets/midterm_review/midterm_review.html#maximum-likelihood-estimation",
    "title": "ECON 626: Midterm Review",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#cramer-rao-lower-bound",
    "href": "problemsets/midterm_review/midterm_review.html#cramer-rao-lower-bound",
    "title": "ECON 626: Midterm Review",
    "section": "Cramer-Rao Lower Bound",
    "text": "Cramer-Rao Lower Bound\n\n\\(X \\in \\R^n\\) distribution \\(P_X \\in \\mathcal{P} = \\{P_\\theta: \\theta \\in \\Theta \\subset \\R^d \\}\\), likelihood \\(\\ell(\\theta;x) = f_X(x;\\theta)\\)"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#score-equality",
    "href": "problemsets/midterm_review/midterm_review.html#score-equality",
    "title": "ECON 626: Midterm Review",
    "section": "Score Equality",
    "text": "Score Equality\n\nIf \\(\\frac{\\partial}{\\partial \\theta} \\int f_X(x;\\theta) d\\mu(x) =  \\int \\frac{\\partial}{\\partial \\theta} f_X(x;\\theta) d\\mu(x)\\), then \\[\n\\int \\underbrace{\\frac{\\partial \\log \\ell(\\theta;x)}{\\partial \\theta}}_{\\text{\"score\"}=s(x,\\theta)} dP_\\theta(x) = 0\n\\]\n\n\nInformation Equality\n\nFischer Information \\(I(\\theta) = \\int s(x,\\theta) s(x,\\theta)' dP_\\theta(x)\\)\nIf \\(\\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} \\int f_X(x;\\theta) d\\mu(x) =  \\int \\frac{\\partial^2}{\\partial \\theta\\partial \\theta'} f_X(x;\\theta)  d\\mu(x)\\), then \\[\nI(\\theta) = -\\int \\underbrace{\\frac{\\partial^2 \\ell(\\theta;x)}{\\partial \\theta \\partial \\theta'}}_{\\text{\"Hessian\"}=h(x,\\theta)} dP_\\theta(x)\n\\]\n\n\n\n2\n\nIf \\(T = \\tau(X)\\) is an unbiased estimator for \\(\\theta\\) and \\[\n\\frac{\\partial}{\\partial \\theta} \\int \\tau(x) f_X(x;\\theta) d\\mu(x) =\n\\int \\tau(x) \\frac{\\partial f_X(x,\\theta)}{\\partial \\theta\\partial \\theta'} d\\mu(x)\n\\] then \\[\n\\int \\tau(x) s(x,\\theta)'dP_\\theta(x) = I\n\\]\n\n\n\nCramér-Rao Bound\nLet \\(T = \\tau(X)\\) be an unbiased estimator, and suppose the condition of the previous slide and of the score equality hold. Then, \\[\n\\var_\\theta(\\tau(X)) \\equiv \\int \\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)\\left(\\tau(x) - \\int \\tau(x) dP_\\theta\\right)' dP\\theta \\geq I(\\theta)^{-1}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nChapter 3, exercise 1.5"
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#hypothesis-testing",
    "href": "problemsets/midterm_review/midterm_review.html#hypothesis-testing",
    "title": "ECON 626: Midterm Review",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_0)\\)=Type I error \\(=P_x(C)\\)\n\\(P(\\text{fail to reject } H_0 | P_x \\in \\mathcal{P}_1)\\)=Type II error\n\\(P(\\text{reject } H_0 | P_x \\in \\mathcal{P}_1)\\) = power\n\\(\\sup_{P_x \\in \\mathcal{P}_0} P_x(C)\\) = size of test\n\n\nNeyman-Pearson Lemma\nLet \\(\\Theta = \\{0, 1\\}\\), \\(f_0\\) and \\(f_1\\) be densities of \\(P_0\\) and \\(P_1\\), $(x) =f_1(x)/f_0(x) $ and \\(C^* =\\{x \\in X: \\tau(x) &gt; c\\}\\). Then among all tests \\(C\\) s.t. \\(P_0(C) = P_0(C^*)\\), \\(C^*\\) is most powerful."
  },
  {
    "objectID": "problemsets/midterm_review/midterm_review.html#gauss-markov-theorem",
    "href": "problemsets/midterm_review/midterm_review.html#gauss-markov-theorem",
    "title": "ECON 626: Midterm Review",
    "section": "Gauss-Markov Theorem",
    "text": "Gauss-Markov Theorem\n\\[\nY = \\theta + u\n\\] with \\(\\theta \\in L \\subset \\R^n\\), \\(L\\) a known subspace. If \\(\\Er[u] = 0\\) and \\(\\Er[uu'] = \\sigma^2 I_n\\), then the best linear unbiased estimator (BLUE) of \\(a'\\theta = a'\\hat{\\theta}\\) where \\(\\hat{\\theta} = P_L y\\)\n\n\n\n\n\n\nTip\n\n\n\nChapter 4, exercise 2.1"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "Asymptotic Theory of Least Squares\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nConvergence in Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nConvergence in Probability\n\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Final - Solutions\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Midterm Review\n\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Midterm Solutions\n\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 1\n\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 2\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 3\n\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 4\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2022\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 5\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 6\n\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nECON 626: Problem Set 7\n\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\nEndogeneity\n\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nEstimation\n\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nGeneralized Method of Moments\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nIdentification\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nInstrumental Variables Estimation\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nLeast Squares as a Projection\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nMeasure\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\n  \n\n\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nPaul Schrimpf\n\n\n\n\n\n\nNo matching items"
  }
]