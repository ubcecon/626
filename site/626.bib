
@misc{song2021,
title={Introduction to Econometrics},
author={Song, Kyunchul},
year={2021}
}


@book{billingsley2013,
  title={Convergence of probability measures},
  author={Billingsley, Patrick},
  year={2013},
  publisher={John Wiley \& Sons}
}

@book{dudley2018,
  title={Real analysis and probability},
  author={Dudley, Richard M},
  year={2018},
  publisher={Chapman and Hall/CRC},
  url={https://doi.org/10.1201/9781351076197}
}

@book{chung2000,
  title={A course in probability theory},
  author={Chung, Kai Lai},
  year={2000},
  publisher={Elsevier}
}

@book{kallenberg1997,
  title={Foundations of modern probability},
  author={Kallenberg, Olav and Kallenberg, Olav},
  volume={2},
  year={1997},
  publisher={Springer},
  url={http://home.ustc.edu.cn/~zyx240014/USTCProbability/files/Foundations%20of%20Modern%20Probability.pdf}
}

@book{durrett2019,
  title={Probability: Theory and Examples},
  author={Durrett, R. and Durrett, R.},
  isbn={9781108473682},
  lccn={2018047195},
  series={Cambridge Series in Statistical and Probabilistic Mathematics},
  url={https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf},
  year={2019},
  publisher={Cambridge University Press}
}

@book{pollard2012,
  title={Convergence of stochastic processes},
  author={Pollard, David},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{andrews1992,
  title={Generic uniform convergence},
  author={Andrews, Donald WK},
  journal={Econometric theory},
  volume={8},
  number={2},
  pages={241--257},
  year={1992},
  publisher={Cambridge University Press}
}

@article{koenker1978,
  title={Regression quantiles},
  author={Koenker, Roger and Bassett Jr, Gilbert},
  journal={Econometrica: journal of the Econometric Society},
  pages={33--50},
  year={1978},
  publisher={JSTOR}
}

@article{pollard1985,
title={New Ways to Prove Central Limit Theorems}, volume={1}, DOI={10.1017/S0266466600011233}, number={3}, journal={Econometric Theory}, author={Pollard, David}, year={1985}, pages={295–313}}



@book{lebanon2012,
  title={Probability: The Analysis of Data, Volume 1},
  author={Lebanon, Guy},
  year={2012},
  publisher={CreateSpace Independent Publishing Platform},
  url={http://theanalysisofdata.com/probability/viewer2.html}
}



@book{royden2010,
  title={Real analysis},
  author={Royden, Halsey and Fitzpatrick, Patrick Michael},
  year={2010},
  publisher={China Machine Press}
}


@book{pollard2002,
  title={A user's guide to measure theoretic probability},
  author={Pollard, David},
  number={8},
  year={2002},
  publisher={Cambridge University Press}
}

@book{tao2011,
  title={An introduction to measure theory},
  author={Tao, Terence},
  volume={126},
  year={2011},
  publisher={American Mathematical Society Providence}
}


@book{vw1996,
  author={Vaart, Aad W and Wellner, Jon A},
  title={Weak convergence and empirical processes},
  year={1996},
  publisher={Springer}
}

@book{casella2024,
  title={Statistical inference},
  author={Casella, George and Berger, Roger},
  year={2024},
  publisher={CRC Press}
}

@book{cinlar2011,
  title={Probability and stochastics},
  author={{\c{C}}inlar, Erhan},
  volume={261},
  year={2011},
  publisher={Springer}
}

@article{angus1994,
author = {Angus, John E.},
title = {The Probability Integral Transform and Related Results},
journal = {SIAM Review},
volume = {36},
number = {4},
pages = {652-654},
year = {1994},
doi = {10.1137/1036146},

URL = {

        https://doi.org/10.1137/1036146



},
eprint = {

        https://doi.org/10.1137/1036146



}
,
    abstract = { A simple proof of the probability integral transform theorem in probability and statistics is given that depends only on probabilistic concepts and elementary properties of continuous functions. This proof yields the theorem in its fullest generality. A similar theorem that forms the basis for the inverse method of random number generation is also discussed and contrasted to the probability integral transform theorem. Typical applications are discussed. Despite their generality and far reaching consequences, these theorems are remarkable in their simplicity and ease of proof. }
}

@book{van2000,
  title={Asymptotic statistics},
  author={Van der Vaart, Aad W},
  volume={3},
  year={2000},
  publisher={Cambridge university press}
}

@book{williams1991,
  title={Probability with martingales},
  author={Williams, David},
  year={1991},
  publisher={Cambridge university press}
}

@article{hall2003,
  title={Generalized method of moments},
  author={Hall, Alastair R},
  journal={A companion to theoretical econometrics},
  pages={230--255},
  year={2003},
  publisher={Wiley Online Library}
}





@incollection{hsiao1983,
title = {Chapter 4 Identification},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {1},
pages = {223-283},
year = {1983},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(83)01008-9},
url = {https://www.sciencedirect.com/science/article/pii/S1573441283010089},
author = {Cheng Hsiao},
abstract = {Publisher Summary
The study of identification has been clearly linked to the design of experiments. In a well-designed experiment the treatment group and the control group are similar in every aspect, except for the treatment. The difference in response may therefore be attributed to the treatment and the parameters of interest are identified. An extensive study of the identifiability conditions for the simultaneous equations models under various assumptions about the underlying structures was provided by Fisher. This chapter discusses the development of the subject since the publication of Fisher's book. It discusses the basic concepts of identification and some identifiability criteria for contemporaneous–simultaneous equation models under linear constraints. The chapter discusses criteria for models subject to nonlinear continuous differentiable constraints and covariance restrictions with special emphasis on the applications to errors in variables and variance components models. The Bayesian view on identification is also discussed in the chapter.}
}

@incollection{matzkin2007,
title = {Chapter 73 Nonparametric identification},
editor = {James J. Heckman and Edward E. Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {6},
pages = {5307-5368},
year = {2007},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(07)06073-4},
url = {https://www.sciencedirect.com/science/article/pii/S1573441207060734},
author = {Matzkin, Rosa L},
keywords = {identification, nonparametric models, simultaneous equations, nonadditive models, nonseparable models},
abstract = {When one wants to estimate a model without specifying the functions and distributions parametrically, or when one wants to analyze the identification of a model independently of any particular parametric specification, it is useful to perform a nonparametric analysis of identification. This chapter presents some of the recent results on the identification of nonparametric econometric models. It considers identification in models that are additive in unobservable random terms and in models that are nonadditive in unobservable random terms. Single equation models as well as models with a system of equations are studied. Among the latter, special attention is given to structural models whose reduced forms are triangular in the unobservable random terms, and to simultaneous equations, where the reduced forms are functions of all the unobservable variables in the system. The chapter first presents some general identification results for single-equation models that are additive in unobservable random terms, single-equation models that are nonadditive in unobservable random terms, single-equation models that possess and index structure, simultaneous equations nonadditive in unobservable random terms, and discrete choice models. Then, particular ways of achieving identification are considered. These include making use of conditional independence restrictions, marginal independence restrictions, shape restrictions on functions, shape restrictions on distributions, and restrictions in both functions and distributions. The objective is to provide insight into some of the recent techniques that have been developed recently, rather than on presenting a complete survey of the literature.}
}

@article{fox2012,
  title={The random coefficients logit model is identified},
  author={Fox, Jeremy T and il Kim, Kyoo and Ryan, Stephen P and Bajari, Patrick},
  journal={Journal of Econometrics},
  volume={166},
  number={2},
  pages={204--212},
  year={2012},
  publisher={Elsevier}
}

@article{christensen2015,
  title={Nonparametric identification of positive eigenfunctions},
  author={Christensen, Timothy M},
  journal={Econometric Theory},
  volume={31},
  number={6},
  pages={1310--1330},
  year={2015},
  publisher={Cambridge University Press}
}


@article{lewbel2019,
Author = {Lewbel, Arthur},
Title = {The Identification Zoo: Meanings of Identification in Econometrics},
Journal = {Journal of Economic Literature},
Volume = {57},
Number = {4},
Year = {2019},
Month = {December},
Pages = {835-903},
DOI = {10.1257/jel.20181361},
URL = {https://www.aeaweb.org/articles?id=10.1257/jel.20181361}}

@incollection{molinari2020,
title = {Chapter 5 - Microeconometrics with partial identification},
editor = {Steven N. Durlauf and Lars Peter Hansen and James J. Heckman and Rosa L. Matzkin},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {7},
pages = {355-486},
year = {2020},
booktitle = {Handbook of Econometrics, Volume 7A},
issn = {1573-4412},
doi = {https://doi.org/10.1016/bs.hoe.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1573441220300027},
author = {Francesca Molinari},
keywords = {Partial identification, Random sets, Incomplete data and models, Discrete choice models, Auction models, Moment inequalities, Support function approach, Criterion function approach, Model misspecification, Computational methods},
abstract = {This chapter reviews the microeconometrics literature on partial identification, focusing on the developments of the last thirty years. The topics presented illustrate that the available data combined with credible maintained assumptions may yield much information about a parameter of interest, even if they do not reveal it exactly. Special attention is devoted to discussing the challenges associated with, and some of the solutions put forward to, (1) obtain a tractable characterization of the values for the parameters of interest which are observationally equivalent, given the available data and maintained assumptions; (2) estimate this set of values; (3) conduct test of hypotheses and make confidence statements. The chapter reviews advances in partial identification analysis both as applied to learning (functionals of) probability distributions that are well-defined in the absence of models, as well as to learning parameters that are well-defined only in the context of particular models. A simple organizing principle is highlighted: the source of the identification problem can often be traced to a collection of random variables that are consistent with the available data and maintained assumptions. This collection may be part of the observed data or be a model implication. In either case, it can be formalized as a random set. Random set theory is then used as a mathematical framework to unify a number of special results and produce a general methodology to carry out partial identification analysis.}
}

@article{matzkin2013,
  title={Nonparametric identification in structural economic models},
  author={Matzkin, Rosa L},
  journal={Annu. Rev. Econ.},
  volume={5},
  number={1},
  pages={457--486},
  year={2013},
  publisher={Annual Reviews}
}

@article{pearl2015,
title={TRYGVE HAAVELMO AND THE EMERGENCE OF CAUSAL CALCULUS}, volume={31},
                  DOI={10.1017/S0266466614000231}, number={1},
                  journal={Econometric Theory}, publisher={Cambridge
                  University Press}, author={Pearl, Judea},
                  year={2015}, pages={152–179}}

@book{pearl2018,
  title={The book of why: the new science of cause and effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic books}
}

@article{imbens2020,
Author = {Imbens, Guido W.},
Title = {Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics},
Journal = {Journal of Economic Literature},
Volume = {58},
Number = {4},
Year = {2020},
Month = {December},
Pages = {1129-79},
DOI = {10.1257/jel.20191597},
URL = {https://www.aeaweb.org/articles?id=10.1257/jel.20191597}}

@article{cochran1953,
  title={Matching in analytical studies},
  author={Cochran, William G},
  journal={American Journal of Public Health and the Nations Health},
  volume={43},
  number={6\_Pt\_1},
  pages={684--691},
  year={1953},
  publisher={American Public Health Association}
}

@article{chernozhukov2021,
  title={Causal impact of masks, policies, behavior on early covid-19 pandemic in the US},
  author={Chernozhukov, Victor and Kasahara, Hiroyuki and Schrimpf, Paul},
  journal={Journal of econometrics},
  volume={220},
  number={1},
  pages={23--62},
  year={2021},
  publisher={Elsevier}
}

@article{wright1934,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2957502},
 author = {Sewall Wright},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {161--215},
 publisher = {Institute of Mathematical Statistics},
 title = {The Method of Path Coefficients},
 urldate = {2022-09-21},
 volume = {5},
 year = {1934}
}

@article{heckman2004,
    author = {Heckman, James and Navarro-Lozano, Salvador},
    title = "{Using Matching, Instrumental Variables, and Control Functions to Estimate Economic Choice Models}",
    journal = {The Review of Economics and Statistics},
    volume = {86},
    number = {1},
    pages = {30-57},
    year = {2004},
    month = {02},
    abstract = "{This paper investigates four topics. (1) It examines the different roles played by the propensity score (the probability of selection into treatment) in matching, instrumental variable, and control function methods. (2) It contrasts the roles of exclusion restrictions in matching and selection models. (3) It characterizes the sensitivity of matching to the choice of conditioning variables and demonstrates the greater robustness of control function methods to misspecification of the conditioning variables. (4) It demonstrates the problem of choosing the conditioning variables in matching and the failure of conventional model selection criteria when candidate conditioning variables are not exogenous in a sense defined in this paper.}",
    issn = {0034-6535},
    doi = {10.1162/003465304323023660},
    url = {https://doi.org/10.1162/003465304323023660},
    eprint = {https://direct.mit.edu/rest/article-pdf/86/1/30/1613851/003465304323023660.pdf},
}

@book{lehmann2006,
  title={Theory of point estimation},
  author={Lehmann, Erich L and Casella, George},
  year={2006},
  publisher={Springer Science \& Business Media},
  url={https://link.springer.com/book/10.1007/b98854}
}

@book{lehmann2005,
  title={Testing statistical hypotheses},
  author={Lehmann, Erich Leo and Romano, Joseph P},
  volume={3},
  publisher={Springer},
  url={https://link.springer.com/book/10.1007/0-387-27605-X}
}

@misc{schrimpf2018,
title={Linearity},
author={Schrimpf, Paul},
year={2018},
url={http://faculty.arts.ubc.ca/pschrimpf/526/linear-526.pdf}
}

@misc{schrimpf2013a,
title={Systems of Linear Equations},
author={Schrimpf, Paul},
year={2013},
url={http://faculty.arts.ubc.ca/pschrimpf/526/lec02systemsOfLinearEquations.pdf}
}

@misc{schrimpf2013b,
title={Matrix Algebra and Introduction to Vector Spaces},
author={Schrimpf, Paul},
year={2013},
url={http://faculty.arts.ubc.ca/pschrimpf/526/lec03matrixAlgebra.pdf}
}

@misc{schrimpf2013c,
title={Vector Spaces},
author={Schrimpf, Paul},
year={2013},
url={http://faculty.arts.ubc.ca/pschrimpf/526/lec04vectorspaces.pdf}
}

@misc{schrimpf2013o,
title={Unconstrained Optimization (especially section 4 on Eigendecomposition)},
author={Schrimpf, Paul},
year={2013},
url={http://faculty.arts.ubc.ca/pschrimpf/526/lec10optimization.pdf}
}

@article{dobler2022,
title = {A short proof of Lévy’s continuity theorem without using tightness},
journal = {Statistics & Probability Letters},
volume = {185},
pages = {109438},
year = {2022},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2022.109438},
url = {https://www.sciencedirect.com/science/article/pii/S0167715222000438},
author = {Christian Döbler},
keywords = {Lévy’s continuity theorem, Characteristic functions, Prohorov’s theorem, Tightness, Uniqueness theorem},
abstract = {In this note we present a new short and direct proof of Lévy’s continuity theorem in arbitrary dimension d, which does not rely on Prohorov’s theorem, Helly’s selection theorem or the uniqueness theorem for characteristic functions. Instead, it is based on convolution with a small (scalar) Gaussian distribution as well as on basic facts about weak convergence and measure theory. Moreover, we show how, by similar means, one may prove the fact that a distribution with integrable characteristic function is absolutely continuous with respect to d-dimensional Lebesgue measure and derive the formula for its density.}
}

@misc{mikusheva2007,
  title={14.384 Time Series Analysis, Fall 2007 (revised 2009)},
  author={Mikusheva, Anna and Schrimpf, Paul},
  url={http://ocw.mit.edu/courses/economics/14-384-time-series-analysis-fall-2013/lecture-notes/},
  year={2007}
}

@incollection{dedecker2007,
  title={Weak dependence},
    author={Dedecker, J{\'e}r{\^o}me and Doukhan, Paul and Lang, Gabriel and Jos{\'e} Rafael, Le{\'o}n R and Louhichi, Sana and Prieur, Cl{\'e}mentine},
      booktitle={Weak dependence: With examples and applications},
        pages={9--20},
          year={2007},
            publisher={Springer},
            url={https://link.springer.com/book/10.1007/978-0-387-69952-3}
            }


@article{newey1987,
  title={A Simple, Positive Semi-definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix},
    author={Newey, Whitney K and West, Kenneth D},
      journal={Econometrica},
        volume={55},
          number={3},
            pages={703--708},
              year={1987},
                publisher={Econometric Society}
                }


@article{chao1972,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2284399},
 abstract = {We investigate the problem of finding the expected value of functions of a random variable X of the form f(X) = (X + A)-n where $X + A > 0$ a.s. and n is a non-negative integer. The technique is to successively integrate the probability generating function and is suggested by the well-known result that successive differentiation leads to the positive moments. The technique is applied to the problem of finding E[1/(X + A)] for the binomial and Poisson distributions.},
 author = {M. T. Chao and W. E. Strawderman},
 journal = {Journal of the American Statistical Association},
 number = {338},
 pages = {429--431},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Negative Moments of Positive Random Variables},
 urldate = {2022-11-14},
 volume = {67},
 year = {1972}
}

@article{andrews2019,
author = {Andrews, Isaiah and Stock, James H. and Sun, Liyang},
title = {Weak Instruments in Instrumental Variables Regression: Theory and Practice},
journal = {Annual Review of Economics},
volume = {11},
number = {1},
pages = {727-753},
year = {2019},
doi = {10.1146/annurev-economics-080218-025643},
URL = {
        https://doi.org/10.1146/annurev-economics-080218-025643
},
eprint = {
        https://doi.org/10.1146/annurev-economics-080218-025643
},
    abstract = { When instruments are weakly correlated with endogenous regressors, conventional methods for instrumental variables (IV) estimation and inference become unreliable. A large literature in econometrics has developed procedures for detecting weak instruments and constructing robust confidence sets, but many of the results in this literature are limited to settings with independent and homoskedastic data, while data encountered in practice frequently violate these assumptions. We review the literature on weak instruments in linear IV regression with an emphasis on results for nonhomoskedastic (heteroskedastic, serially correlated, or clustered) data. To assess the practical importance of weak instruments, we also report tabulations and simulations based on a survey of papers published in the American Economic Review from 2014 to 2018 that use IV. These results suggest that weak instruments remain an important issue for empirical practice, and that there are simple steps that researchers can take to better handle weak instruments in applications. }
}


@article{aizer2018,
Author = {Aizer, Anna and Currie, Janet and Simon, Peter and Vivier, Patrick},
Title = {Do Low Levels of Blood Lead Reduce Children's Future Test Scores?},
Journal = {American Economic Journal: Applied Economics},
Volume = {10},
Number = {1},
Year = {2018},
Month = {January},
Pages = {307-41},
DOI = {10.1257/app.20160404},
URL = {http://www.aeaweb.org/articles?id=10.1257/app.20160404}}

@article{swy2002,
author = {James H Stock and Jonathan H Wright and Motohiro Yogo},
title = {A Survey of Weak Instruments and Weak Identification in Generalized Method of Moments},
journal = {Journal of Business \& Economic Statistics},
volume = {20},
number = {4},
pages = {518-529},
year  = {2002},
publisher = {Taylor & Francis},
doi = {10.1198/073500102288618658},

URL = {
        https://doi.org/10.1198/073500102288618658

},
eprint = {
        https://doi.org/10.1198/073500102288618658

}

}

@techreport{stockyogo2002,
 title = "Testing for Weak Instruments in Linear IV Regression",
 author = "Stock, James H and Yogo, Motohiro",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Technical Working Paper Series",
 number = "284",
 year = "2002",
 month = "November",
 doi = {10.3386/t0284},
 URL = "http://www.nber.org/papers/t0284",
 abstract = {Weak instruments can produce biased IV estimators and hypothesis tests with large size distortions. But what, precisely, are weak instruments, and how does one detect them in practice? This paper proposes quantitative definitions of weak instruments based on the maximum IV estimator bias, or the maximum Wald test size distortion, when there are multiple endogenous regressors. We tabulate critical values that enable using the first-stage F-statistic (or, when there are multiple endogenous regressors, the Cragg-Donald (1993) statistic) to test whether given instruments are weak. A technical contribution is to justify sequential asymptotic approximations for IV statistics with many weak instruments.},
}

@article{lee2022,
Author = {Lee, David S and McCrary, Justin and Moreira, Marcelo J. and Porter, Jack},
Title = {Valid t-Ratio Inference for IV},
Journal = {American Economic Review},
Volume = {112},
Number = {10},
Year = {2022},
Month = {October},
Pages = {3260-90},
DOI = {10.1257/aer.20211063},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20211063}}

@article{keane2021,
  title={A practical guide to weak instruments},
  author={Keane, Michael P and Neal, Timothy},
  year={2021},
  publisher={UNSW Economics Working Paper No. 2021-05c}
}

@article{hansen1982,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1912775},
 abstract = {This paper studies estimators that make sample analogues of population orthogonality conditions close to zero. Strong consistency and asymptotic normality of such estimators is established under the assumption that the observable variables are stationary and ergodic. Since many linear and nonlinear econometric estimators reside within the class of estimators studied in this paper, a convenient summary of the large sample properties of these estimators, including some whose large sample properties have not heretofore been discussed, is provided.},
 author = {Lars Peter Hansen},
 journal = {Econometrica},
 number = {4},
 pages = {1029--1054},
 publisher = {[Wiley, Econometric Society]},
 title = {Large Sample Properties of Generalized Method of Moments Estimators},
 urldate = {2022-11-29},
 volume = {50},
 year = {1982}
}

@article{hs1982,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1911873},
 abstract = {This paper describes a method for estimating and testing nonlinear rational expectations models directly from stochastic Euler equations. The estimation procedure makes sample counterparts to the population orthogonality conditions implied by the economic model close to zero. An attractive feature of this method is that the parameters of the dynamic objective functions of economic agents can be estimated without explicitly solving for the stochastic equilibrium.},
 author = {Lars Peter Hansen and Kenneth J. Singleton},
 journal = {Econometrica},
 number = {5},
 pages = {1269--1286},
 publisher = {[Wiley, Econometric Society]},
 title = {Generalized Instrumental Variables Estimation of Nonlinear Rational Expectations Models},
 urldate = {2022-11-29},
 volume = {50},
 year = {1982}
}

@Inbook{hansen2010,
author="Hansen, Lars Peter",
editor="Durlauf, Steven N.
and Blume, Lawrence E.",
title="Generalized method of moments estimation",
bookTitle="Macroeconometrics and Time Series Analysis",
year="2010",
publisher="Palgrave Macmillan UK",
address="London",
pages="105--118",
abstract="Generalized method of moments (GMM) refers to a class of estimators constructed from the sample moment counterparts of population moment conditions (sometimes known as orthogonality conditions) of the data generating model. GMM estimators have become widely used, for the following reasons:1.GMM estimators have large sample properties that are easy to characterize. A family of such estimators can be studied simultaneously in ways that make asymptotic efficiency comparisons easy. The method also provides a natural way to construct tests which take account of both sampling and estimation error.2.In practice, researchers find it useful that GMM estimators may be constructed without specifying the full data generating process (which would be required to write down the maximum likelihood estimator). This characteristic has been exploited in analysing partially specified economic models, studying potentially misspecified dynamic models designed to match target moments, and constructing stochastic discount factor models that link asset pricing to sources of macroeconomic risk.",
isbn="978-0-230-28083-0",
doi="10.1057/9780230280830_13",
url="https://doi.org/10.1057/9780230280830_13"
}

@article{anderson2019,
    author = {Anderson, Michael L},
    title = "As the Wind Blows: The Effects of Long-Term Exposure to Air Pollution on Mortality",
    journal = {Journal of the European Economic Association},
    volume = {18},
    number = {4},
    pages = {1886-1927},
    year = {2019},
    month = {10},
    abstract = "{There is strong evidence that short-run fluctuations in air pollution negatively impact infant health and contemporaneous adult health, but there is less evidence on the causal link between long-term exposure to air pollution and increased adult mortality. This project estimates the impact of long-term exposure to air pollution on mortality by leveraging quasi-random variation in pollution levels generated by wind patterns near major highways. I combine geocoded data on the residence of every decedent in Los Angeles over three years, high-frequency wind data, and Census short form data. Using these data, I estimate the effect of downwind exposure to highway-generated pollutants on the age-specific mortality rate by using orientation to the nearest major highway as an instrument for pollution exposure. I find that doubling the percentage of time spent downwind of a highway increases mortality among individuals 75 or older by 3.8\\%–6.5\\%. These estimates are robust and imply significant loss of life years.}",
    issn = {1542-4766},
    doi = {10.1093/jeea/jvz051},
    url = {https://doi.org/10.1093/jeea/jvz051},
    eprint = {https://academic.oup.com/jeea/article-pdf/18/4/1886/33680131/jvz051.pdf},
}

@misc{borusyak2018,
  title={Revisiting event study designs},
  author={Borusyak, Kirill and Jaravel, Xavier},
  year={2018},
  url={https://scholar.harvard.edu/files/borusyak/files/borusyak_jaravel_event_studies.pdf}
}

@article{goodmanbacon2021,
title = {Difference-in-differences with variation in treatment timing},
journal = {Journal of Econometrics},
volume = {225},
number = {2},
pages = {254-277},
year = {2021},
note = {Themed Issue: Treatment Effect 1},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2021.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0304407621001445},
author = {Andrew Goodman-Bacon},
keywords = {Difference-in-differences, Variation in treatment timing, Two-way fixed effects, Treatment effect heterogeneity},
abstract = {The canonical difference-in-differences (DD) estimator contains two time periods, ”pre” and ”post”, and two groups, ”treatment” and ”control”. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.}
}

@article{sun2021,
title = {Estimating dynamic treatment effects in event studies with heterogeneous treatment effects},
journal = {Journal of Econometrics},
volume = {225},
number = {2},
pages = {175-199},
year = {2021},
note = {Themed Issue: Treatment Effect 1},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S030440762030378X},
author = {Liyang Sun and Sarah Abraham},
keywords = {Difference-in-differences, Two-way fixed effects, Pretrend test},
abstract = {To estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.}
}

@article{callaway2021,
title = {Difference-in-Differences with multiple time periods},
journal = {Journal of Econometrics},
volume = {225},
number = {2},
pages = {200-230},
year = {2021},
note = {Themed Issue: Treatment Effect 1},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2020.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304407620303948},
author = {Brantly Callaway and Pedro H.C. Sant’Anna},
keywords = {Difference-in-Differences, Dynamic treatment effects, Doubly robust, Event study, Variation in treatment timing, Treatment effect heterogeneity, Semi-parametric},
abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.}
}

@article{laporte2005,
title = {Estimation of panel data models with binary indicators when treatment effects are not constant over time},
journal = {Economics Letters},
volume = {88},
number = {3},
pages = {389-396},
year = {2005},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2005.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S016517650500145X},
author = {Audrey Laporte and Frank Windmeijer},
keywords = {Panel data, Treatment effects},
abstract = {We show that two commonly employed estimation procedures to deal with correlated unobserved heterogeneity in panel data models, within-groups and first-differenced OLS, can lead to very different estimates of treatment effects when these are not constant over time and treatment is a state that only changes occasionally. It is therefore important to allow for flexible time varying treatment effects when estimating panel data models with binary indicator variables as is illustrated by an example of the effects of marital status on mental wellbeing.}
}

@article{dd2020,
Author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
Title = {Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects},
Journal = {American Economic Review},
Volume = {110},
Number = {9},
Year = {2020},
Month = {September},
Pages = {2964-96},
DOI = {10.1257/aer.20181169},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.20181169}}


@article{wooldridge2005,
    author = {Wooldridge, Jeffrey M.},
    title = "{Fixed-Effects and Related Estimators for Correlated Random-Coefficient and Treatment-Effect Panel Data Models}",
    journal = {The Review of Economics and Statistics},
    volume = {87},
    number = {2},
    pages = {385-390},
    year = {2005},
    month = {05},
    abstract = "{I derive conditions under which a class of fixed-effects estimators consistently estimates the population-averaged slope coefficients in panel data models with individual-specific slopes, where the slopes are allowed to be correlated with the covariates. In addition to including the usual fixed-effects estimator, the results apply to estimators that eliminate individual-specific trends. I apply the results, and propose alternative estimators, to estimation of average treatment in a class of nonlinear unobserved-effects models.}",
    issn = {0034-6535},
    doi = {10.1162/0034653053970320},
    url = {https://doi.org/10.1162/0034653053970320},
    eprint = {https://direct.mit.edu/rest/article-pdf/87/2/385/1614013/0034653053970320.pdf},
}

@article{dd2022,
    author = {de Chaisemartin, Clément and D’Haultfœuille, Xavier},
    title = "{Two-way fixed effects and differences-in-differences with heterogeneous treatment effects: a survey}",
    journal = {The Econometrics Journal},
    volume = {26},
    number = {3},
    pages = {C1-C30},
    year = {2022},
    month = {06},
    abstract = "{Linear regressions with period and group fixed effects are widely used to estimate policie’s effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been shown that those regressions may produce misleading estimates if the policy’s effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue and that proposes alternative estimators robust to heterogeneous effects. We use those alternative estimators to revisit Wolfers (2006a).}",
    issn = {1368-4221},
    doi = {10.1093/ectj/utac017},
    url = {https://doi.org/10.1093/ectj/utac017},
    eprint = {https://academic.oup.com/ectj/article-pdf/26/3/C1/51707976/utac017.pdf},
}

@article{roth2023,
title = {What’s trending in difference-in-differences? A synthesis of the recent econometrics literature},
journal = {Journal of Econometrics},
volume = {235},
number = {2},
pages = {2218-2244},
year = {2023},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2023.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0304407623001318},
author = {Jonathan Roth and Pedro H.C. Sant’Anna and Alyssa Bilinski and John Poe},
keywords = {Difference-in-differences, Causal Inference, Staggered Treatment timing, Sensitivity Analysis, Clustering, Parallel trends, Treatment Effect Heterogeneity},
abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of “canonical” assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.}
}



@article{roth2022,
Author = {Roth, Jonathan},
Title = {Pretest with Caution: Event-Study Estimates after Testing for Parallel Trends},
Journal = {American Economic Review: Insights},
Volume = {4},
Number = {3},
Year = {2022},
Month = {September},
Pages = {305-22},
DOI = {10.1257/aeri.20210236},
URL = {https://www.aeaweb.org/articles?id=10.1257/aeri.20210236}}

@article{rambachan2023,
    author = {Rambachan, Ashesh and Roth, Jonathan},
    title = "{A More Credible Approach to Parallel Trends}",
    journal = {The Review of Economic Studies},
    volume = {90},
    number = {5},
    pages = {2555-2591},
    year = {2023},
    month = {02},
    abstract = "{This paper proposes tools for robust inference in difference-in-differences and event-study designs where the parallel trends assumption may be violated. Instead of requiring that parallel trends holds exactly, we impose restrictions on how different the post-treatment violations of parallel trends can be from the pre-treatment differences in trends (“pre-trends”). The causal parameter of interest is partially identified under these restrictions. We introduce two approaches that guarantee uniformly valid inference under the imposed restrictions, and we derive novel results showing that they have desirable power properties in our context. We illustrate how economic knowledge can inform the restrictions on the possible violations of parallel trends in two economic applications. We also highlight how our approach can be used to conduct sensitivity analyses showing what causal conclusions can be drawn under various restrictions on the possible violations of the parallel trends assumption.}",
    issn = {0034-6527},
    doi = {10.1093/restud/rdad018},
    url = {https://doi.org/10.1093/restud/rdad018},
    eprint = {https://academic.oup.com/restud/article-pdf/90/5/2555/51356029/rdad018.pdf},
}

@article{chen2023,
author = {Chen, Weiwei and French, Michael T.},
title = {Marijuana legalization and traffic fatalities revisited},
journal = {Southern Economic Journal},
volume = {90},
number = {2},
pages = {259-276},
keywords = {marijuana legalization, medical marijuana, recreational marijuana, traffic fatalities},
doi = {https://doi.org/10.1002/soej.12657},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/soej.12657},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/soej.12657},
abstract = {Abstract The legal landscape for marijuana in the United States has changed dramatically over the last three decades. While several studies have examined the relationship between marijuana legalization and traffic fatalities, some of the research is becoming outdated and existing evidence remains mixed. Our research revisits the topic with two updates. First, our study includes states that legalized marijuana more recently and provides updated evidence on the effects of marijuana legalization. Second, considering recent discussions about the limitations of difference-in-differences designs, we employ alternative estimators that are robust to heterogeneous and dynamic treatment effects. Overall, our alternative estimators suggest either a smaller reduction (i.e., 3.9\% drop in the overall fatality rate) or no change in traffic fatalities associated with legalizing marijuana for medical use, compared to the two-way fixed-effect estimator. We find no significant impact on traffic fatalities associated with legalizing marijuana for recreational use.},
year = {2023}
}


@techreport{lee2023,
 title = "What to do when you can't use '1.96' Confidence Intervals for IV",
 author = "Lee, David S and McCrary, Justin and Moreira, Marcelo J and Porter, Jack R and Yap, Luther",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "31893",
 year = "2023",
 month = "November",
 doi = {10.3386/w31893},
 URL = "http://www.nber.org/papers/w31893",
 abstract = {To address the well-established large-sample invalidity of the +/-1.96 critical values for the t-ratio in the single variable just-identified IV model, applied research typically qualifies the inference based on the first-stage-F (Staiger and Stock (1997) and Stock and Yogo (2005)). We fully extend this F-based approach to its logical conclusion by presenting new critical values for the t-ratio to additionally accommodate values of F that do not meet existing thresholds needed for validity. These new t-ratio critical values simultaneously fix the main problem of over-rejection (invalidity) and the under-appreciated possibility of under-rejection (conservativeness) that can occur when relying solely on the usual 1.96 critical value. We show that the corresponding new confidence intervals are generally expected to be substantially shorter than competing “robust to weak instrument” intervals, including those from the recommended benchmark of Anderson and Rubin (1949) (AR). In a sample of 89 specifications from 10 recent empirical studies drawn from five general interest journals, the new “VtF” intervals are shorter than AR intervals 100 percent of the time, and even more likely to produce statistically significant results than the usual +/-1.96 procedure.},
}


@article{ass2019,
author = {Andrews, Isaiah and Stock, James H. and Sun, Liyang},
title = {Weak Instruments in Instrumental Variables Regression: Theory and Practice},
journal = {Annual Review of Economics},
volume = {11},
number = {1},
pages = {727-753},
year = {2019},
doi = {10.1146/annurev-economics-080218-025643},

URL = {

        https://doi.org/10.1146/annurev-economics-080218-025643



},
eprint = {

        https://doi.org/10.1146/annurev-economics-080218-025643



}
,
    abstract = { When instruments are weakly correlated with endogenous regressors, conventional methods for instrumental variables (IV) estimation and inference become unreliable. A large literature in econometrics has developed procedures for detecting weak instruments and constructing robust confidence sets, but many of the results in this literature are limited to settings with independent and homoskedastic data, while data encountered in practice frequently violate these assumptions. We review the literature on weak instruments in linear IV regression with an emphasis on results for nonhomoskedastic (heteroskedastic, serially correlated, or clustered) data. To assess the practical importance of weak instruments, we also report tabulations and simulations based on a survey of papers published in the American Economic Review from 2014 to 2018 that use IV. These results suggest that weak instruments remain an important issue for empirical practice, and that there are simple steps that researchers can take to better handle weak instruments in applications. }
}

@article{keane2023,
title = {Instrument strength in IV estimation and inference: A guide to theory and practice},
journal = {Journal of Econometrics},
volume = {235},
number = {2},
pages = {1625-1653},
year = {2023},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2022.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0304407623000222},
author = {Michael Keane and Timothy Neal},
keywords = {Instrumental variables, Weak instruments, 2SLS, Endogeneity, F-test, Size distortion, Anderson–Rubin test, Likelihood ratio test, LIML, GMM, Fuller, JIVE},
abstract = {Two stage least squares (2SLS) has poor properties if instruments are exogenous but weak. But how strong do instruments need to be for 2SLS estimates and test statistics to exhibit acceptable properties? A common standard is that first-stage F≥10. This is adequate to ensure two-tailed t-tests have modest size distortions. But other problems persist: In particular, we show 2SLS standard errors are artificially small in samples where the estimate is most contaminated by the OLS bias. Hence, if the bias is positive, the t-test has little power to detect true negative effects, and inflated power to find positive effects. This phenomenon, which we call a “power asymmetry,” persists even if first-stage F is in the thousands. Robust tests like Anderson–Rubin perform better, and should be used in lieu of the t-test even with strong instruments. We also show how 2SLS test statistics typically suffer from very low power if first-stage F is only 10, leading us to suggest a higher standard of instrument strength in empirical practice.}
}

@misc{arkhangelsky2023,
Author = {Dmitry Arkhangelsky and Guido Imbens},
Title = {Causal Models for Longitudinal and Panel Data: A Survey},
Year = {2023},
Eprint = {arXiv:2311.15458},
}

@article{efs2013,
 ISSN = {00028282},
 URL = {http://www.jstor.org/stable/23469640},
 abstract = {We use employee-level panel data from a single firm to explore the possibility that individuals may select insurance coverage in part based on their anticipated behavioral ("moral hazard") response to insurance, a phenomenon we label "selection on moral hazard." Using a model of plan choice and medical utilization, we present evidence of heterogenous moral hazard as well as selection on it, and explore some of its implications. For example, we show that, at least in our context, abstracting from selection on moral hazard could lead to overestimates of the spending reduction associated with introducing a high-deductible health insurance option.},
 author = {Liran Einav and Amy Finkelstein and Stephen P. Ryan and Paul Schrimpf and Mark R. Cullen},
 journal = {The American Economic Review},
 number = {1},
 pages = {178--219},
 publisher = {American Economic Association},
 title = {Selection on Moral Hazard in Health Insurance},
 urldate = {2023-12-11},
 volume = {103},
 year = {2013}
}

@article{macchiavello2020,
    author = {Macchiavello, Rocco and Morjaria, Ameet},
    title =  {Competition and Relational Contracts in the Rwanda Coffee Chain},
    journal = {The Quarterly Journal of Economics},
    volume = {136},
    number = {2},
    pages = {1089-1143},
    year = {2020},
    month = {12},
    abstract = "{How does competition affect market outcomes when formal contracts are not enforceable and parties resort to relational contracts? Difficulties with measuring relational contracts and dealing with the endogeneity of competition have frustrated attempts to answer this question. We make progress by studying relational contracts between upstream farmers and downstream mills in Rwanda’s coffee industry. First, we identify salient dimensions of their relational contracts and measure them through an original survey of mills and farmers. Second, we take advantage of an engineering model for the optimal placement of mills to construct an instrument that isolates geographically determined variation in competition. Conditional on the suitability for mills’ placement in the catchment area, we find that mills surrounded by more suitable areas (i) face more competition from other mills, (ii) use fewer relational contracts with farmers, and (iii) exhibit worse performance. An additional competing mill also (iv) reduces the aggregate quantity of coffee supplied to mills by farmers and (v) makes farmers worse off. Competition hampers relational contracts directly by increasing farmers’ temptation to default on the relational contract and indirectly by reducing mills’ profits.}",
    issn = {0033-5533},
    doi = {10.1093/qje/qjaa048},
    url = {https://doi.org/10.1093/qje/qjaa048},
    eprint = {https://academic.oup.com/qje/article-pdf/136/2/1089/42901754/qjaa048.pdf},
}

@book{Buhlmann/vandeGeer:11:Springer,
  title={Statistics for High-Dimensional Data: Methods, Theory and Applications},
  author={B{\"u}hlmann, P. and van de Geer, S.},
  isbn={9783642201929},
  lccn={2011930793},
  series={Springer Series in Statistics},
  url={https://books.google.ca/books?id=S6jYXmh988UC},
  year={2011},
  publisher={Springer Berlin Heidelberg}
}


@misc{Rigollet:15:HighDim,
Author = {Philippe Rigollet and Jan-Christian Hütter},
Title = {High-Dimensional Statistics},
Year = {2023},
Eprint = {arXiv:2310.19244},
}

@incollection{newey1994,
title = {Chapter 36 Large sample estimation and hypothesis testing},
series = {Handbook of Econometrics},
booktitle={Handbook of Econometrics},
publisher = {Elsevier},
volume = {4},
pages = {2111-2245},
year = {1994},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(05)80005-4},
url = {https://www.sciencedirect.com/science/article/pii/S1573441205800054},
author = {Whitney K. Newey and Daniel McFadden},
abstract = {Asymptotic distribution theory is the primary method used to examine the properties of econometric estimators and tests. We present conditions for obtaining cosistency and asymptotic normality of a very general class of estimators (extremum estimators). Consistent asymptotic variance estimators are given to enable approximation of the asymptotic distribution. Asymptotic efficiency is another desirable property then considered. Throughout the chapter, the general results are also specialized to common econometric estimators (e.g. MLE and GMM), and in specific examples we work through the conditions for the various results in detail. The results are also extended to two-step estimators (with finite-dimensional parameter estimation in the first step), estimators derived from nonsmooth objective functions, and semiparametric two-step estimators (with nonparametric estimation of an infinite-dimensional parameter in the first step). Finally, the trinity of test statistics is considered within the quite general setting of GMM estimation, and numerous examples are given.}
}

@article{pakes1989,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1913622},
 abstract = {A general central limit theorem is proved for estimators defined by minimization of the length of a vector-valued, random criterion function. No smoothness assumptions are imposed on the criterion function, in order that the results might apply to a broad class of simulation estimators. Complete analyses of two simulation estimators, one introduced by Pakes and the other by McFadden, illustrate the application of the general theorems.},
 author = {Ariel Pakes and David Pollard},
 journal = {Econometrica},
 number = {5},
 pages = {1027--1057},
 publisher = {[Wiley, Econometric Society]},
 title = {Simulation and the Asymptotics of Optimization Estimators},
 urldate = {2024-08-28},
 volume = {57},
 year = {1989}
}

@article{buchinsky1998,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/2998578},
 abstract = {The paper introduces an alternative estimator for the linear censored quantile regression model. The objective function is globally convex and the estimator is a solution to a linear programming problem. Hence, a global minimizer is obtained in a finite number of simplex iterations. The suggested estimator also applies to the case where the censoring point is an unknown function of a set of regressors. It is shown that, under fairly weak conditions, the estimator has a $\sqrt{n}\ \text{-convergence}$ rate and is asymptotically normal. In the case of a fixed censoring point, its asymptotic property is nearly equivalent to that of the estimator suggested by Powell (1984, 1986a). A Monte Carlo study performed shows that the suggested estimator has very desirable small sample properties. It precisely corrects for the bias induced by censoring, even when there is a large amount of censoring, and for relatively small sample sizes.},
 author = {Moshe Buchinsky and Jinyong Hahn},
 journal = {Econometrica},
 number = {3},
 pages = {653--671},
 publisher = {[Wiley, Econometric Society]},
 title = {An Alternative Estimator for the Censored Quantile Regression Model},
 urldate = {2024-08-28},
 volume = {66},
 year = {1998}
}

@book{causalmlbook,
author={V. Chernozhukov and C. Hansen and N. Kallus and M. Spindler and V. Syrgkanis},
year={2024},
title={CasualMLBook: Applied Causal Inference Powered by ML and AI},
url={https://causalml-book.org/}
}

@article{raic2019,
author = {Martin Raič},
title = {{A multivariate Berry–Esseen theorem with explicit constants}},
volume = {25},
journal = {Bernoulli},
number = {4A},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {2824 -- 2853},
keywords = {Berry–Esseen theorem, explicit constants, Lyapunov bound, multivariate central limit theorem, Stein’s method},
year = {2019},
doi = {10.3150/18-BEJ1072},
URL = {https://doi.org/10.3150/18-BEJ1072}
}

@misc{tao2010,
author={Terence Tao},
title={254A, Notes 2: The central limit theorem},
url={https://terrytao.wordpress.com/2010/01/05/254a-notes-2-the-central-limit-theorem/},
year={2010},
month={January}
}

@book{dd2023,
  title={Credible answers to hard questions: Differences-in-differences for natural experiments},
    author={de Chaisemartin, C and D’Haultf{\oe}uille, X},
  year={2023},
  url={https://dx.doi.org/10.2139/ssrn.4487202}
}

@misc{londschien2024,
      title={Weak-instrument-robust subvector inference in instrumental variables regression: A subvector Lagrange multiplier test and properties of subvector Anderson-Rubin confidence sets},
      author={Malte Londschien and Peter Bühlmann},
      year={2024},
      eprint={2407.15256},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2407.15256},
}

@article{angrist2024,
title = {One instrument to rule them all: The bias and coverage of just-ID IV},
journal = {Journal of Econometrics},
volume = {240},
number = {2},
pages = {105398},
year = {2024},
issn = {0304-4076},
doi = {https://doi.org/10.1016/j.jeconom.2022.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0304407623000295},
author = {Joshua Angrist and Michal Kolesár},
keywords = {Instrumental variables, Weak instruments, Bias, Confidence intervals},
abstract = {We revisit the finite-sample behavior of single-variable just-identified instrumental variables (just-ID IV) estimators, arguing that in most microeconometric applications, the usual inference strategies are likely reliable. Three widely-cited applications are used to explain why this is so. We then consider pretesting strategies of the form t1>c, where t1 is the first-stage t-statistic, and the first-stage sign is given. Although pervasive in empirical practice, pretesting on the first-stage F-statistic exacerbates bias and distorts inference. We show, however, that median bias is both minimized and roughly halved by setting c=0, that is by screening on the sign of the estimated first stage. This bias reduction is a free lunch: conventional confidence interval coverage is unchanged by screening on the estimated first-stage sign. To the extent that IV analysts sign-screen already, these results strengthen the case for a sanguine view of the finite-sample behavior of just-ID IV.}
}

@article{gkm2024,
title={A POWERFUL SUBVECTOR ANDERSON–RUBIN TEST IN LINEAR INSTRUMENTAL VARIABLES REGRESSION WITH CONDITIONAL HETEROSKEDASTICITY}, volume={40}, DOI={10.1017/S0266466622000627}, number={5}, journal={Econometric Theory}, author={Guggenberger, Patrik and Kleibergen, Frank and Mavroeidis, Sophocles}, year={2024}, pages={957–1002}}


@article{tuvaandorj2024,
author = {Purevdorj Tuvaandorj},
title = {Robust Permutation Tests in Linear Instrumental Variables Regression},
journal = {Journal of the American Statistical Association},
volume = {0},
number = {ja},
pages = {1--24},
year = {2024},
publisher = {ASA Website},
doi = {10.1080/01621459.2024.2412363},


URL = {

        https://doi.org/10.1080/01621459.2024.2412363



},
eprint = {

        https://doi.org/10.1080/01621459.2024.2412363



}

}


@techreport{mogstad2024,
 title = "Instrumental Variables with Unobserved Heterogeneity in Treatment Effects",
 author = "Mogstad, Magne and Torgovitsky, Alexander",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "32927",
 year = "2024",
 month = "September",
 doi = {10.3386/w32927},
 URL = "http://www.nber.org/papers/w32927",
 abstract = {This chapter synthesizes and critically reviews the modern instrumental variables (IV) literature that allows for unobserved heterogeneity in treatment effects (UHTE). We start by discussing why UHTE is often an essential aspect of IV applications in economics and we explain the conceptual challenges raised by allowing for it. Then we review and survey two general strategies for incorporating UHTE. The first strategy is to continue to use linear IV estimators designed for classical constant (homogeneous) treatment effect models, acknowledge their likely misspecification, and attempt to reverse engineer an attractive interpretation in the presence of UHTE. This strategy commonly leads to interpretations of linear IV that involve local average treatment effects (LATEs). We review the various ways in which the use and justification of LATE interpretations have expanded and contracted since their introduction in the early 1990s. The second strategy is to forward engineer new estimators that explicitly allow for UHTE. This strategy has its roots in the Gronau-Heckman selection model of the 1970s, ideas from which have been revitalized through marginal treatment effects (MTE) analysis. We discuss implementation of MTE methods and draw connections with related control function and bounding methods that are scattered throughout the econometric and causal inference literature.},
}

@techreport{blandhol2022,
  title={When is TSLS actually late?},
  author={Blandhol, Christine and Bonney, John and Mogstad, Magne and Torgovitsky, Alexander},
  year={2022}
}

@book{causalml2024,
  title={Applied Causal Inference Powered by ML and AI},
  author={Chernozhukov, V. and Hansen, C. and Kallus, N. and Spindler, M. and Syrgkanis, V.},
  year={2024},
  url={https://causalml-book.org/}
  }

@article{epz2011,
Author = {Enikolopov, Ruben and Petrova, Maria and Zhuravskaya, Ekaterina},
Title = {Media and Political Persuasion: Evidence from Russia},
Journal = {American Economic Review},
Volume = {101},
Number = {7},
Year = {2011},
Month = {December},
Pages = {3253–85},
DOI = {10.1257/aer.101.7.3253},
URL = {https://www.aeaweb.org/articles?id=10.1257/aer.101.7.3253}}



@article{yang2024,
Author = {Yang, Lin and Lin, Yatang and Wang, Jin and Peng, Fangyuan},
Title = {Achieving Air Pollution Control Targets with Technology-Aided Monitoring: Better Enforcement or Localized Efforts?},
Journal = {American Economic Journal: Economic Policy},
Volume = {16},
Number = {4},
Year = {2024},
Month = {November},
Pages = {280–315},
DOI = {10.1257/pol.20220810},
URL = {https://www.aeaweb.org/articles?id=10.1257/pol.20220810}}

@article{ayabakan2024,
author = {Ayabakan, Sezgin and Bardhan, Indranil R. and Zheng, Zhiqiang (Eric)},
title = {Impact of Telehealth and Process Virtualization on Healthcare Utilization},
journal = {Information Systems Research},
volume = {35},
number = {1},
pages = {45-65},
year = {2024},
doi = {10.1287/isre.2023.1220},

URL = {

        https://doi.org/10.1287/isre.2023.1220



},
eprint = {

        https://doi.org/10.1287/isre.2023.1220



}
,
    abstract = { Telehealth has emerged as a tool to improve patient access by virtualizing healthcare services, particularly during the COVID-19 pandemic. However, concerns have been raised that telehealth may actually increase healthcare spending by leading to new types of utilization. Our research provides empirical evidence that this concern is unfounded based on a state-wide study of patient visit-level data of telehealth use in 58 hospitals in Maryland from 2012 to 2021. On average, telehealth use can reduce future outpatient visits by 13.6\% within 30 days after a telehealth visit, leading to a cost reduction of \$239. The benefits of telehealth are most apparent for diseases with high potential for process virtualization, such as mental health, skin disorders, metabolic, and musculoskeletal diseases. Although telehealth has a substitution effect on future healthcare utilization, this effect is not observed among rural patients who use telehealth as a gateway to utilize more primary care and specialist services. Our findings suggest that policymakers should promote the use of telehealth in a value-based healthcare environment by providing monetary incentives to expand telehealth use among patients and providers, and expand the scope of telehealth services to include consultation with specialists especially among rural patients. }
}